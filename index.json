[{"authors":null,"categories":null,"content":"I am in my final year as a undergraduate/concurrent master’s student at Harvard University interested in computational protein structure modeling. I’m interested in any opportunity after graduation that will allow me to contribute to the field.\nI’m currently working on diffusion models for protein structure in Regina Barzilay’s group at MIT CSAIL. I am also a teaching fellow for Harvard’s Stat 110 (Introduction to Probability) for the third time this fall.\n","date":1607817600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1607817600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am in my final year as a undergraduate/concurrent master’s student at Harvard University interested in computational protein structure modeling. I’m interested in any opportunity after graduation that will allow me to contribute to the field.","tags":null,"title":"Srihari Ganesh","type":"authors"},{"authors":null,"categories":null,"content":"Trying to make more visually useful Stat 110 notes for the Fall 2023 term. Will be updated week-by-week.\nWeek 9: Transformations, Gamma/Beta, Conditional Expectation Lecture dates: 11/7, 11/9; Section date: 11/15\nWeek 8: Multinomial, Multivariate Normal Lecture dates: 10/31, 11/2; Section date: 11/8\nWeek 7: Poisson Processes, Joint Distributions, and Covariance Lecture dates: 10/24, 10/26; Section date: 11/1\nWeek 6: Universality of the Uniform, Normal, Expo, and Moments Lecture dates: 10/17, 10/19; Section date: 10/25\nWeek 5: Continuous Distributions Lecture date: 10/12; Section date: 10/18\nWeek 4: Discrete Distributions and Expectation Lecture dates: 9/24, 9/26, 10/3; Section date: 10/4\nWeek 3: Conditional Probability Examples and Random Variables Lecture dates: 9/19, 9/21; Section date: 9/27\nWeek 2: Conditional Probability Lecture dates: 9/12, 9/14; Section date: 9/20\nWeek 1: Counting and Definitions of Probability Lecture dates: 9/5, 9/7; Section date: 9/13\n","date":1693526400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1693526400,"objectID":"251c39527051152bc61531454f92de65","permalink":"https://srihari-ganesh.github.io/stat110/","publishdate":"2023-09-01T00:00:00Z","relpermalink":"/stat110/","section":"stat110","summary":"Stat 110 (Introduction to Probability) @ Harvard, Fall 2023","tags":["expository","teaching"],"title":"Stat 110 Section Notes","type":"book"},{"authors":null,"categories":null,"content":"0. Logistical Info Section date: 11/15 Associated lectures: 11/7, 11/9 Associated pset: Pset 9, due 11/17 Office hours on 11/15 from 7-9pm at Quincy Dining Hall Remember to fill out the attendance form 0.1 Summary + Practice Problem PDFs Summary + Practice Problems PDF\nPractice Problem Solutions PDF\n","date":1699833600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1699833600,"objectID":"8df5728c937e821ee4beb816a130fc2f","permalink":"https://srihari-ganesh.github.io/stat110/week9/","publishdate":"2023-11-13T00:00:00Z","relpermalink":"/stat110/week9/","section":"stat110","summary":"Lecture dates: 11/7, 11/9; Section date: 11/15","tags":["expository","teaching"],"title":"Week 9: Transformations, Gamma/Beta, Conditional Expectation","type":"book"},{"authors":null,"categories":null,"content":"0. Logistical Info Section date: 11/8 Associated lectures: 10/31, 11/2 Associated pset: Pset 8, due 11/10 Office hours on 11/8 from 9-11pm at Quincy Dining Hall Remember to fill out the attendance form 0.1 Summary + Practice Problem PDFs Summary + Practice Problems PDF\nPractice Problem Solutions PDF\n1. Multinomial We first generalize the notion of Bernoulli trials to many categories; this vocabulary for “categorical trials” is not standard/necessary for the class, just introduced by me to help define the Multinomial.\nConsider categorical trials, where the outcome of a trial falls into one of $k$ categories (e.g., the roll of a die has $6$ categories, the flip of a coin has $2$, etc.). Let $\\mathbf p \\in \\mathbb R^k$ be a probability vector (where each entry is in $[0, 1]$ and the entries add up to $p$), where $p_i$ is the probability that the outcome falls into the $i^{\\text{th}}$ category. $\\newcommand{\\Mult}{\\mathrm{Mult}}\\newcommand{\\cov}{\\mathrm{Cov}}\\newcommand{\\Pois}{\\mathrm{Pois}}\\newcommand{\\Bin}{\\mathrm{Bin}}$\nMultinomial story: Suppose we run $n$ independent and identically distributed (i.i.d.) categorical trials with $k$ categories and probability vector $\\mathbf p$. Let $\\mathbf X$ (a $k$-dimensional random vector) count the number of trials that fell into each category. Then $\\mathbf X$ is distributed Multinomial: $\\mathbf X \\sim \\Mult_k(n, \\mathbf p)$.\n1.1 Multinomial Properties Marginal: For $\\mathbf X \\sim \\Mult_k(n, \\mathbf p)$, $X_j \\sim \\Bin(n, p_j)$. Conditioning: For $\\mathbf X \\sim \\Mult_k(n, \\mathbf p)$, \\begin{align*} (X_2, \\ldots, X_n) | X_1 = x_1 \u0026amp;\\sim \\Mult_{k-1}(n-x_1, \\left(\\frac{p_2}{1 - p_1}, \\ldots, \\frac{p_n}{1-p_1} \\right) ). \\end{align*} Lumping: Suppose $\\mathbf X \\sim \\Mult_k(n, \\mathbf p)$. Then we can group (lump) categories in any way to get a new Multinomial random variable by adding up the associated probabilities. For example, if $(X_1, X_2, X_3, X_4, X_5) \\sim \\Mult_5\\left( n, (p_1, p_2, p_3, p_4, p_5) \\right)$, then some valid examples are \\begin{align*} (X_1+X_4, X_2, X_3+X_5) \u0026amp;\\sim \\Mult_3\\left(n, ( p_1+p_4, p_2, p_3+p_5 )\\right),\\\\ (X_1+X_2, X_3, X_4, X_5) \u0026amp;\\sim \\Mult_4\\left(n, ( p_1+p_2, p_3, p_4, p_5 )\\right). \\end{align*} Covariance: For $\\mathbf X \\sim \\Mult_k(n, \\mathbf p)$, $\\cov(X_i, X_j) = -np_ip_j$. Chicken-Egg extension: Suppose $N \\sim \\Pois(\\lambda)$ and $\\mathbf X | N = n \\sim \\Mult_k (n, \\mathbf p)$ where $k, \\mathbf p$ don’t depend on $n$. Then for $j = 1, 2, \\ldots, k$, \\begin{align*} X_j \u0026amp;\\sim \\Pois(\\lambda p_j). \\end{align*} 2. Multivariate Normal Suppose $\\mathbf X$ is a $k$-dimensional random vector. Then $\\mathbf X$ follows Multivariate Normal (MVN) distribution if for any constants $t_1, \\ldots, t_k \\in \\mathbb R$, \\begin{align*} t_1 X_1 + \\cdots + t_k X_k \\end{align*} is Normal (where $0$ is consider to follow a degenerate Normal distribution). The $k=2$ case is called the Bivariate Normal.\n2.1 Multivariate Normal Properties Uncorrelated MVN implies independence: Suppose $(X, Y)$ is bivariate normal with $\\cov(X, Y) = 0$ (i.e., $X$ and $Y$ are uncorrelated). Then $X$ and $Y$ are independent.\nMore generally, if $\\mathbf X$ and $\\mathbf Y$ (potentially vectors) are components of the same MVN and $X_i, Y_j$ are uncorrelated for any $i, j$, then $\\mathbf X$ and $\\mathbf Y$ are independent. Please note the specific conditions under which this result holds. It is always true that independent random variables are uncorrelated, but the converse is not a general truth. For example, two uncorrelated Normal random variables are not necessarily independent; we could only make that statement if we knew they were components of the same MVN. Independence of sum and difference: Suppose $X \\sim \\mathcal N(\\mu_1, \\sigma^2)$ and $Y \\sim \\mathcal N(\\mu_2, \\sigma^2)$ are independent. Then $X+Y$ and $X-Y$ are also independent. Concatenation: Suppose $\\mathbf X = (X_1, \\ldots, X_n)$ and $\\mathbf Y = (Y_1, \\ldots Y_m)$ are both Multivariate Normal with $\\mathbf X, \\mathbf Y$ independent of each other. Then $(X_1, \\ldots, X_n, Y_1, \\ldots, Y_m)$ is also Multivariate Normal. Subvector: Suppose $(X, Y, Z)$ is Multivariate Normal. Then $(X, Y)$ is also Multivariate Normal. In general, any subvector of a Multivariate Normal still follows a Multivariate Normal distribution. ","date":1699315200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1699315200,"objectID":"0db5c07fc28c5cdba2003c3ef5bf49e3","permalink":"https://srihari-ganesh.github.io/stat110/week8/","publishdate":"2023-11-07T00:00:00Z","relpermalink":"/stat110/week8/","section":"stat110","summary":"Lecture dates: 10/31, 11/2; Section date: 11/8","tags":["expository","teaching"],"title":"Week 8: Multinomial, Multivariate Normal","type":"book"},{"authors":null,"categories":null,"content":"0. Logistical Info Section date: 11/1 Associated lectures: 10/24, 10/26 Associated pset: Pset 7, due 11/3 Office hours on 11/1 from 7-9pm at Quincy Dining Hall Remember to fill out the attendance form 0.1 Summary + Practice Problem PDFs Summary + Practice Problems PDF\nPractice Problem Solutions PDF\n1. Moment Generating Functions (MGFs) For a random variable $X$, the $\\mathbf{n^{th}}$ moment is $E(X^n)$.\nFor a random variable $X$, the moment generating function (MGF) is $$M_X(t) = E(e^{tX})$$ for $t \\in \\mathbb{R}$. If the MGF exists, then \\begin{align*} M_X(0) \u0026amp;= 1,\\\\ \\frac{d^n}{dt^n} M_X(t) |_{t=0} = M_X^{(n)}(t) \u0026amp;= E(X^n). \\end{align*} You should sanity-check that $M_X(0) = 1$ whenever you calculate an MGF. Useful MGF results:\nFor independent random variables, $X, Y$ with MGFs $M_X, M_Y$, then $M_{X+Y}(t) = M_X(t) M_Y(t)$. For random variable $X$ and scalars $a, b$, \\begin{align*} M_{a+bX}(t) = e^{at} M_X(bt) \\end{align*} since $M_{a+bX}(t) = E(e^{t(a+bX)}) = e^{at} E(e^{btX})$. A distribution is uniquely determined by any of the following:\nPMF (common for discrete), PDF, CDF (common for continuous), MGF, or matching to a named distribution (common). 2. Poisson Processes Consider a problem similar to Blissville/Blotchville, where $T_1, T_2, \\ldots,$ represent the arrival times of busses (the amount of time from when we started waiting to when each bus arrives). Then the bus arrival process is a Poisson process with rate $\\lambda$ if it satisfies the following conditions:\nFor any interval in time of length $t \u0026gt; 0$, the number of arrivals in that interval is distributed $\\mathrm{Pois}(\\lambda t)$. For any non-overlapping (disjoint) intervals of time, the number of bus arrivals are independent. This applies for any “arrival process” where $T_1, T_2, \\ldots$ correspond to arrival times. Pay attention to units: $\\lambda$ is a rate. So if $\\lambda$ has units of arrivals per hour, then $t$ should have units of hours. Results:\nInter-arrival times: In a Poisson process with rate $\\lambda$, the inter-arrival times (the time for the first arrival, $T_1$, and the times between consecutive arrives $T_2-T_1, T_3-T_2, \\ldots$) are each independently distributed \\begin{align*} T_1, T_2-T_1, T_3-T_2, \\ldots \\stackrel{i.i.d.}{\\sim} \\mathrm{Expo}(\\lambda). \\end{align*} Additionally note that $T_2, T_3, \\ldots,$ are not exponentially distributed. In fact, they follow Gamma distributions (which we will introduce soon): $T_n \\sim \\mathrm{Gamma}(n, \\lambda)$. Count-time duality: Fix a time $t \u0026gt; 0$. Let $N_t$ be the number of arrivals in the time interval $[0, t]$, and let $T_n$ be the arrival time of the $n$-th arrival. Then \\begin{align*} (T_n \u0026gt; t) = (N_t \u0026lt; n). \\end{align*} 3. Marginal, Conditional, and Joint Distributions Marginal, conditional, and joint distributions\nConsider two random variables $X, Y$.\nJoint Marginal Conditional Distribution $(X, Y)$ $X$ $X\\vert Y=y$ PMF $P(X = x, Y = y)$ $P(X = x)$ $P(X = x\\vert Y = y)$ CDF $P(X \\le x, Y \\le y)$ $P(X \\le x)$ $P(X \\le x \\vert Y = y)$ For example, $P(X \\vert Y = y)$ is a marginal PMF. All of these apply if we flip $X$ and $Y$, and PDFs follow analogously from PMFs.\nMarginalization: If we know the joint distribution of random variables $(X, Y)$, then we can find the marginal distribution of $X$ (and analogously, $Y$) by LOTP: \\begin{align*} P(X = x) \u0026amp;= \\sum_y P(X = x, Y = y), \u0026amp; \\text{$X,Y$ discrete}.\\\\ f_X(x) \u0026amp;= \\int_{-\\infty}^\\infty f_{X,Y}(x, y), \u0026amp; \\text{$X,Y$ continuous}. \\end{align*} Note that marginal distributions of $X$ and $Y$ are not sufficient (not enough information) to find the joint distribution of $X, Y$. Joint from marginal and conditional: If we know the marginal distribution of $X$ and the conditional distributions $Y | X=x$ for any $x$, then we can find the joint distribution of $(X, Y)$ by factoring out our probability: \\begin{align*} P(X = x, Y = y) \u0026amp;= P(X = x) P(Y = y | X = x), \u0026amp; \\text{$X, Y$ discrete.}\\\\ f_{X, Y} (x, y) \u0026amp;= f_{X}(x) f_{Y|X=x} (y), \u0026amp; \\text{ $X, Y$ continuous.} \\end{align*} Independence of random variables: Random variables $X, Y$ are independent if for all $x$ and $y$, any of the following hold (they imply each other, if valid): \\begin{align*} F_{X, Y} (x, y) = P(X \\le x, Y \\le Y) \u0026amp;= P(X \\le x) P(Y \\le Y) = F_X(x)F_Y(y), \u0026amp; \\text{ CDFs for any $X, Y$.}\\\\ P(X = x, Y = y) \u0026amp;= P(X = x) P(Y = y), \u0026amp; \\text{PMFs for discrete $X, Y$.}\\\\ f_{X, Y} (x, y) \u0026amp;= f_X(x) f_Y(y), \u0026amp;\\text{PDFs for continuous, $X, Y$.} \\end{align*} 2D LOTUS: Let $X, Y$ be random variables with known joint distribution. For $g: \\mathrm{support}(X) \\times \\mathrm{support}(Y) \\to \\mathbb R$, LOTUS extends to 2 dimensions (or analogously for any larger dimensions) to give \\begin{align*} E(g(X, Y)) \u0026amp;= \\begin{cases} \\sum_x \\sum_y g(x, y) P(X = x, Y = y), \u0026amp; \\text{ $X, Y$ discrete}\\\\ \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} g(x, y) f_{X, Y}(x, y)dx dy, \u0026amp; \\text{ $X, Y$ continuous}. \\end{cases} \\end{align*} 4. Covariance and Correlation Covariance and Correlation …","date":1699228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1699228800,"objectID":"cac427f62539a9f51e3d007b4e742c23","permalink":"https://srihari-ganesh.github.io/stat110/week7/","publishdate":"2023-11-06T00:00:00Z","relpermalink":"/stat110/week7/","section":"stat110","summary":"Lecture dates: 10/24, 10/26; Section date: 11/1","tags":["expository","teaching"],"title":"Week 7: Poisson Processes, Joint Distributions, and Covariance","type":"book"},{"authors":null,"categories":null,"content":"0. Logistical Info Section date: 10/25 Associated lectures: 10/17, 10/19 Associated pset: Pset 6, due 10/27 Office hours on 10/25 from 7-9pm at Quincy Dining Hall Please reach out if you wanted to sign up for a midterm debrief and missed the chance Remember to fill out the attendance form 0.1 Summary + Practice Problem PDFs Summary + Practice Problems PDF\nPractice Problem Solutions PDF\n1. Universality of the Uniform Recall that the standard uniform, $U \\sim \\mathrm{Unif}(0, 1)$, has support $(0, 1)$ with PDF $1$ in the support.\nUniversality of the Uniform (UoU): If $F$ is a valid CDF that is continuous and strictly increasing over the support, then\nLet $U \\sim \\mathrm{Unif}(0, 1)$. Then $F^{-1} (U)$ is a random variable with CDF $F$. Let $X$ have CDF $F$. Then $F(X) \\sim \\mathrm{Unif}(0,1)$. The first result applies to discrete random variables as well. The second result only works for continuous random variables.\nThis result is quite useful for simulation - if you have access to draws from a Uniform distribution, then you can transform them into draws from any distribution with a known (inverse) CDF.\nWe can prove UoU with the tools we’ve learned in class. For continuous random variables with $F$ as described in the theorem,\nFor $x \\in \\mathbb{R}$, \\begin{align*} P(F^{-1}(U) \u0026lt; x) = P(F(F^{-1}(U)) \u0026lt; F(x)) = P(U \u0026lt; F(x)) = F(x). \\end{align*} So $F^{-1}(U)$ has CDF $F$. We used the CDF of $U$ in the last step, since $F(x) \\in [0, 1]$. For $u \\in [0, 1]$, \\begin{align*} P(F(X) \u0026lt; u) \u0026amp;= P\\left(F^{-1}(F(X)) \u0026lt; F^{-1}(u)\\right)\\\\ \u0026amp;= P(X \u0026lt; F^{-1}(u)) = F(F^{-1}(u)) = u, \\end{align*} so $F(X) \\sim \\mathrm{Unif}(0, 1)$ since it has the CDF of a standard uniform. 2. Normal distribution 2.1 Standard Normal $Z \\sim \\mathcal{N}(0, 1)$ is a standard Normal random variable with support $\\mathbb R$. We notate the CDF as $\\Phi$ and PDF as $\\phi$.\n(Symmetry) The standard Normal is symmetric about $0$. In math, for $x \\in \\mathbb R$, $\\phi(x) = \\phi(-x)$. This also implies that $\\Phi(x) = 1 - \\Phi(-x)$. So $\\Phi(0) = 0.5$. For $Z \\sim \\mathcal{N}(0, 1)$, $-Z \\sim \\mathcal{N}(0, 1)$ as well. (Empirical rule/68-95-99.7 rule) \\begin{align*} P(-1 \u0026lt; Z \u0026lt; 1) \u0026amp;\\approx 0.68,\\\\ P(-2 \u0026lt; Z \u0026lt; 2) \u0026amp;\\approx 0.95,\\\\ P(-3 \u0026lt; Z \u0026lt; 3) \u0026amp;\\approx 0.997. \\end{align*} In this class, you can give exact answers in terms of $\\Phi$ and $\\phi$. On psets, you should also use a calculator/programming language/the empirical rule to get numerical approximations of $\\Phi$.\n2.2 Normal $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ (with $\\mu \\in \\mathbb R, \\sigma \u0026gt; 0$) is a Normal random variable with mean $\\mu$ and variance $\\sigma^2$, and also has support $\\mathbb R$.\n(Location-scale) For $Z \\sim \\mathcal{N}(0, 1)$, $\\mu + \\sigma Z \\sim \\mathcal{N}(\\mu, \\sigma^2)$.\nMore generally, for $X \\sim \\mathcal{N}(\\mu_1, \\sigma_1^2)$, $\\mu_2 + \\sigma_2 X \\sim \\mathcal{N}(\\mu_2 + \\mu_1 \\sigma_2, \\sigma_1^2 \\sigma_2^2)$.\n(Standardization) For $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, $\\frac{X-\\mu}{\\sigma} \\sim \\mathcal{N}(0, 1)$.\\ We often use this to get results in terms of $\\Phi$: \\begin{align*} P(X \u0026lt; x) = P(\\frac{X-\\mu}{\\sigma} \u0026lt; \\frac{x-\\mu}{\\sigma}) = \\Phi(\\frac{x-\\mu}{\\sigma}). \\end{align*}\n(Empirical rule) For $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, \\begin{align*} P(\\mu-\\sigma \u0026lt; X \u0026lt; \\mu+\\sigma) \u0026amp;\\approx 0.68\\\\ P(\\mu-2\\sigma \u0026lt; X \u0026lt; \\mu+2\\sigma) \u0026amp;\\approx 0.95\\\\ P(\\mu-3\\sigma \u0026lt; X \u0026lt; \\mu+3\\sigma) \u0026amp;\\approx 0.997 \\end{align*}\n(Sum of independent Normals) Let $X \\sim \\mathcal{N}(\\mu_1, \\sigma_1^2)$ and $Y \\sim \\mathcal{N}(\\mu_2, \\sigma_2^2)$ with $X, Y$ independent. Then \\begin{align*} X + Y \u0026amp;\\sim \\mathcal{N}(\\mu_1 + \\mu_2, \\sigma_1^2 + \\sigma_2^2),\\\\ X - Y \u0026amp;\\sim \\mathcal{N}(\\mu_1 - \\mu_2, \\sigma_1^2 + \\sigma_2^2). \\end{align*}\n(Variance when subtracting) See that we always add the variance above! This is also a general rule: for any independent random variables $X$ and $Y$, \\begin{align*} Var(X+Y) = Var(X - Y) = Var(X) + Var(Y). \\end{align*} See that this is consistent with the fact that $Var(-Y) = (-1)^2 Var(Y) = Var(Y)$. 3. Exponential distribution $X \\sim \\mathrm{Expo}(\\lambda)$ is an Exponential random variable with mean $\\frac{1}{\\lambda}$ and variance $\\frac{1}{\\lambda^2}$. $\\lambda$ is called the rate parameter.\n(Memorylessness) For $X \\sim \\mathrm{Expo}(\\lambda)$ and any $s, t \u0026gt; 0$, the memoryless property of the Exponential distribution states the following (equivalent) results: \\begin{align*} P(X \u0026gt; s + t \\vert X \u0026gt; s) \u0026amp;= P(X \u0026gt; t)\\\\ (X - s \\vert X \u0026gt; s) \u0026amp;\\sim \\mathrm{Expo}(\\lambda). \\end{align*} See specifically that $X-s | X\u0026gt;s$ is independent of the value of $s$.\nThe Exponential distribution is the only continuous distribution with this property. Additionally, the Geometric distribution is the only discrete distribution with support ${0, \\ldots, }$ that is memoryless.\nFor most results we talk about, you can’t put a random variable in the place of a constant - you might recall from last week’s problem set that we couldn’t let the sum of $N$ independent $\\mathrm{Pois}(\\lambda)$ r.v.s, with …","date":1697932800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697932800,"objectID":"0c603db2c1f9bde191c2df0ca36e5497","permalink":"https://srihari-ganesh.github.io/stat110/week6/","publishdate":"2023-10-22T00:00:00Z","relpermalink":"/stat110/week6/","section":"stat110","summary":"Lecture dates: 10/17, 10/19; Section date: 10/25","tags":["expository","teaching"],"title":"Week 6: Universality of the Uniform, Normal, Expo, and Moments","type":"book"},{"authors":null,"categories":null,"content":"0. Logistical Info Section date: 10/18 Associated lecture: 10/12 Associated pset: Pset 5, due 10/20 Office hours on 10/18 from 7-9pm at Quincy Dining Hall Remember to fill out the attendance form 0.1 Summary + Practice Problem PDFs Summary + Practice Problems PDF\nPractice Problem Solutions PDF\n1. Continuous Random Variables A continuous random variable has an interval for its support.\nMore precisely: a continuous random variable has an uncountable support, while discrete random variables have finite/countably infinite supports. We heavily lean on the cumulative distribution function (CDF): for any random variable $X$, the CDF $F: \\mathbb R \\to [0, 1]$ is defined $F(x) = P(X \\le x)$.\nWe don’t use the probability mass function (PMF) anymore, because $P(X = x) = 0$ for any $x$. This probability $0$ does not mean “impossible,” though. We instead use the probability density function:\nThe probability density function of a random variable $X$ with CDF $F$ is a function $f: \\mathbb{R} \\to \\mathbb{R}$: $$ f(x) = \\frac{d}{dx} F(x) $$ Probability densities are the continuous analog to probabilities, but they are NOT probabilities. A valid PDF is nonnegative and satisfies $$ \\int_{-\\infty}^\\infty f(x) dx = 1 $$ 1.1 Uses of CDFs and PDFs For any random variable $X$ (continuous or discrete), you can use the CDF to calculate the following: \\begin{align*} P(X \u0026gt; x) \u0026amp;= 1 - P(X \\le x) = 1-F(x)\\\\ P(x_1 \u0026lt; X \\le x_2) \u0026amp;= P(X \\le x_2) - P(X \\le x_1) = F(x_2) - F(x_1) \\end{align*} For the CDFs of continuous random variables,\nYou can assume the CDF is differentiable. $P(X \\le x) = P(X \u0026lt; x)$, so you can swap out $\\le$ and $\u0026lt;$ in calculations like the above. For a continuous random variable, we can find the probabilities of intervals by integrating the PDF and adjusting the bounds: \\begin{align*} P(X \\le x) = P(X \u0026lt; x) \u0026amp;= \\int_{-\\infty}^x f(x) dx\\\\ P(X \\ge x) = P(X \u0026gt; x) \u0026amp;= \\int_{x}^{\\infty} f(x) dx\\\\ P(x_1 \u0026lt; X \u0026lt; x_2) \u0026amp;= \\int_{x_1}^{x_2} f(x) dx. \\end{align*}\n1.2 Continuous analogs of all of our tools The general rules are:\nIntegrals instead of sums PDFs instead of PMFs So here’s a table with the tools we’ve talked about:\nTool Discrete Continuous Expectation $E(X) = \\sum_{x} x P(X = x)$ $E(X) = \\int_{-\\infty}^{\\infty} x f_X(x) dx$ LOTUS $E(g(X)) = \\sum_x g(x) P(X = x)$ $E(g(X)) = \\int_{-\\infty}^\\infty g(x) f_X(x) dx$ Bayes’ rule $P(X = x \\vert Y = y) = \\frac{P(Y=y \\vert X = x) P(X = x)}{P(A)}$ $f_{X\\vert Y=y}(x) = \\frac{f_{Y\\vert X=x}(y) f_X(x)}{f_Y(y)}$ 2. Uniform For any interval $(a, b)$, we can define a uniform distribution with that support, denoted $U \\sim \\mathrm{Unif}(a, b)$. A uniform distribution is equivalent to having a constant PDF over the support. There is no uniform whose support is the full real line. \\begin{align*} f_U(x) \u0026amp;= \\begin{cases} \\frac{1}{b-a} \u0026amp; x \\in (a, b)\\\\ 0 \u0026amp; x \\notin (a, b) \\end{cases}\\\\ F_U(x) = P(U \u0026lt; x) \u0026amp;= \\begin{cases} 0 \u0026amp; x \\le a\\\\\\ \\frac{x-a}{b-a} \u0026amp; x \\in (a, b)\\\\ 1 \u0026amp; x \\ge b \\end{cases} \\end{align*}\n","date":1697241600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697241600,"objectID":"4947aacee1486c0583afde13c79588a7","permalink":"https://srihari-ganesh.github.io/stat110/week5/","publishdate":"2023-10-14T00:00:00Z","relpermalink":"/stat110/week5/","section":"stat110","summary":"Lecture date: 10/12; Section date: 10/18","tags":["expository","teaching"],"title":"Week 5: Continuous Distributions","type":"book"},{"authors":null,"categories":null,"content":"0. Logistical Info Section date: 10/4 Associated lectures: 9/24, 9/26, 10/3 Associated pset: Pset 4, due 10/6 Midterm: 10/10 Office hours on 10/4 from 7-9pm at Quincy Dining Hall Exam office hours on 10/7 and 10/9 from 8-10pm at Quincy Dining Hall Remember to fill out the attendance form Given the structure of my section, I’m shifting away from a lot of explanation on this webpage. I may come back in the future and add more examples, but it doesn’t make much sense since we’re mainly doing practice problems in section. So this week, I have no concise summary section on the webpage because it’s all pretty tight - check out the handout below if you want it. 0.1 Summary + Practice Problem PDFs Summary + Practice Problems PDF\nPractice Problem Solutions PDF\n1. Random variables Let’s use the precise mathematical definition from last time: random variables assign real numbers to possible outcomes of an experiment. In other words, they map the sample space to the real line. So for a random variable $X$, for every outcome in the sample space, $\\omega \\in S$, there is a corrsponding real number, $X(\\omega) \\in \\mathbb{R}$.\nHere’s the terminology of a random variable that we’ve talked about thus far, where we continue using $X$ as an example of a random variable.\nThe support: what is the set of values that a random variable can take on? This is equivalent to the image/range of of $X$ on $S$, $X(S)$. For a named distribution like a Binomial, we say $X$ is distributed Binomial using $X \\sim \\mathrm{Bin}(n, p)$, where we have to set possible values of the parameters $n$ and $p$ for our specific problem. You CANNOT set $X = \\mathrm{Bin}(n, p)$: named distributions cannot equal random variables, they are just a blueprint for what the random variable looks like. The probability mass function (PMF): for any real number $x$ (or $t$ or $y$, it’s just a filler variable), what is the probability that $X$ takes on this value? This is notated $P(X = x)$. You should address every possible value of $x$: $P(X = x) = 0$ if $x$ is not in the support of $X$, $\\sum_{x \\in \\text{ support of } X } P(X = x) = 1$, and every probability should be valid (nonnegative, between $0$ and $1$ inclusive). NEW: the cumulative density function (CDF): for any real number $x$, what is the probability that $X$ takes on a value that is less than or equal to $x$? This is notated $P(X \\le x)$. You should again address every possible value of $x$, both in and outside of the support. Here, the requirements for a valid CDF are that $P(X \\le x) = 0$ if $x$ is less than the smallest value in the support and $P(X \\le x) = 1$ if $x$ is greater than the biggest value in the support. For an infinite support, we should have $P(X \\le x) \\to 0$ as $x \\to -\\infty$ and $P(X \\le x) \\to 1$ as $x \\to \\infty$. Additionally, a CDF should be non-decreasing (i.e., either increasing or a flat line). We often abbreviate to say random variables are independent and identically distributed (i.i.d.). Here’s a general approach for defining the distribution of a random variable (r.v.). You can give the distribution using either the PMF, the CDF, or a named distribution with the parameters defined.\nDefine the support of your r.v. See if the random variable matches the story of any of the named distributions we have discussed. To see if an r.v. matches a distribution, some things to check are For which named distributions is the support of your r.v. possible? Are there draws/samples/trials? If so, are they independent? If there is sampling, is it done with or without replacement? If you can match a named distribution, what are the parameters? Are those parameters allowed for that named distribution? If you can’t match a named distribution, how can you calculate the PMF using the information you checked about sampling and your counting skills? 2. Discrete distributions You can find details like the support, PMF, CDF, expectation, and variance in the table of distributions on page 605 of the textbook or page 3 of the midterm handout. We’ll focus on the stories and connections between distributions. For these discrete random variables (except for the Poisson), you should develop comfort with calculating their PMFs from scratch.\n2.1 Bernoulli Story: We run a trial with probability $p$ of success. Let the random variable $X$ be $1$ if the trial succeeds or $0$ if the trial fails. Then $X \\sim \\mathrm{Bern}(p)$.\nConnections:\nFor $X \\sim \\mathrm{Bern}(p)$, $1-X \\sim \\mathrm{Bern}(1-p)$. For $X \\sim \\mathrm{Bern}(p)$, $X^2 = X$, so $X^2 \\sim \\mathrm{Bern}(p)$. If you’re wondering why, check the support! 2.2 Binomial Story: We run $n$ independent trials, each with an equal probability $p$ of success. Let $X$ be the number of successful trials. Then $X \\sim \\mathrm{Bin}(n,p)$.\nConnections:\nFor $n$ independent and identically distributed Bernoulli random variables $X_1, \\ldots, X_n \\stackrel{i.i.d.}{\\sim} Bern(p)$, $$\\sum_{i=1}^n X_i \\sim \\mathrm{Bin}(n, p).$$ This means $\\mathrm{Bern}(p)$ is …","date":1696204800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696204800,"objectID":"3e5517c495e5b4df00d931d8848899a4","permalink":"https://srihari-ganesh.github.io/stat110/week4/","publishdate":"2023-10-02T00:00:00Z","relpermalink":"/stat110/week4/","section":"stat110","summary":"Lecture dates: 9/24, 9/26, 10/3; Section date: 10/4","tags":["expository","teaching"],"title":"Week 4: Discrete Distributions and Expectation","type":"book"},{"authors":null,"categories":null,"content":"0. Logistical Info Section date: 9/27 Associated lectures: 9/19, 9/21 Associated pset: Pset 3, due 9/29 Office hours on 9/27 from 7-9pm at Quincy Dining Hall Remember to fill out the attendance form Scroll to section 4 for a concise content summary. 0.1 Summary + Practice Problem PDFs Summary + Practice Problems PDF\nPractice Problem Solutions PDF\n1. Examples from class The lecture on 9/19 was full of examples of conditional probability. Here are my takeaways for each. I did not rewrite the examples because solutions are quite well-written in the book and/or in lecture.\n1.1 Winter girl (Examples 2.2.5-2.2.7 in the book) Define your events as specifically as possible, there are a lot of details that surprisingly can change probabilities. See if you can simplify your events (both mathematically and logically). For example, if $A$ is the event that there’s at least one girl and $B$ is the event that there are two girls, $P(A, B) = P(B)$ since if there are two girls, there is automatically at least one girls. 1.2 Monty Hall (Example 2.7.1 and many practice problems in the book) You can use the Law of Total Probability in some extreme ways! Condition on things that make your life much, much easier - in Monty Hall problems (and the variants that Joe likes to write), I very often condition on the location of the car or use Bayes’ rule to move information about the car’s location into the condition!\n1.3 Simpson’s paradox (Example 2.8.3 in the book) I think it’s a good to develop the skill of coming up with similar paradoxes - at the very least, it can test your understanding of probability. My understanding of this phenomenon is that there are two tasks - a hard and an easy task. Doctor A might have a better success rate in each task, but Doctor B can still have a higher overall success rate.\nThis happens because Doctor A does more of the harder task, which drags their average down, while Doctor B inflates their average by doing the easier task more often. There are some other intuitive corollaries - like how some students may learn a lot but have lower GPAs then other students because they take a higher proportion of challenging classes.\nTo construct these paradoxes, I think you need a hard task (where both doctors have a “low” success rate) and an easy task (where both doctors have a higher success rate). Then doctor A has to do more of the hard task, while doctor B needs to do more of the easy task, to weight their averages differently.\n1.4 Gambler’s ruin (Example 2.7.3 in the book) You will be assessed (in pset and/or exam) on your ability to apply the gambler’s ruin result in other contexts, but you will never have to re-derive it. So you can figure out how variables in your problem correspond to gambler’s ruin (or even set up the difference equation) then just jump to plugging in the solution given.\nIn the gambler’s ruin problem, gambler $A$ starts with $i$ dollars and gambler $B$ starts with $N-i$ dollars. They keep making 1 dollar bets (which gambler $A$ has a probability $p$ of winning) until someone runs out of money (either gambler $A$ has $0$ dollars or gambler $B$ has 0 dollars). We often define $q = 1-p$ for notational convenience.\nIf we define $p_i$ to be the probability that gambler $A$ wins if they start with $i$ dollars, then using first-step analysis we find that $$ p_i = p_{i+1} p + p_{i-1}q. $$ This gives a difference equation solution of $$ p_i = \\begin{cases} \\frac{1 - (\\frac{q}{p})^i}{1 - (\\frac{q}{p})^N}\u0026amp; p \\ne 1/2\\\\ \\frac{i}{N}\u0026amp; p = 1/2 \\end{cases} $$\nTo match a problem to this, you should\nMake sure that “bets” are worth 1 dollar each. Make sure that gambler $A$ loses if they hit 0 dollars, and wins if they hit some fixed amount of dollars ($N$) Make sure there is a constant probability of winning each bet. 2. Random variables 2.1 Definition A random variable summarizes some experiment. So if you have a sample space $S$, for each possible outcome $\\omega \\in S$, your random variable takes on a certain (real number). Here are some examples of random variables:\nSay we’re rolling a die, so $S$ is the set of possible rolls. Then we could have $X = 1$ if we roll a $1$, $X = 2$ if we roll a $2$, and so on. We could define another random variable $Y$ to be the square of the roll. so $Y = 1$ if we roll a 1, $Y = 16$ if we roll a $4$, etc. Random variables don’t have to take on different values for every outcome. So we could have $Z = 2$ if we roll an even number and $Z = 1$ if we roll an odd number. They also don’t have to be discrete values - we could have a random variable $T$ represent the exact temperature in the room right now. 2.2 Defining discrete random variables A random variable is discrete if it has a finite or countably infinite number of values (something like $1, 2, 3, 4, \\ldots$ is countably infinite. If instead your random variable can take on any value in an interval - like any real number, or any real number between 2 and 4, etc. - then it is uncountable).\nWe can uniquely describe …","date":1695686400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695686400,"objectID":"0eb0524cfeceda00a2289760a17f5467","permalink":"https://srihari-ganesh.github.io/stat110/week3/","publishdate":"2023-09-26T00:00:00Z","relpermalink":"/stat110/week3/","section":"stat110","summary":"Lecture dates: 9/19, 9/21; Section date: 9/27","tags":["expository","teaching"],"title":"Week 3: Conditional Probability Examples and Random Variables","type":"book"},{"authors":null,"categories":null,"content":"0. Logistical Info Section date: 9/20 Associated lectures: 9/12, 9/14 Associated pset: Pset 2, due 9/22 Office hours on 9/20 from 7-9pm at Quincy Dining Hall Remember to fill out the attendance form Scroll to section 5 for a concise content summary. 0.1 Summary + Practice Problem PDFs Summary + Practice Problems PDF\nPractice Problem Solutions PDF\n1. Brushing up on the definition of probability We’ll restate the axioms for the general definition of probability:\nDefinition of probability:\nThere are just two axioms (rules that probabilities have to follow):\n$P(S) = 1, P(\\emptyset) = 0.$ If events $A_1, A_2, \\ldots$ are disjoint, then $$ P\\left( \\bigcup_{j=1}^\\infty A_j\\right) = \\sum_{j=1}^\\infty P(A_j). $$ In other words, if $A_1, A_2, \\ldots$ partition some event $B$, then $P(B) = \\sum_{j=1}^\\infty P(A_j)$.\nTips for calculating probabilities:\nDefine events for every aspect of the problem (e.g., “$A$ = the event that it rains tomorrow, $B$ = the event that it rained today”) Write out the probabilities that you are given in the problem using notation (e.g., “$P(A|B) = 1/2$, $P(B) = 1/4$). Write the probability that you want to calculate using notation (e.g., we want to calculate the unconditional probability that it rains tomorrow, $P(A)$). Figure out how the tools we have learned allow you to utilize the probabiliies that you do know (step 2) to calculate the probabilities that you don’t know (step 3). There are some important results that follow:\nProbability of a complement: If $A$ is an event a sample space $S$, $$ P(A) = 1 - P(A^c). $$ Concisely, the probability of an event occuring is $1$ minus the probability of the event not occuring. Probability of a union: For events $A$, $B$, we have $$ P(A \\cup B) = P(A) + P(B) - P(A \\cap B). $$ It’s also useful to “disjointify” $A \\cup B$ into a partition ($A \\cup B^c, A \\cap B, A^c \\cup B$) which allows us to use the second axiom and get $$ P(A \\cup B) = P(A \\cup B^c) + P(A \\cap B) + P(A^c \\cup B). $$ Principle of Inclusion-Exclusion (PIE): this is a general formula for the probability of the union of $n$ events \\begin{align*} P(\\bigcup_{i=1}^n A_i) \u0026amp;= \\sum_i P(A_i) - \\sum_{i \u0026lt; j} P(A_i \\cap A_j)\\\\ \u0026amp;+ \\sum_{i \u0026lt; j \u0026lt; k} P(A_i \\cap A_j \\cap A_k) - \\cdots + (-1)^{n+1} P(\\bigcap_{i=1}^n A_i). \\end{align*} Note that the formula for the probability of the union of two events is the $n=2$ case of PIE. A potential workflow (that you saw on Pset 1) for the probability of an intersection, $P(A_1 \\cap \\cdots \\cap A_n)$, is to Use complementary counting and DeMorgan’s law (in that order) to turn the intersection into a union: \\begin{align*} P(A_1 \\cap \\cdots \\cap A_n) \u0026amp;= 1 - P((A_1 \\cap \\cdots \\cap A_n)^c)\\\\ \u0026amp;= 1 - P(A_1^c \\cup \\cdots \\cup A_n^c) \\end{align*} Apply PIE to the union $P(A_1^c \\cup \\cdots \\cup A_n^c)$ 2. Conditional Probability Notation note:\nWe will start writing $P(A \\cap B)$ as $P(A, B)$ (i.e., commas between events and intersections are equivalent).\nConditional probability:\nIf $A$ and $B$ are two events, then the probability that $A$ occurs conditional on the fact that $B$ occurs (or given that $B$ occurs) is notated as $P(A|B)$ and equals $$ P(A|B) = \\frac{P(A, B)}{P(B)}. $$ All conditions go to the right of the bar symbol $|$.\nWe read $P(A|B)$ is the “probability of $A$ given $B$” or “probability of $A$ conditioned on $B$” Intuitively, we can consider that if we know $B$ occurs, $B$ basically becomes our new sample space, so we take the probability that both $A$ and $B$ occurs, $P(A, B)$, and rescale it by the probability that $B$ occurs, $P(B)$.\nWe’re also quick to note that conditional probabilities are the same as “normal” probabilities — in fact, all probabilities can be considered conditional, we just treat some conditions more implicitly than others since they are more obvious/always involved to the problem. We’ll use extra conditioning to refer to problems where some conditions are always present (i.e., we never want to/don’t know how to calculate the probability of those conditionns). For example, to calculate the probability that it rains tomorrow ($A$) given that it rained today $B$, we would right $P(A|B)$. However, we are implicitly conditioning on a lot of things: that the world exists tomorrow ($W$), that I will be on Harvard campus when I check whether it rains $H$, etc. So we could incorporate these extra conditions into our problem to write the definition of conditional probability with extra conditioning: $$ P(A|B, H, W) = \\frac{P(A, B | H, W)}{P(B | H, W)} $$ As you can see, when we want certain events to be extra conditions, they are conditions in every related probability we calculate. Each time we apply a formula for conditional probability, we have to choose whether to treat the each condition like $B$ (free to move around) or like $H$ (extra conditioning/always a condition).\n3. Tools using Conditional Probability If you ever need to solve a problem involving a sequence of things (like a game with many turns, or a random walk, or so …","date":1694822400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694822400,"objectID":"10c4997f7a77acf0752ac8c5370457f1","permalink":"https://srihari-ganesh.github.io/stat110/week2/","publishdate":"2023-09-16T00:00:00Z","relpermalink":"/stat110/week2/","section":"stat110","summary":"Lecture dates: 9/12, 9/14; Section date: 9/20","tags":["expository","teaching"],"title":"Week 2: Conditional Probability","type":"book"},{"authors":null,"categories":null,"content":"0. Info Section date: 9/13 Associated lectures: 9/5, 9/7 Associated pset: Pset 1, due 9/15 Where to find me Section: Wednesdays 3-4:15pm, see section info on Canvas for location. Section will always cover content for the pset due on Friday (in two days) Specifically, this means we’ll focus on the lectures from the previous week (six and eight days previous), and not the lecture from the day before section Office Hours: Wednesdays 7-9pm, see office hours info on Canvas/Google Calendar for location. I will often stay for the office hours that come right after (Wednesday 9-11pm), and occasionally may drop by at the Thursday 7:30-9:30pm office hours in the same location as well. Syllabus clarifications Problem Sets: due Fridays at 5pm. They will cover content up to the previous week (e.g., up to the lecture 8 days before the due date). Attendance: If your grade ends up being slightly below a grade cutoff, attendance in lecture in section may boost you over. Attendance (or lack thereof) will not be used to penalize. Me: Srihari Ganesh (senior) Concentrating in Chemical \u0026amp; Physical Biology and Math w/ a master’s in Stat Doing computational biology research in diffusion models for protein structure generation Love teaching Stat + playing IM sports on the side Happy to chat about everything though I’m not an expert on anything 1. Set Theory A set is an unordered collection of unique items. For example, $A = \\{ 8, 1, 2 \\}$ is a set with items (elements) $8$, $1$, and $2$.\nSets are unordered, so $\\{8, 1, 2 \\} = \\{2, 8, 1\\}$. Sets contain unique elemnts, so $\\{8, 1, 2, 1\\}$ is NOT a set. Sets are a collection of any type of item, so $\\{A, B, C\\}$, $\\{\\text{car, cow, walnut}\\}$, and $\\{$❤️, 👨‍🏫, ⏳$\\}$ are all sets. 1.1 Definitions $A$ is a subset of $B$ (notated $A \\subset B$ or $A \\subseteq B$) if every element of $A$ is also an element of $B$.\nThe empty set (notated $\\emptyset$) is the set with no elements, $\\{\\}$.\nThe union (notated $A \\cup B$) is the set of all elements that are in $A$ or $B$ (or both).\nThe intersection (notated $A \\cap B$) is the set of all elements that are in both $A$ and $B$.\n$A$ and $B$ are disjoint if $A$ and $B$ share no elements (notated $A \\cap B = \\emptyset$)\nA collection of sets $A_1, \\ldots, A_n$ are disjoint if no pair of sets shares any elements ($A_i \\cap A_j = \\emptyset$ for all $i, j$) A collection of sets $B_1, B_2, \\ldots, B_n$ are a partition of $B$ if\n$B_1, B_2, \\ldots, B_n$ are disjoint, and $B_1 \\cup B_2 \\cup \\cdots \\cup B_n = B$. Intuitively, $B_1, B_2, \\ldots, B_n$ are (non-overlapping) puzzle pieces that form the the puzzle, $B$, when put together.\nThe complement of a set $A$ (notated $A^c$) is the set of all elements not in $A$\nComplements have to be taken relative to some superset $S$ (notated $A \\subset S$). We think of this set as the “universe” that $A$ lives in, and is usually implied by context. ex. Let $S = \\{1, 2, 3 \\}, A = \\{1, 3\\}$. Then $A^c = \\{2\\}$. Note that $A$ and $A^c$ partition $S$. The difference between two sets $B \\setminus A$ is the set of all elements in $B$ which are not in $A$.\nNote that $B \\cap A$, $B \\setminus A$ partition $B$. An equivalent notation is $B \\setminus A = B \\cap A^c$. The cardinality of a set, |A|, indicates the size of the set. For finite sets, it is the number of elements in the set.\nWe notate set membership in the following way: letting $A = \\{8, 1, 2\\}$,\n“$2$ is in $A$” is notated $2 \\in A$. “$0$ is not in $A$” is notated $0 \\notin A$. 1.2 Verbal Understanding We want to think about basic set operations in words.\n$x \\in A$ reads as “$x$ is in $A$”. We tend to work with sets by thinking about whether some variable or number is in the (e.g., whether or not $x \\in A$) instead of just looking at the set written out like $A = \\{8, 1, 2\\}$. $A \\cup B$ reads as “$A$ or $B$”. So we can say $x \\in A \\cup B$ intuitively reads as “$x$ is in $A$ or $x$ is in $B$”. Be careful to note that we are using the inclusive or, so $A \\textit{ or } B$ means $A$, $B$, or both. $A \\cap B$ reads as “$A$ and $B$”. So $x \\in A \\cap B$ reads as “$x$ is in $A$ and $x$ is in $B$”. $A^c$ reads as “not $A$”, so $x \\in A^c$ reads as “$x$ is not in $A$” We can apply this so see that the two versions of the set difference are equivalent. $x \\in B \\setminus A$ reads as “$x$ is in $B$ but not in $A$”, while $x \\in B \\cap A^c$ reads as “$x$ is in B and in $A^c$ (i.e., not in $A$).”\nWhat if we have multiple operations in the same expression?\n$x \\in (A \\cap B)^c$ reads as “$x$ is not in both $A$ and $B$”. This means that either $x$ is not in $A$ or $x$ is not in $B$, so $x \\in A^c \\cup B^c$. $x \\in (A \\cup B)^c$ read as “$x$ is not in $A$ or $B$”. This means that $x$ is not in $A$ and $x$ is not in $B$, so $x \\in A^c \\cap B^c$. We see in the next section that these statements are essentially DeMorgan’s laws. 1.3 DeMorgan’s Laws DeMorgan’s Laws \\begin{align} (A \\cap B)^c \u0026amp;= A^c \\cup B^c \\\\ (A \\cup B)^c \u0026amp;= A^c \\cap B^c \\end{align} By talking through the expressions, you …","date":1693353600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693353600,"objectID":"26974da8975636691009f04590ab26a6","permalink":"https://srihari-ganesh.github.io/stat110/week1/","publishdate":"2023-08-30T00:00:00Z","relpermalink":"/stat110/week1/","section":"stat110","summary":"Lecture dates: 9/5, 9/7; Section date: 9/13","tags":["expository","teaching"],"title":"Week 1: Counting and Definitions of Probability","type":"book"},{"authors":null,"categories":null,"content":"","date":1661817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661817600,"objectID":"3a3e349399c313bb295002316e9a3eea","permalink":"https://srihari-ganesh.github.io/projects/stat110/","publishdate":"2022-08-30T00:00:00Z","relpermalink":"/projects/stat110/","section":"projects","summary":"Made for Fall 2023 @ Harvard","tags":["expository","teaching"],"title":"Stat 110 Section Notes","type":"projects"},{"authors":null,"categories":null,"content":"Direct coupling analysis (DCA) is used to identify coevolving protein residues and understand structural and functional constraints in proteins. In such methods, Potts models parametrize the proteins and using a multiple sequence alignment (MSA); these sequences are assumed to come from a single underlying Potts model. However, MSAs can contain heterogeneity: for example, in an MSA of the murA-lpxC system, only a subset of the seqeunces interact. As an undergraduate researcher in the Marks Lab at Harvard Medical School, I implemented a mixture model with the EVcouplings pipeline using an expectation-maximization algorithm to simultaneously infer class assignments of each sequence in an MSA and the Potts model parameters of each class. However, the algorithm failed to infer clusters correlated with interaction. This work was supported by the Harvard College Research Program (HCRP).\n","date":1661472e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661472e3,"objectID":"696608a128be33064a440809678c8c1f","permalink":"https://srihari-ganesh.github.io/projects/potts/","publishdate":"2022-08-26T00:00:00Z","relpermalink":"/projects/potts/","section":"projects","summary":"Debora Marks Lab @ Harvard Medical School, Summer 2022","tags":["research","laboratory"],"title":"Potts Mixture Models for Protein Residue Direct Coupling Analysis","type":"projects"},{"authors":null,"categories":null,"content":"The DNA fragment assembly problem is an important and active field of research with applications in every field of biology. Exploratory work has previously been done to attempt to apply reinforcement learning to fragment assembly - namely, Q-learning was used on an episodic Markov Decision Process (MDP) model of the problem. For my final project in Stat 234 (a graduate reinforcement learning seminar taught by Professor Susan Murphy), I implemented Real-Time Dynamic Programming (RTDP), a learning algorithm that is more suited for searching sparse MDPs, and found improved performance over Q-learning in simulations on sythetic microgenomes. However, issues with memory requirements and the lack of flexibility when using an MDP formulation re-establish that such approaches are likely infeasible at scale.\n","date":1651968e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651968e3,"objectID":"1d1ff217077aae40a972759b6fca5b97","permalink":"https://srihari-ganesh.github.io/projects/stat234/","publishdate":"2022-05-08T00:00:00Z","relpermalink":"/projects/stat234/","section":"projects","summary":"Stat 234 (Sequential Decision Making) @ Harvard, Spring 2022","tags":["research","course project"],"title":"Real-Time Dynamic Programming for *De Novo* Genome Assembly","type":"projects"},{"authors":null,"categories":null,"content":"Bacteria can maintain a near-constant population size for years without an external food source in a state known as the long-term stationary phase (LTSP). However, they do not hibernate - instead, they survive by recycling the nutrients of dead bacteria, creating a dynamic and evolving environment. In the Cluzel Lab at Harvard, we experimentally introduced an evolutionary pressure in E. coli by transforming a plasmid with overexpressed nonessential proteins, including YFP, into the bacteria to increase their protein burden, and followed the state of the plasmid using the fluorescent reporter. We also developed a system of differential equations to model the system: the introduction of oscillating protein burdens led to coexistence between proteins of different average burdens, which could allow us to better explain past experimental observations. This work was supported by the Program for Research in Science and Engineering (PRISE) at Harvard College.\n","date":1628726400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628726400,"objectID":"084a5f1789256f5671461ce7b9945a4d","permalink":"https://srihari-ganesh.github.io/projects/ltsp/","publishdate":"2021-08-12T00:00:00Z","relpermalink":"/projects/ltsp/","section":"projects","summary":"Philippe Cluzel Lab @ Harvard Molecular and Cellular Biology, Summer 2021","tags":["research","laboratory"],"title":"Modeling Evolutionary Dynamics of *E. coli* in the Long-Term Stationary Phase","type":"projects"},{"authors":null,"categories":null,"content":"As a student in Math 22b, taught by Dr. Dusty Grundmeier, I learned introductory proof-based vector calculus with some exposure to limits and continuity. However, I was interested in getting more exposure to real analysis, so for my final project I learned about existence and uniqueness theorems for differential equations and wrote an expository paper presenting a simple case of such a theorem.\n","date":1620432e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620432e3,"objectID":"a7fb9b9a08ed4151910d21f68d053958","permalink":"https://srihari-ganesh.github.io/projects/math22b/","publishdate":"2021-05-08T00:00:00Z","relpermalink":"/projects/math22b/","section":"projects","summary":"Math 22b (Vector Calculus) @ Harvard, Spring 2021","tags":["expository","course project"],"title":"A Local Existence and Uniqueness Theorem for First Order Differential Equations","type":"projects"},{"authors":["Srihari Ganesh","吳恩達"],"categories":["Demo","教程"],"content":"import libr print(\u0026#39;hello\u0026#39;) Overview The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It’s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more Get Started 👉 Create a new site 📚 Personalize your site 💬 Chat with the Wowchemy community or Hugo community 🐦 Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy 💡 Request a feature or report a bug for Wowchemy ⬆️ Updating Wowchemy? View the Update Tutorial and Release Notes Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n❤️ Click here to become a sponsor and help support Wowchemy’s future ❤️ As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features 🦄✨\nEcosystem Hugo Academic CLI: Automatically import publications from BibTeX Inspiration Check out the latest demo of what you’ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files. Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://srihari-ganesh.github.io/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome 👋 We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","开源"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":null,"categories":null,"content":"Wowchemy is designed to give technical content creators a seamless experience. You can focus on the content and Wowchemy handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you’ll find some examples of the types of technical content that can be rendered with Wowchemy.\nExamples Code Wowchemy supports a Markdown extension for highlighting code syntax. You can customize the styles under the syntax_highlighter option in your config/_default/params.yaml file.\n```python import pandas as pd data = pd.read_csv(\u0026#34;data.csv\u0026#34;) data.head() ``` renders as\nimport pandas as pd data = pd.read_csv(\u0026#34;data.csv\u0026#34;) data.head() Mindmaps Wowchemy supports a Markdown extension for mindmaps.\nSimply insert a Markdown markmap code block and optionally set the height of the mindmap as shown in the example below.\nA simple mindmap defined as a Markdown list:\n```markmap {height=\u0026#34;200px\u0026#34;} - Hugo Modules - wowchemy - wowchemy-plugins-netlify - wowchemy-plugins-netlify-cms - wowchemy-plugins-reveal ``` renders as\n- Hugo Modules - wowchemy - wowchemy-plugins-netlify - wowchemy-plugins-netlify-cms - wowchemy-plugins-reveal A more advanced mindmap with formatting, code blocks, and math:\n```markmap - Mindmaps - Links - [Wowchemy Docs](https://wowchemy.com/docs/) - [Discord Community](https://discord.gg/z8wNYzb) - [GitHub](https://github.com/wowchemy/wowchemy-hugo-themes) - Features - Markdown formatting - **inline** ~~text~~ *styles* - multiline text - `inline code` - ```js console.log(\u0026#39;hello\u0026#39;); console.log(\u0026#39;code block\u0026#39;); ``` - Math: $x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$ ``` renders as\n- Mindmaps - Links - [Wowchemy Docs](https://wowchemy.com/docs/) - [Discord Community](https://discord.gg/z8wNYzb) - [GitHub](https://github.com/wowchemy/wowchemy-hugo-themes) - Features - Markdown formatting - **inline** ~~text~~ *styles* - multiline text - `inline code` - ```js console.log(\u0026#39;hello\u0026#39;); console.log(\u0026#39;code block\u0026#39;); ``` - Math: $x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$ Charts Wowchemy supports the popular Plotly format for interactive charts.\nSave your Plotly JSON in your page folder, for example line-chart.json, and then add the {{\u0026lt; chart data=\u0026#34;line-chart\u0026#34; \u0026gt;}} shortcode where you would like the chart to appear.\nDemo:\nYou might also find the Plotly JSON Editor useful.\nMath Wowchemy supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.yaml file.\nTo render inline or block math, wrap your LaTeX math with {{\u0026lt; math \u0026gt;}}$...${{\u0026lt; /math \u0026gt;}} or {{\u0026lt; math \u0026gt;}}$$...$${{\u0026lt; /math \u0026gt;}}, respectively. (We wrap the LaTeX math in the Wowchemy math shortcode to prevent Hugo rendering our math as Markdown. The math shortcode is new in v5.5-dev.)\nExample math block:\n{{\u0026lt; math \u0026gt;}} $$ \\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2} $$ {{\u0026lt; /math \u0026gt;}} renders as\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$ Example inline math {{\u0026lt; math \u0026gt;}}$\\nabla F(\\mathbf{x}_{n})${{\u0026lt; /math \u0026gt;}} renders as $\\nabla F(\\mathbf{x}_{n})$.\nExample multi-line math using the math linebreak (\\\\):\n{{\u0026lt; math \u0026gt;}} $$f(k;p_{0}^{*}) = \\begin{cases}p_{0}^{*} \u0026amp; \\text{if }k=1, \\\\ 1-p_{0}^{*} \u0026amp; \\text{if }k=0.\\end{cases}$$ {{\u0026lt; /math \u0026gt;}} renders as\n$$ f(k;p_{0}^{*}) = \\begin{cases}p_{0}^{*} \u0026amp; \\text{if }k=1, \\\\ 1-p_{0}^{*} \u0026amp; \\text{if }k=0.\\end{cases} $$ Diagrams Wowchemy supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid graph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] ``` renders as\ngraph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] An example sequence diagram:\n```mermaid sequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! ``` renders as\nsequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! An example Gantt diagram:\n```mermaid gantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d ``` renders …","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"07e02bccc368a192a0c76c44918396c3","permalink":"https://srihari-ganesh.github.io/post/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/post/writing-technical-content/","section":"post","summary":"Wowchemy is designed to give technical content creators a seamless experience. You can focus on the content and Wowchemy handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.","tags":null,"title":"Writing technical content in Markdown","type":"post"},{"authors":["Srihari Ganesh"],"categories":null,"content":" Create your slides in Markdown - click the Slides button to check out the example. Supplementary notes can be added here, including code, math, and images.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://srihari-ganesh.github.io/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":["Srihari Ganesh"],"categories":[],"content":"from IPython.core.display import Image Image(\u0026#39;https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png\u0026#39;) print(\u0026#34;Welcome to Academic!\u0026#34;) Welcome to Academic! Install Python and JupyterLab Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata (front matter).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post\u0026#39;s title date: 2019-09-01 # Put any other Academic metadata here... --- Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image, place an image named featured into your post’s folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=. Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"https://srihari-ganesh.github.io/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://srihari-ganesh.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Srihari Ganesh","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. Create your slides in Markdown - click the Slides button to check out the example. Supplementary notes can be added here, including code, math, and images.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://srihari-ganesh.github.io/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Srihari Ganesh","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. Create your slides in Markdown - click the Slides button to check out the example. Supplementary notes can be added here, including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"https://srihari-ganesh.github.io/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"An example conference paper","type":"publication"}]
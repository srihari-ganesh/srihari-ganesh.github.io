<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>expository | Srihari Ganesh</title><link>https://srihari-ganesh.github.io/tag/expository/</link><atom:link href="https://srihari-ganesh.github.io/tag/expository/index.xml" rel="self" type="application/rss+xml"/><description>expository</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 13 Nov 2023 00:00:00 +0000</lastBuildDate><image><url>https://srihari-ganesh.github.io/media/icon_hu238e72ff877ecedb95a45ec9a2f2431b_15216_512x512_fill_lanczos_center_3.png</url><title>expository</title><link>https://srihari-ganesh.github.io/tag/expository/</link></image><item><title>Week 9: Transformations, Gamma/Beta, Conditional Expectation</title><link>https://srihari-ganesh.github.io/stat110/week9/</link><pubDate>Mon, 13 Nov 2023 00:00:00 +0000</pubDate><guid>https://srihari-ganesh.github.io/stat110/week9/</guid><description>&lt;h2 id="0-logistical-info">0. Logistical Info&lt;/h2>
&lt;ul>
&lt;li>Section date: 11/15&lt;/li>
&lt;li>Associated lectures: 11/7, 11/9&lt;/li>
&lt;li>Associated pset: Pset 9, due 11/17&lt;/li>
&lt;li>Office hours on 11/15 from 7-9pm at Quincy Dining Hall&lt;/li>
&lt;li>Remember to fill out the attendance form&lt;/li>
&lt;/ul>
&lt;h3 id="01-summary--practice-problem-pdfs">0.1 Summary + Practice Problem PDFs&lt;/h3>
&lt;p>&lt;a href="https://srihari-ganesh.github.io/uploads/stat110/Section%209%20Handout.pdf" target="_blank">Summary + Practice Problems PDF&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://srihari-ganesh.github.io/uploads/stat110/Section%209%20Solutions.pdf" target="_blank">Practice Problem Solutions PDF&lt;/a>&lt;/p></description></item><item><title>Week 8: Multinomial, Multivariate Normal</title><link>https://srihari-ganesh.github.io/stat110/week8/</link><pubDate>Tue, 07 Nov 2023 00:00:00 +0000</pubDate><guid>https://srihari-ganesh.github.io/stat110/week8/</guid><description>&lt;h2 id="0-logistical-info">0. Logistical Info&lt;/h2>
&lt;ul>
&lt;li>Section date: 11/8&lt;/li>
&lt;li>Associated lectures: 10/31, 11/2&lt;/li>
&lt;li>Associated pset: Pset 8, due 11/10&lt;/li>
&lt;li>Office hours on 11/8 from 9-11pm at Quincy Dining Hall&lt;/li>
&lt;li>Remember to fill out the attendance form&lt;/li>
&lt;/ul>
&lt;h3 id="01-summary--practice-problem-pdfs">0.1 Summary + Practice Problem PDFs&lt;/h3>
&lt;p>&lt;a href="https://srihari-ganesh.github.io/uploads/stat110/Section%208%20Handout.pdf" target="_blank">Summary + Practice Problems PDF&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://srihari-ganesh.github.io/uploads/stat110/Section%208%20Solutions.pdf" target="_blank">Practice Problem Solutions PDF&lt;/a>&lt;/p>
&lt;h2 id="1-multinomial">1. Multinomial&lt;/h2>
&lt;p>We first generalize the notion of Bernoulli trials to many categories; this vocabulary for &amp;ldquo;categorical trials&amp;rdquo; is not standard/necessary for the class, just introduced by me to help define the Multinomial.&lt;/p>
&lt;p>Consider &lt;em>categorical trials&lt;/em>, where the outcome of a trial falls into one of $k$ categories (e.g., the roll of a die has $6$ categories, the flip of a coin has $2$, etc.). Let $\mathbf p \in \mathbb R^k$ be a probability vector (where each entry is in $[0, 1]$ and the entries add up to $p$), where $p_i$ is the probability that the outcome falls into the $i^{\text{th}}$ category. $\newcommand{\Mult}{\mathrm{Mult}}\newcommand{\cov}{\mathrm{Cov}}\newcommand{\Pois}{\mathrm{Pois}}\newcommand{\Bin}{\mathrm{Bin}}$&lt;/p>
&lt;p>&lt;strong>Multinomial story&lt;/strong>: Suppose we run $n$ independent and identically distributed (i.i.d.) categorical trials with $k$ categories and probability vector $\mathbf p$. Let $\mathbf X$ (a $k$-dimensional random vector) count the number of trials that fell into each category. Then $\mathbf X$ is distributed &lt;strong>Multinomial&lt;/strong>: $\mathbf X \sim \Mult_k(n, \mathbf p)$.&lt;/p>
&lt;h3 id="11-multinomial-properties">1.1 Multinomial Properties&lt;/h3>
&lt;ul>
&lt;li>&lt;em>Marginal&lt;/em>: For $\mathbf X \sim \Mult_k(n, \mathbf p)$, $X_j \sim \Bin(n, p_j)$.&lt;/li>
&lt;li>&lt;em>Conditioning&lt;/em>: For $\mathbf X \sim \Mult_k(n, \mathbf p)$,
\begin{align*}
(X_2, \ldots, X_n) | X_1 = x_1 &amp;amp;\sim \Mult_{k-1}(n-x_1, \left(\frac{p_2}{1 - p_1}, \ldots, \frac{p_n}{1-p_1} \right) ).
\end{align*}&lt;/li>
&lt;li>&lt;strong>Lumping&lt;/strong>: Suppose $\mathbf X \sim \Mult_k(n, \mathbf p)$. Then we can group (&lt;strong>lump&lt;/strong>) categories in any way to get a new Multinomial random variable by adding up the associated probabilities. For example, if $(X_1, X_2, X_3, X_4, X_5) \sim \Mult_5\left( n, (p_1, p_2, p_3, p_4, p_5) \right)$, then some valid examples are
\begin{align*}
(X_1+X_4, X_2, X_3+X_5) &amp;amp;\sim \Mult_3\left(n, ( p_1+p_4, p_2, p_3+p_5 )\right),\\
(X_1+X_2, X_3, X_4, X_5) &amp;amp;\sim \Mult_4\left(n, ( p_1+p_2, p_3, p_4, p_5 )\right).
\end{align*}&lt;/li>
&lt;li>&lt;em>Covariance&lt;/em>: For $\mathbf X \sim \Mult_k(n, \mathbf p)$, $\cov(X_i, X_j) = -np_ip_j$.&lt;/li>
&lt;li>&lt;em>Chicken-Egg extension&lt;/em>: Suppose $N \sim \Pois(\lambda)$ and $\mathbf X | N = n \sim \Mult_k (n, \mathbf p)$ where $k, \mathbf p$ don&amp;rsquo;t depend on $n$. Then for $j = 1, 2, \ldots, k$,
\begin{align*}
X_j &amp;amp;\sim \Pois(\lambda p_j).
\end{align*}&lt;/li>
&lt;/ul>
&lt;h2 id="2-multivariate-normal">2. Multivariate Normal&lt;/h2>
&lt;p>Suppose $\mathbf X$ is a $k$-dimensional random vector. Then $\mathbf X$ follows &lt;strong>Multivariate Normal (MVN)&lt;/strong> distribution if for any constants $t_1, \ldots, t_k \in \mathbb R$,
\begin{align*}
t_1 X_1 + \cdots + t_k X_k
\end{align*}
is Normal (where $0$ is consider to follow a &lt;em>degenerate&lt;/em> Normal distribution). The $k=2$ case is called the &lt;strong>Bivariate Normal&lt;/strong>.&lt;/p>
&lt;h3 id="21-multivariate-normal-properties">2.1 Multivariate Normal Properties&lt;/h3>
&lt;ul>
&lt;li>&lt;em>Uncorrelated MVN implies independence&lt;/em>: Suppose $(X, Y)$ is bivariate normal with $\cov(X, Y) = 0$ (i.e., $X$ and $Y$ are uncorrelated). Then $X$ and $Y$ are independent.&lt;br>
More generally, if $\mathbf X$ and $\mathbf Y$ (potentially vectors) are components of the same MVN and $X_i, Y_j$ are uncorrelated for any $i, j$, then $\mathbf X$ and $\mathbf Y$ are independent.&lt;/li>
&lt;/ul>
&lt;div class="alert alert-warning">
&lt;div>
Please note the specific conditions under which this result holds. It is always true that independent random variables are uncorrelated, but the converse is not a general truth. For example, two uncorrelated Normal random variables are not necessarily independent; we could only make that statement if we knew they were components of the same MVN.
&lt;/div>
&lt;/div>
&lt;ul>
&lt;li>&lt;em>Independence of sum and difference&lt;/em>: Suppose $X \sim \mathcal N(\mu_1, \sigma^2)$ and $Y \sim \mathcal N(\mu_2, \sigma^2)$ are independent. Then $X+Y$ and $X-Y$ are also independent.&lt;/li>
&lt;li>&lt;em>Concatenation&lt;/em>: Suppose $\mathbf X = (X_1, \ldots, X_n)$ and $\mathbf Y = (Y_1, \ldots Y_m)$ are both Multivariate Normal with $\mathbf X, \mathbf Y$ independent of each other. Then $(X_1, \ldots, X_n, Y_1, \ldots, Y_m)$ is also Multivariate Normal.&lt;/li>
&lt;li>&lt;em>Subvector&lt;/em>: Suppose $(X, Y, Z)$ is Multivariate Normal. Then $(X, Y)$ is also Multivariate Normal. In general, any subvector of a Multivariate Normal still follows a Multivariate Normal distribution.&lt;/li>
&lt;/ul></description></item><item><title>Week 7: Poisson Processes, Joint Distributions, and Covariance</title><link>https://srihari-ganesh.github.io/stat110/week7/</link><pubDate>Mon, 06 Nov 2023 00:00:00 +0000</pubDate><guid>https://srihari-ganesh.github.io/stat110/week7/</guid><description>&lt;h2 id="0-logistical-info">0. Logistical Info&lt;/h2>
&lt;ul>
&lt;li>Section date: 11/1&lt;/li>
&lt;li>Associated lectures: 10/24, 10/26&lt;/li>
&lt;li>Associated pset: Pset 7, due 11/3&lt;/li>
&lt;li>Office hours on 11/1 from 7-9pm at Quincy Dining Hall&lt;/li>
&lt;li>Remember to fill out the attendance form&lt;/li>
&lt;/ul>
&lt;h3 id="01-summary--practice-problem-pdfs">0.1 Summary + Practice Problem PDFs&lt;/h3>
&lt;p>&lt;a href="https://srihari-ganesh.github.io/uploads/stat110/Section%207%20Handout.pdf" target="_blank">Summary + Practice Problems PDF&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://srihari-ganesh.github.io/uploads/stat110/Section%207%20Solutions.pdf" target="_blank">Practice Problem Solutions PDF&lt;/a>&lt;/p>
&lt;h2 id="1-moment-generating-functions-mgfs">1. Moment Generating Functions (MGFs)&lt;/h2>
&lt;p>For a random variable $X$, the $\mathbf{n^{th}}$ &lt;strong>moment&lt;/strong> is $E(X^n)$.&lt;/p>
&lt;div class="alert alert-note">
&lt;div>
For a random variable $X$, the &lt;strong>moment generating function (MGF)&lt;/strong> is $$M_X(t) = E(e^{tX})$$ for $t \in \mathbb{R}$. If the MGF exists, then
\begin{align*}
M_X(0) &amp;amp;= 1,\\
\frac{d^n}{dt^n} M_X(t) |_{t=0} = M_X^{(n)}(t) &amp;amp;= E(X^n).
\end{align*}
You should sanity-check that $M_X(0) = 1$ whenever you calculate an MGF.
&lt;/div>
&lt;/div>
&lt;p>Useful MGF results:&lt;/p>
&lt;ul>
&lt;li>For independent random variables, $X, Y$ with MGFs $M_X, M_Y$, then $M_{X+Y}(t) = M_X(t) M_Y(t)$.&lt;/li>
&lt;li>For random variable $X$ and scalars $a, b$,
\begin{align*}
M_{a+bX}(t) = e^{at} M_X(bt)
\end{align*} since $M_{a+bX}(t) = E(e^{t(a+bX)}) = e^{at} E(e^{btX})$.&lt;/li>
&lt;/ul>
&lt;div class="alert alert-note">
&lt;div>
&lt;p>A distribution is uniquely determined by any of the following:&lt;/p>
&lt;!-- \begin{enumerate} -->
&lt;ul>
&lt;li>PMF (common for discrete),&lt;/li>
&lt;li>PDF,&lt;/li>
&lt;li>CDF (common for continuous),&lt;/li>
&lt;li>MGF, or&lt;/li>
&lt;li>matching to a named distribution (common).&lt;/li>
&lt;/ul>
&lt;!-- \end{enumerate} -->
&lt;/div>
&lt;/div>
&lt;h2 id="2-poisson-processes">2. Poisson Processes&lt;/h2>
&lt;div class="alert alert-note">
&lt;div>
&lt;p>Consider a problem similar to Blissville/Blotchville, where $T_1, T_2, \ldots,$ represent the arrival times of busses (the amount of time from when we started waiting to when each bus arrives). Then the bus arrival process is a &lt;strong>Poisson process&lt;/strong> with rate $\lambda$ if it satisfies the following conditions:&lt;/p>
&lt;ol>
&lt;li>For any interval in time of length $t &amp;gt; 0$, the number of arrivals in that interval is distributed $\mathrm{Pois}(\lambda t)$.&lt;/li>
&lt;li>For any non-overlapping (disjoint) intervals of time, the number of bus arrivals are independent.
This applies for any &amp;ldquo;arrival process&amp;rdquo; where $T_1, T_2, \ldots$ correspond to arrival times.&lt;/li>
&lt;/ol>
&lt;/div>
&lt;/div>
&lt;div class="alert alert-warning">
&lt;div>
Pay attention to units: $\lambda$ is a rate. So if $\lambda$ has units of arrivals per hour, then $t$ should have units of hours.
&lt;/div>
&lt;/div>
&lt;p>Results:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Inter-arrival times&lt;/strong>: In a Poisson process with rate $\lambda$, the inter-arrival times (the time for the first arrival, $T_1$, and the times between consecutive arrives $T_2-T_1, T_3-T_2, \ldots$) are each independently distributed
\begin{align*}
T_1, T_2-T_1, T_3-T_2, \ldots \stackrel{i.i.d.}{\sim} \mathrm{Expo}(\lambda).
\end{align*}&lt;/li>
&lt;/ul>
&lt;div class="alert alert-warning">
&lt;div>
Additionally note that $T_2, T_3, \ldots,$ are &lt;em>not&lt;/em> exponentially distributed. In fact, they follow Gamma distributions (which we will introduce soon): $T_n \sim \mathrm{Gamma}(n, \lambda)$.
&lt;/div>
&lt;/div>
&lt;ul>
&lt;li>&lt;strong>Count-time duality&lt;/strong>: Fix a time $t &amp;gt; 0$. Let $N_t$ be the number of arrivals in the time interval $[0, t]$, and let $T_n$ be the arrival time of the $n$-th arrival. Then
\begin{align*}
(T_n &amp;gt; t) = (N_t &amp;lt; n).
\end{align*}&lt;/li>
&lt;/ul>
&lt;h2 id="3-marginal-conditional-and-joint-distributions">3. Marginal, Conditional, and Joint Distributions&lt;/h2>
&lt;div class="alert alert-note">
&lt;div>
&lt;p>&lt;em>Marginal, conditional, and joint distributions&lt;/em>&lt;/p>
&lt;p>Consider two random variables $X, Y$.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Joint&lt;/th>
&lt;th>Marginal&lt;/th>
&lt;th>Conditional&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Distribution&lt;/td>
&lt;td>$(X, Y)$&lt;/td>
&lt;td>$X$&lt;/td>
&lt;td>$X\vert Y=y$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PMF&lt;/td>
&lt;td>$P(X = x, Y = y)$&lt;/td>
&lt;td>$P(X = x)$&lt;/td>
&lt;td>$P(X = x\vert Y = y)$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CDF&lt;/td>
&lt;td>$P(X \le x, Y \le y)$&lt;/td>
&lt;td>$P(X \le x)$&lt;/td>
&lt;td>$P(X \le x \vert Y = y)$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>For example, $P(X \vert Y = y)$ is a marginal PMF. All of these apply if we flip $X$ and $Y$, and PDFs follow analogously from PMFs.&lt;/p>
&lt;/div>
&lt;/div>
&lt;ul>
&lt;li>&lt;strong>Marginalization&lt;/strong>:
If we know the joint distribution of random variables $(X, Y)$, then we can find the marginal distribution of $X$ (and analogously, $Y$) by LOTP:
\begin{align*}
P(X = x) &amp;amp;= \sum_y P(X = x, Y = y), &amp;amp; \text{$X,Y$ discrete}.\\
f_X(x) &amp;amp;= \int_{-\infty}^\infty f_{X,Y}(x, y), &amp;amp; \text{$X,Y$ continuous}.
\end{align*}&lt;/li>
&lt;/ul>
&lt;div class="alert alert-warning">
&lt;div>
Note that marginal distributions of $X$ and $Y$ are not sufficient (not enough information) to find the joint distribution of $X, Y$.
&lt;/div>
&lt;/div>
&lt;ul>
&lt;li>&lt;em>Joint from marginal and conditional&lt;/em>:
If we know the marginal distribution of $X$ and the conditional distributions $Y | X=x$ for any $x$, then we can find the joint distribution of $(X, Y)$ by factoring out our probability:
\begin{align*}
P(X = x, Y = y) &amp;amp;= P(X = x) P(Y = y | X = x), &amp;amp; \text{$X, Y$ discrete.}\\
f_{X, Y} (x, y) &amp;amp;= f_{X}(x) f_{Y|X=x} (y), &amp;amp; \text{ $X, Y$ continuous.}
\end{align*}&lt;/li>
&lt;/ul>
&lt;div class="alert alert-note">
&lt;div>
&lt;strong>Independence of random variables&lt;/strong>:
Random variables $X, Y$ are &lt;strong>independent&lt;/strong> if for all $x$ and $y$, any of the following hold (they imply each other, if valid):
\begin{align*}
F_{X, Y} (x, y) = P(X \le x, Y \le Y) &amp;amp;= P(X \le x) P(Y \le Y) = F_X(x)F_Y(y), &amp;amp; \text{ CDFs for any $X, Y$.}\\
P(X = x, Y = y) &amp;amp;= P(X = x) P(Y = y), &amp;amp; \text{PMFs for discrete $X, Y$.}\\
f_{X, Y} (x, y) &amp;amp;= f_X(x) f_Y(y), &amp;amp;\text{PDFs for continuous, $X, Y$.}
\end{align*}
&lt;/div>
&lt;/div>
&lt;ul>
&lt;li>&lt;em>2D LOTUS&lt;/em>:
Let $X, Y$ be random variables with known joint distribution. For $g: \mathrm{support}(X) \times \mathrm{support}(Y) \to \mathbb R$, LOTUS extends to 2 dimensions (or analogously for any larger dimensions) to give
\begin{align*}
E(g(X, Y)) &amp;amp;= \begin{cases}
\sum_x \sum_y g(x, y) P(X = x, Y = y), &amp;amp; \text{ $X, Y$ discrete}\\
\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x, y) f_{X, Y}(x, y)dx dy, &amp;amp; \text{ $X, Y$ continuous}.
\end{cases}
\end{align*}&lt;/li>
&lt;/ul>
&lt;h2 id="4-covariance-and-correlation">4. Covariance and Correlation&lt;/h2>
&lt;div class="alert alert-note">
&lt;div>
&lt;p>&lt;strong>Covariance and Correlation&lt;/strong>&lt;/p>
&lt;p>$\newcommand{\cov}{\mathrm{Cov}}\newcommand{\corr}{\mathrm{Corr}}\newcommand{\var}{\mathrm{Var}}\newcommand{\sd}{\mathrm{SD}}$
The &lt;strong>covariance&lt;/strong> of random variables $X, Y$ is
\begin{align*}
\cov(X, Y) &amp;amp;= E\left( \left[X - EX \right] \left[Y - EY\right]\right)
\end{align*}
where $EX$ is shorthand for $E(X)$. Equivalently,
\begin{align*}
\cov(X, Y) &amp;amp;= E(XY) - E(X) E(Y).
\end{align*}&lt;/p>
&lt;p>The &lt;strong>correlation&lt;/strong> of random variables $X, Y$ is
\begin{align*}
\corr(X, Y) &amp;amp;= \frac{\cov(X, Y)}{\sqrt{\var(X) \var(Y)}}\
&amp;amp;= \frac{\cov(X, Y)}{\sd(X)\sd(Y)},
\end{align*}
where $\sd(X) = \sqrt{\var(X)}$ is the standard deviation of $X$. Equivalently, we first standardize $X$ and $Y$, then find their covariance:
\begin{align*}
\corr(X, Y) &amp;amp;= \cov\left( \frac{X - E(X)}{\sd(X)}, \frac{Y - E(Y)}{\sd (Y)} \right).
\end{align*}&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>$X$ and $Y$ are&lt;/p>
&lt;ul>
&lt;li>&lt;strong>positively correlated&lt;/strong> if $\corr(X, Y) &amp;gt; 0$,&lt;/li>
&lt;li>&lt;strong>negatively correlated&lt;/strong> if $\corr(X, Y) &amp;lt; 0$,&lt;/li>
&lt;li>&lt;strong>uncorrelated&lt;/strong> if $\corr(X, Y) = 0$.&lt;/li>
&lt;/ul>
&lt;p>Since correlation and covariance have the same sign, this also applies for positive/negative/zero covariance.&lt;/p>
&lt;p>&lt;strong>Properties of covariance&lt;/strong>: see page 327 in Blitzstein &amp;amp; Huang for full list. Let $X, Y, W, Z$ be random variables, as well as those of the form $X_1, X_2, \ldots,$.&lt;/p>
&lt;ul>
&lt;li>If $X, Y$ are independent, then $\cov(X, Y) = 0$ (so $X, Y$ are uncorrelated).&lt;/li>
&lt;li>$\cov(X, X) = \var(X)$.&lt;/li>
&lt;li>$\var(\sum_i X_i) = \sum_i \var(X_i) + \sum_{i&amp;lt;j} 2 \cov(X_i, X_j)$.
&lt;ul>
&lt;li>This can be especially useful for finding the variance of a sum of indicators.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>\begin{align*}\cov(X+Y, W+Z) &amp;amp;= \cov(X, W) + \cov(X, Z)\\ &amp;amp;+ \cov(Y, W) + \cov(Y, Z).\end{align*}&lt;/li>
&lt;li>$\cov(aX, bY) = ab \cov(X, Y)$.&lt;/li>
&lt;/ul>
&lt;p>The last two properties are referred to as &lt;strong>bilinearity&lt;/strong>.&lt;/p>
&lt;p>&lt;strong>Properties of correlation&lt;/strong>
Let $X, Y$ be random variables.&lt;/p>
&lt;ul>
&lt;li>-If $X, Y$ are independent, then $\corr(X, Y) = 0$ (so $X, Y$ are uncorrelated)&lt;/li>
&lt;li>$-1 \le \corr(X, Y) \le 1$.&lt;/li>
&lt;/ul>
&lt;div class="alert alert-warning">
&lt;div>
&lt;strong>Uncorrelated does NOT imply independent&lt;/strong>: In the previous two results, we noted independent random variables have zero correlation and zero covariance. However, the converse does not apply: uncorrelated random variables are not necessarily independent.
&lt;/div>
&lt;/div></description></item><item><title>Week 6: Universality of the Uniform, Normal, Expo, and Moments</title><link>https://srihari-ganesh.github.io/stat110/week6/</link><pubDate>Sun, 22 Oct 2023 00:00:00 +0000</pubDate><guid>https://srihari-ganesh.github.io/stat110/week6/</guid><description>&lt;h2 id="0-logistical-info">0. Logistical Info&lt;/h2>
&lt;ul>
&lt;li>Section date: 10/25&lt;/li>
&lt;li>Associated lectures: 10/17, 10/19&lt;/li>
&lt;li>Associated pset: Pset 6, due 10/27&lt;/li>
&lt;li>Office hours on 10/25 from 7-9pm at Quincy Dining Hall&lt;/li>
&lt;li>Please reach out if you wanted to sign up for a midterm debrief and missed the chance&lt;/li>
&lt;li>Remember to fill out the attendance form&lt;/li>
&lt;/ul>
&lt;h3 id="01-summary--practice-problem-pdfs">0.1 Summary + Practice Problem PDFs&lt;/h3>
&lt;p>&lt;a href="https://srihari-ganesh.github.io/uploads/stat110/Section%206%20Handout.pdf" target="_blank">Summary + Practice Problems PDF&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://srihari-ganesh.github.io/uploads/stat110/Section%206%20Solutions.pdf" target="_blank">Practice Problem Solutions PDF&lt;/a>&lt;/p>
&lt;h2 id="1-universality-of-the-uniform">1. Universality of the Uniform&lt;/h2>
&lt;p>Recall that the standard uniform, $U \sim \mathrm{Unif}(0, 1)$, has support $(0, 1)$ with PDF $1$ in the support.&lt;/p>
&lt;div class="alert alert-note">
&lt;div>
&lt;p>&lt;strong>Universality of the Uniform (UoU)&lt;/strong>: If $F$ is a valid CDF that is continuous and strictly increasing over the support, then&lt;/p>
&lt;ol>
&lt;li>Let $U \sim \mathrm{Unif}(0, 1)$. Then $F^{-1} (U)$ is a random variable with CDF $F$.&lt;/li>
&lt;li>Let $X$ have CDF $F$. Then $F(X) \sim \mathrm{Unif}(0,1)$.&lt;/li>
&lt;/ol>
&lt;p>The first result applies to discrete random variables as well. The second result only works for continuous random variables.&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>This result is quite useful for simulation - if you have access to draws from a Uniform distribution, then you can transform them into draws from any distribution with a known (inverse) CDF.&lt;/p>
&lt;p>We can prove UoU with the tools we&amp;rsquo;ve learned in class. For continuous random variables with $F$ as described in the theorem,&lt;/p>
&lt;ol>
&lt;li>For $x \in \mathbb{R}$,
\begin{align*}
P(F^{-1}(U) &amp;lt; x) = P(F(F^{-1}(U)) &amp;lt; F(x)) = P(U &amp;lt; F(x)) = F(x).
\end{align*}
So $F^{-1}(U)$ has CDF $F$. We used the CDF of $U$ in the last step, since $F(x) \in [0, 1]$.&lt;/li>
&lt;li>For $u \in [0, 1]$,
\begin{align*}
P(F(X) &amp;lt; u) &amp;amp;= P\left(F^{-1}(F(X)) &amp;lt; F^{-1}(u)\right)\\
&amp;amp;= P(X &amp;lt; F^{-1}(u)) = F(F^{-1}(u)) = u,
\end{align*}
so $F(X) \sim \mathrm{Unif}(0, 1)$ since it has the CDF of a standard uniform.&lt;/li>
&lt;/ol>
&lt;h2 id="2-normal-distribution">2. Normal distribution&lt;/h2>
&lt;h3 id="21-standard-normal">2.1 Standard Normal&lt;/h3>
&lt;p>$Z \sim \mathcal{N}(0, 1)$ is a &lt;strong>standard Normal&lt;/strong> random variable with support $\mathbb R$. We notate the CDF as $\Phi$ and PDF as $\phi$.&lt;/p>
&lt;ul>
&lt;li>(&lt;em>Symmetry&lt;/em>) The standard Normal is symmetric about $0$. In math, for $x \in \mathbb R$, $\phi(x) = \phi(-x)$.
&lt;ul>
&lt;li>This also implies that $\Phi(x) = 1 - \Phi(-x)$.&lt;/li>
&lt;li>So $\Phi(0) = 0.5$.&lt;/li>
&lt;li>For $Z \sim \mathcal{N}(0, 1)$, $-Z \sim \mathcal{N}(0, 1)$ as well.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>(&lt;em>Empirical rule/68-95-99.7 rule&lt;/em>)
\begin{align*}
P(-1 &amp;lt; Z &amp;lt; 1) &amp;amp;\approx 0.68,\\
P(-2 &amp;lt; Z &amp;lt; 2) &amp;amp;\approx 0.95,\\
P(-3 &amp;lt; Z &amp;lt; 3) &amp;amp;\approx 0.997.
\end{align*}&lt;/li>
&lt;/ul>
&lt;p>In this class, you can give exact answers in terms of $\Phi$ and $\phi$. On psets, you should also use a calculator/programming language/the empirical rule to get numerical approximations of $\Phi$.&lt;/p>
&lt;h3 id="22-normal">2.2 Normal&lt;/h3>
&lt;p>$X \sim \mathcal{N}(\mu, \sigma^2)$ (with $\mu \in \mathbb R, \sigma &amp;gt; 0$) is a &lt;strong>Normal&lt;/strong> random variable with mean $\mu$ and variance $\sigma^2$, and also has support $\mathbb R$.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>(&lt;em>Location-scale&lt;/em>)
For $Z \sim \mathcal{N}(0, 1)$, $\mu + \sigma Z \sim \mathcal{N}(\mu, \sigma^2)$.&lt;/p>
&lt;p>More generally, for $X \sim \mathcal{N}(\mu_1, \sigma_1^2)$, $\mu_2 + \sigma_2 X \sim \mathcal{N}(\mu_2 + \mu_1 \sigma_2, \sigma_1^2 \sigma_2^2)$.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>(&lt;em>Standardization&lt;/em>)
For $X \sim \mathcal{N}(\mu, \sigma^2)$, $\frac{X-\mu}{\sigma} \sim \mathcal{N}(0, 1)$.\
We often use this to get results in terms of $\Phi$:
\begin{align*}
P(X &amp;lt; x) = P(\frac{X-\mu}{\sigma} &amp;lt; \frac{x-\mu}{\sigma}) = \Phi(\frac{x-\mu}{\sigma}).
\end{align*}&lt;/p>
&lt;/li>
&lt;li>
&lt;p>(&lt;em>Empirical rule&lt;/em>) For $X \sim \mathcal{N}(\mu, \sigma^2)$,
\begin{align*}
P(\mu-\sigma &amp;lt; X &amp;lt; \mu+\sigma) &amp;amp;\approx 0.68\\
P(\mu-2\sigma &amp;lt; X &amp;lt; \mu+2\sigma) &amp;amp;\approx 0.95\\
P(\mu-3\sigma &amp;lt; X &amp;lt; \mu+3\sigma) &amp;amp;\approx 0.997
\end{align*}&lt;/p>
&lt;/li>
&lt;li>
&lt;p>(&lt;em>Sum of independent Normals&lt;/em>)
Let $X \sim \mathcal{N}(\mu_1, \sigma_1^2)$ and $Y \sim \mathcal{N}(\mu_2, \sigma_2^2)$ with $X, Y$ independent. Then
\begin{align*}
X + Y &amp;amp;\sim \mathcal{N}(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2),\\
X - Y &amp;amp;\sim \mathcal{N}(\mu_1 - \mu_2, \sigma_1^2 + \sigma_2^2).
\end{align*}&lt;/p>
&lt;/li>
&lt;/ul>
&lt;div class="alert alert-warning">
&lt;div>
(&lt;em>Variance when subtracting&lt;/em>)
See that we always add the variance above! This is also a general rule: for any independent random variables $X$ and $Y$,
\begin{align*}
Var(X+Y) = Var(X - Y) = Var(X) + Var(Y).
\end{align*}
See that this is consistent with the fact that $Var(-Y) = (-1)^2 Var(Y) = Var(Y)$.
&lt;/div>
&lt;/div>
&lt;h2 id="3-exponential-distribution">3. Exponential distribution&lt;/h2>
&lt;p>$X \sim \mathrm{Expo}(\lambda)$ is an &lt;strong>Exponential&lt;/strong> random variable with mean $\frac{1}{\lambda}$ and variance $\frac{1}{\lambda^2}$. $\lambda$ is called the &lt;strong>rate parameter&lt;/strong>.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>(&lt;em>Memorylessness&lt;/em>)
For $X \sim \mathrm{Expo}(\lambda)$ and any $s, t &amp;gt; 0$, the &lt;strong>memoryless&lt;/strong> property of the Exponential distribution states the following (equivalent) results:
\begin{align*}
P(X &amp;gt; s + t \vert X &amp;gt; s) &amp;amp;= P(X &amp;gt; t)\\
(X - s \vert X &amp;gt; s) &amp;amp;\sim \mathrm{Expo}(\lambda).
\end{align*}
See specifically that $X-s | X&amp;gt;s$ is independent of the value of $s$.&lt;/p>
&lt;p>The Exponential distribution is the only continuous distribution with this property. Additionally, the Geometric distribution is the only discrete distribution with support ${0, \ldots, }$ that is memoryless.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;div class="alert alert-warning">
&lt;div>
&lt;p>For most results we talk about, you can&amp;rsquo;t put a random variable in the place of a constant - you might recall from last week&amp;rsquo;s problem set that we couldn&amp;rsquo;t let the sum of $N$ independent $\mathrm{Pois}(\lambda)$ r.v.s, with $N$ random, be distributed $\mathrm{Pois}(N\lambda)$. However, with memorylessness, you can put random variables in the place of the $s$ above - so for some random variable $Y$, $P(X &amp;gt; t +Y | X &amp;gt; Y) = P(X &amp;gt; t)$ and $(X-Y|X &amp;gt; Y) \sim \mathrm{Expo}(\lambda)$ still.&lt;/p>
&lt;details class="spoiler " id="spoiler-0">
&lt;summary>&lt;ins>Click for proof&lt;/ins>&lt;/summary>
&lt;p>We can prove by using LOTP and applying the constant version of memorylessness. We&amp;rsquo;ll assume $Y$ is discrete here, but continuous case is analogous (swap sums for integrals, PMFs for PDFs).
\begin{align*}
P(X &amp;gt; t+Y | X &amp;gt; Y) &amp;amp;= \sum_{y} P(X &amp;gt; t+y | X &amp;gt; Y, Y = y) P(Y=y)\\
&amp;amp;= \sum_{y} P(X &amp;gt; t+y | X &amp;gt; y, Y = y) P(Y=y).
\end{align*}
We&amp;rsquo;ll take a brief sidebar to show that $P(X &amp;gt; t + y | X &amp;gt; y , Y = y) = P(X &amp;gt; t+y | X&amp;gt;y)$; I think you can jump from the former to the latter using unconditional independence of $X$ and $Y$ since the extra condition is a function of $X$, but we&amp;rsquo;ll be explicit here. We will use the definition of conditional probability, the fact that $X&amp;gt;t+y$ implies that $X&amp;gt;y$, and the unconditional independence of $X$ and $Y$.
\begin{align*}
P(X &amp;gt; t+y | X &amp;gt; y, Y = y) &amp;amp;= \frac{P(X &amp;gt; t+y, X &amp;gt; y, Y =y)}{P(X &amp;gt; y, Y = y)}\\
&amp;amp;= \frac{P(X &amp;gt; t+y, Y =y)}{P(X &amp;gt; y ,Y =y)}\\
&amp;amp;= \frac{P(X &amp;gt; t+y) P(Y = y) }{P(X&amp;gt;y)P(Y=y)}\\
&amp;amp;= \frac{P(X&amp;gt;t+y)}{P(X&amp;gt;y)}\\
&amp;amp;= \frac{P(X&amp;gt;t+y, X&amp;gt;y)}{P(X&amp;gt;y)}\\
&amp;amp;= P(X&amp;gt;t+y | X&amp;gt;y).
\end{align*}
With this information,
\begin{align*}
P(X &amp;gt; t+Y | X &amp;gt;Y) &amp;amp;= \sum_y P(X&amp;gt;t+y|X&amp;gt;y, Y=y) P(Y=y)\\
&amp;amp;= \sum_y P(X&amp;gt;t+y|X&amp;gt;y) P(Y=y)\\
&amp;amp;= \sum_y P(X&amp;gt;t) P(Y=y)\\
&amp;amp;= P(X&amp;gt;t) \sum_y P(Y=y)\\
&amp;amp;= P(X&amp;gt;t) (1) = P(X&amp;gt;t),
\end{align*}
where we use memorylessness to say $P(X&amp;gt;t+y | X&amp;gt;y) = P(X&amp;gt;t)$.&lt;/p>
&lt;/details>
&lt;/div>
&lt;/div>
&lt;ul>
&lt;li>(&lt;em>Example of Memorylessness&lt;/em>)
Suppose you&amp;rsquo;re waiting for a bus that will arrive in $X \sim \mathrm{Expo}(\lambda)$ minutes. If you wait for the bus for 10 minutes and it has not arrived, then the remaining time that you have to wait is still distributed $\mathrm{Expo}(\lambda)$: $X - 10 | X &amp;gt; 10 \sim \mathrm{Expo}(\lambda)$. So no matter how long you wait, the remaining time for you to wait has the same distribution.&lt;/li>
&lt;li>(&lt;em>Minimum of Expos&lt;/em>)
The minimum of $n$ i.i.d. $\mathrm{Expo}(\lambda)$ random variables is distributed $\mathrm{Expo}(n\lambda)$. In notation, for $X_1, \ldots, X_n \overset{i.i.d.}{\sim} \mathrm{Expo}(\lambda)$, $\min(X_1, \ldots, X_n) \sim \mathrm{Expo}(n\lambda)$.&lt;/li>
&lt;/ul>
&lt;div class="alert alert-warning">
&lt;div>
&lt;p>&lt;strong>Maximum of Expos&lt;/strong>&lt;/p>
&lt;p>The maximum of $n$ i.i.d. Exponential distributions is &lt;em>not&lt;/em> does not follow an Exponential distribution.&lt;/p>
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
&lt;p>&lt;strong>Finding the distribution of minimums/maximums&lt;/strong>&lt;/p>
&lt;p>The results above can be found in the book, but they provide a general template for finding the distributions of minimums and maximums.&lt;/p>
&lt;p>Let $X_1, \ldots, X_n$ be any random variables. Then the events ${\min(X_1, \ldots, X_n) &amp;gt; x}$ and $(X_1 &amp;gt; x) \cap (X_2 &amp;gt; x) \cap \cdots \cap (X_n &amp;gt; x)$ are equivalent. To convince yourself of this, think about what this means in words: the minimum of a set of numbers is greater than $x$ if and only if each one of the numbers is great than $x$.&lt;/p>
&lt;p>To find the CDF of $\min(X_1, \ldots, X_n)$, a common workflow is
\begin{align*}
P(\min(X_1, \ldots, X_n) \le x) &amp;amp;= 1 - P(\min(X_1, \ldots, X_n) &amp;gt; x) = 1 - P(X_1 &amp;gt; x, X_2 &amp;gt; x, \ldots, X_n &amp;gt; x).
\end{align*}
If $X_1, \ldots, X_n$ are independent, then we can get that
\begin{align*}
P(X_1 &amp;gt; x, X_2 &amp;gt; x, \ldots, X_n &amp;gt; x) &amp;amp;= P(X_1 &amp;gt; x) P(X_2 &amp;gt; x) \cdots P(X_n &amp;gt; x)
\end{align*}
If $X_1, \ldots, X_n$ are also identically distributed, we conclude with
\begin{align*}
P(X_1 &amp;gt; x) P(X_2 &amp;gt; x) \cdots P(X_n &amp;gt; x) &amp;amp;= (P(X_1 &amp;gt; x))^n.
\end{align*}&lt;/p>
&lt;p>For maximums, we follow a similar workflow, except instead using the fact that $${\max(X_1, \ldots, X_n) &amp;lt; x} = \bigcap_{i=1}^n (X_i &amp;lt; x).$$&lt;/p>
&lt;/div>
&lt;/div>
&lt;h2 id="4-momentsmoment-generating-functions">4. Moments/Moment Generating Functions&lt;/h2>
&lt;p>For a random variable $X$, the $\mathbf{n^{th}}$ &lt;strong>moment&lt;/strong> is $E(X^n)$.&lt;/p>
&lt;div class="alert alert-note">
&lt;div>
&lt;p>&lt;strong>Moment Generating Function&lt;/strong>&lt;/p>
&lt;p>For a random variable $X$, the &lt;strong>moment generating function (MGF)&lt;/strong> is $M_X(t) = E(e^{tX})$ for $t \in \mathbb{R}$. If the MGF exists, then
\begin{align*}
M_X(0) &amp;amp;= 1,\
\frac{d^n}{dt^n} M_X(t) |_{t=0} = M_X^{(n)}(t) &amp;amp;= E(X^n).
\end{align*}
You should sanity-check that $M_X(0) = 1$ whenever you calculate an MGF.&lt;/p>
&lt;/div>
&lt;/div></description></item><item><title>Week 5: Continuous Distributions</title><link>https://srihari-ganesh.github.io/stat110/week5/</link><pubDate>Sat, 14 Oct 2023 00:00:00 +0000</pubDate><guid>https://srihari-ganesh.github.io/stat110/week5/</guid><description>&lt;h2 id="0-logistical-info">0. Logistical Info&lt;/h2>
&lt;ul>
&lt;li>Section date: 10/18&lt;/li>
&lt;li>Associated lecture: 10/12&lt;/li>
&lt;li>Associated pset: Pset 5, due 10/20&lt;/li>
&lt;li>Office hours on 10/18 from 7-9pm at Quincy Dining Hall&lt;/li>
&lt;li>Remember to fill out the attendance form&lt;/li>
&lt;/ul>
&lt;h3 id="01-summary--practice-problem-pdfs">0.1 Summary + Practice Problem PDFs&lt;/h3>
&lt;p>&lt;a href="https://srihari-ganesh.github.io/uploads/stat110/Section%205%20Handout.pdf" target="_blank">Summary + Practice Problems PDF&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://srihari-ganesh.github.io/uploads/stat110/Section%205%20Solutions.pdf" target="_blank">Practice Problem Solutions PDF&lt;/a>&lt;/p>
&lt;h2 id="1-continuous-random-variables">1. Continuous Random Variables&lt;/h2>
&lt;p>A &lt;strong>continuous&lt;/strong> random variable has an interval for its support.&lt;/p>
&lt;ul>
&lt;li>More precisely: a continuous random variable has an uncountable support, while discrete random variables have finite/countably infinite supports.&lt;/li>
&lt;/ul>
&lt;p>We heavily lean on the &lt;strong>cumulative distribution function (CDF)&lt;/strong>: for any random variable $X$, the CDF $F: \mathbb R \to [0, 1]$ is defined $F(x) = P(X \le x)$.&lt;/p>
&lt;p>We don&amp;rsquo;t use the probability mass function (PMF) anymore, because $P(X = x) = 0$ for any $x$. This probability $0$ does not mean &amp;ldquo;impossible,&amp;rdquo; though. We instead use the probability density function:&lt;/p>
&lt;div class="alert alert-note">
&lt;div>
The &lt;strong>probability density function&lt;/strong> of a random variable $X$ with CDF $F$ is a function $f: \mathbb{R} \to \mathbb{R}$:
$$
f(x) = \frac{d}{dx} F(x)
$$
Probability densities are the continuous analog to probabilities, but they are NOT probabilities. A valid PDF is nonnegative and satisfies
$$
\int_{-\infty}^\infty f(x) dx = 1
$$
&lt;/div>
&lt;/div>
&lt;h3 id="11-uses-of-cdfs-and-pdfs">1.1 Uses of CDFs and PDFs&lt;/h3>
&lt;p>For &lt;em>any&lt;/em> random variable $X$ (continuous or discrete), you can use the CDF to calculate the following:
\begin{align*}
P(X &amp;gt; x) &amp;amp;= 1 - P(X \le x) = 1-F(x)\\
P(x_1 &amp;lt; X \le x_2) &amp;amp;= P(X \le x_2) - P(X \le x_1) = F(x_2) - F(x_1)
\end{align*}
For the CDFs of continuous random variables,&lt;/p>
&lt;ul>
&lt;li>You can assume the CDF is differentiable.&lt;/li>
&lt;li>$P(X \le x) = P(X &amp;lt; x)$, so you can swap out $\le$ and $&amp;lt;$ in calculations like the above.&lt;/li>
&lt;/ul>
&lt;p>For a continuous random variable, we can find the probabilities of intervals by integrating the PDF and adjusting the bounds:
\begin{align*}
P(X \le x) = P(X &amp;lt; x) &amp;amp;= \int_{-\infty}^x f(x) dx\\
P(X \ge x) = P(X &amp;gt; x) &amp;amp;= \int_{x}^{\infty} f(x) dx\\
P(x_1 &amp;lt; X &amp;lt; x_2) &amp;amp;= \int_{x_1}^{x_2} f(x) dx.
\end{align*}&lt;/p>
&lt;h3 id="12-continuous-analogs-of-all-of-our-tools">1.2 Continuous analogs of all of our tools&lt;/h3>
&lt;p>The general rules are:&lt;/p>
&lt;ul>
&lt;li>Integrals instead of sums&lt;/li>
&lt;li>PDFs instead of PMFs&lt;/li>
&lt;/ul>
&lt;p>So here&amp;rsquo;s a table with the tools we&amp;rsquo;ve talked about:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Tool&lt;/th>
&lt;th>Discrete&lt;/th>
&lt;th>Continuous&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Expectation&lt;/td>
&lt;td>$E(X) = \sum_{x} x P(X = x)$&lt;/td>
&lt;td>$E(X) = \int_{-\infty}^{\infty} x f_X(x) dx$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LOTUS&lt;/td>
&lt;td>$E(g(X)) = \sum_x g(x) P(X = x)$&lt;/td>
&lt;td>$E(g(X)) = \int_{-\infty}^\infty g(x) f_X(x) dx$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Bayes&amp;rsquo; rule&lt;/td>
&lt;td>$P(X = x \vert Y = y) = \frac{P(Y=y \vert X = x) P(X = x)}{P(A)}$&lt;/td>
&lt;td>$f_{X\vert Y=y}(x) = \frac{f_{Y\vert X=x}(y) f_X(x)}{f_Y(y)}$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="2-uniform">2. Uniform&lt;/h2>
&lt;p>For any interval $(a, b)$, we can define a uniform distribution with that support, denoted $U \sim \mathrm{Unif}(a, b)$. A uniform distribution is equivalent to having a constant PDF over the support. There is no uniform whose support is the full real line.
\begin{align*}
f_U(x) &amp;amp;= \begin{cases}
\frac{1}{b-a} &amp;amp; x \in (a, b)\\
0 &amp;amp; x \notin (a, b)
\end{cases}\\
F_U(x) = P(U &amp;lt; x) &amp;amp;= \begin{cases}
0 &amp;amp; x \le a\\\
\frac{x-a}{b-a} &amp;amp; x \in (a, b)\\
1 &amp;amp; x \ge b
\end{cases}
\end{align*}&lt;/p></description></item><item><title>Week 4: Discrete Distributions and Expectation</title><link>https://srihari-ganesh.github.io/stat110/week4/</link><pubDate>Mon, 02 Oct 2023 00:00:00 +0000</pubDate><guid>https://srihari-ganesh.github.io/stat110/week4/</guid><description>&lt;h2 id="0-logistical-info">0. Logistical Info&lt;/h2>
&lt;ul>
&lt;li>Section date: 10/4&lt;/li>
&lt;li>Associated lectures: 9/24, 9/26, 10/3&lt;/li>
&lt;li>Associated pset: Pset 4, due 10/6&lt;/li>
&lt;li>Midterm: 10/10&lt;/li>
&lt;li>Office hours on 10/4 from 7-9pm at Quincy Dining Hall&lt;/li>
&lt;li>Exam office hours on 10/7 and 10/9 from 8-10pm at Quincy Dining Hall&lt;/li>
&lt;li>Remember to fill out the attendance form&lt;/li>
&lt;li>Given the structure of my section, I&amp;rsquo;m shifting away from a lot of explanation on this webpage. I may come back in the future and add more examples, but it doesn&amp;rsquo;t make much sense since we&amp;rsquo;re mainly doing practice problems in section. So this week, I have no concise summary section on the webpage because it&amp;rsquo;s all pretty tight - check out the handout below if you want it.&lt;/li>
&lt;/ul>
&lt;h3 id="01-summary--practice-problem-pdfs">0.1 Summary + Practice Problem PDFs&lt;/h3>
&lt;p>&lt;a href="https://srihari-ganesh.github.io/uploads/stat110/Section%204%20Handout.pdf" target="_blank">Summary + Practice Problems PDF&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://srihari-ganesh.github.io/uploads/stat110/Section%204%20Solutions.pdf" target="_blank">Practice Problem Solutions PDF&lt;/a>&lt;/p>
&lt;h2 id="1-random-variables">1. Random variables&lt;/h2>
&lt;p>Let&amp;rsquo;s use the precise mathematical definition from last time: &lt;strong>random variables&lt;/strong> assign real numbers to possible outcomes of an experiment. In other words, they map the sample space to the real line. So for a random variable $X$, for every outcome in the sample space, $\omega \in S$, there is a corrsponding real number, $X(\omega) \in \mathbb{R}$.&lt;/p>
&lt;p>Here&amp;rsquo;s the terminology of a random variable that we&amp;rsquo;ve talked about thus far, where we continue using $X$ as an example of a random variable.&lt;/p>
&lt;ul>
&lt;li>The &lt;strong>support&lt;/strong>: what is the set of values that a random variable can take on? This is equivalent to the image/range of of $X$ on $S$, $X(S)$.&lt;/li>
&lt;li>For a named distribution like a Binomial, we say $X$ is distributed Binomial using $X \sim \mathrm{Bin}(n, p)$, where we have to set possible values of the parameters $n$ and $p$ for our specific problem. You CANNOT set $X = \mathrm{Bin}(n, p)$: named distributions cannot equal random variables, they are just a blueprint for what the random variable looks like.&lt;/li>
&lt;li>The &lt;strong>probability mass function (PMF)&lt;/strong>: for any real number $x$ (or $t$ or $y$, it&amp;rsquo;s just a filler variable), what is the probability that $X$ takes on this value? This is notated $P(X = x)$.
&lt;ul>
&lt;li>You should address every possible value of $x$: $P(X = x) = 0$ if $x$ is not in the support of $X$, $\sum_{x \in \text{ support of } X } P(X = x) = 1$, and every probability should be valid (nonnegative, between $0$ and $1$ inclusive).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;em>NEW:&lt;/em> the &lt;strong>cumulative density function (CDF)&lt;/strong>: for any real number $x$, what is the probability that $X$ takes on a value that is less than or equal to $x$? This is notated $P(X \le x)$.
&lt;ul>
&lt;li>You should again address every possible value of $x$, both in and outside of the support.&lt;/li>
&lt;li>Here, the requirements for a valid CDF are that $P(X \le x) = 0$ if $x$ is less than the smallest value in the support and $P(X \le x) = 1$ if $x$ is greater than the biggest value in the support. For an infinite support, we should have $P(X \le x) \to 0$ as $x \to -\infty$ and $P(X \le x) \to 1$ as $x \to \infty$.&lt;/li>
&lt;li>Additionally, a CDF should be non-decreasing (i.e., either increasing or a flat line).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>We often abbreviate to say random variables are &lt;strong>independent and identically distributed (i.i.d.)&lt;/strong>.&lt;/li>
&lt;/ul>
&lt;p>Here&amp;rsquo;s a general approach for defining the distribution of a random variable (r.v.). You can give the distribution using either the PMF, the CDF, or a named distribution with the parameters defined.&lt;/p>
&lt;ol>
&lt;li>Define the support of your r.v.&lt;/li>
&lt;li>See if the random variable matches the story of any of the named distributions we have discussed. To see if an r.v. matches a distribution, some things to check are
&lt;ul>
&lt;li>For which named distributions is the support of your r.v. possible?&lt;/li>
&lt;li>Are there draws/samples/trials? If so, are they independent?&lt;/li>
&lt;li>If there is sampling, is it done with or without replacement?&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>If you can match a named distribution, what are the parameters? Are those parameters allowed for that named distribution?&lt;/li>
&lt;li>If you can&amp;rsquo;t match a named distribution, how can you calculate the PMF using the information you checked about sampling and your counting skills?&lt;/li>
&lt;/ol>
&lt;h2 id="2-discrete-distributions">2. Discrete distributions&lt;/h2>
&lt;p>You can find details like the support, PMF, CDF, expectation, and variance in the table of distributions on page 605 of the textbook or page 3 of the midterm handout. We&amp;rsquo;ll focus on the stories and connections between distributions. For these discrete random variables (except for the Poisson), you should develop comfort with calculating their PMFs from scratch.&lt;/p>
&lt;h3 id="21-bernoulli">2.1 Bernoulli&lt;/h3>
&lt;p>&lt;strong>Story&lt;/strong>: We run a trial with probability $p$ of success. Let the random variable $X$ be $1$ if the trial succeeds or $0$ if the trial fails. Then $X \sim \mathrm{Bern}(p)$.&lt;/p>
&lt;p>&lt;strong>Connections&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>For $X \sim \mathrm{Bern}(p)$, $1-X \sim \mathrm{Bern}(1-p)$.&lt;/li>
&lt;li>For $X \sim \mathrm{Bern}(p)$, $X^2 = X$, so $X^2 \sim \mathrm{Bern}(p)$. If you&amp;rsquo;re wondering why, check the support!&lt;/li>
&lt;/ul>
&lt;h3 id="22-binomial">2.2 Binomial&lt;/h3>
&lt;p>&lt;strong>Story&lt;/strong>: We run $n$ &lt;strong>independent&lt;/strong> trials, each with an &lt;strong>equal&lt;/strong> probability $p$ of success. Let $X$ be the number of successful trials. Then $X \sim \mathrm{Bin}(n,p)$.&lt;/p>
&lt;p>&lt;strong>Connections&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>For $n$ &lt;strong>independent and identically distributed&lt;/strong> Bernoulli random variables $X_1, \ldots, X_n \stackrel{i.i.d.}{\sim} Bern(p)$, $$\sum_{i=1}^n X_i \sim \mathrm{Bin}(n, p).$$
&lt;ul>
&lt;li>This means $\mathrm{Bern}(p)$ is equivalent to $\mathrm{Bin}(1,p)$.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>For independent random variables $X \sim \mathrm{Bin}(n, p)$ and $Y \sim \mathrm{Bin}(m, p)$, $$X+Y \sim \mathrm{Bin}(n+m,p).$$&lt;/li>
&lt;/ul>
&lt;h3 id="23-hypergeometric">2.3 Hypergeometric&lt;/h3>
&lt;p>&lt;strong>Story&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>&lt;em>Capture/recapture elk&lt;/em>: There are $N$ elk in the forest. In the past, we captured and tagged $m$ of the elk. We now recapture $n$ of the elk, where every set of $n$ is equally likely and elk are sampled without replacement. Let $X$ be the number of tagged elk among our $n$ recaptured elk. Then $X \sim \mathrm{HGeom}(m, N-m, n)$.&lt;/li>
&lt;li>&lt;em>White and black balls in an urn&lt;/em>: There are $w$ white balls and $b$ black balls in a urn. We draw $n$ balls from the urn without replacement, where each set of $n$ balls is equally likely to be drawn. Let $X$ be the number of white balls in our sample. Then $X \sim \mathrm{HGeom}(w, b, n)$.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Connections&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>Notice the comparison between the Binomial and the Hypergeometric: using the urn story, if we sampled &lt;em>with&lt;/em> replacement our random variable would be distributed $\mathrm{Bin}(n, \frac{w}{w+b})$.&lt;/li>
&lt;/ul>
&lt;h3 id="24-geometricfirst-success">2.4 Geometric/First Success&lt;/h3>
&lt;p>&lt;strong>Story&lt;/strong>: Suppose we&amp;rsquo;re running independent Bernoulli trials with probability $p$ of success. We stop running trials once one succeeds. Let $X$ be the number of failed trials before (and &lt;em>not&lt;/em> including) the first successful trial. Then $X \sim \mathrm{Geom}(p)$.&lt;/p>
&lt;p>&lt;strong>Connections&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>The First Success distribution is essentially the same as the Geometric, but we include the first successful trial as part of our count. So it always holds that for $X \sim \mathrm{Geom}(p)$, we have $X+1 \sim \mathrm{FS}(p)$.&lt;/li>
&lt;li>Note that the Geometric/First Success distributions have infinite supports, while the Binomial has a fixed number of trials. This is a quick way to tell them apart.&lt;/li>
&lt;/ul>
&lt;h3 id="25-negative-binomial">2.5 Negative Binomial&lt;/h3>
&lt;p>&lt;strong>Story&lt;/strong>: Suppose we&amp;rsquo;re running independent Bernoulli trials with probability $p$ of success. We stop running trials after the $r^{th}$ success. Let $X$ be the number of failed trials before the $r^{th}$ success (not including any of the successes in that count). Then $X \sim \mathrm{NBin}(r,p)$.&lt;/p>
&lt;p>&lt;strong>Connections&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>For &lt;strong>independent and identically distributed&lt;/strong> $X_1, X_2, \ldots, X_r \stackrel{i.i.d.}{\sim} \mathrm{Geom}(p)$, we get $\sum_{i=1}^r X_i \sim \mathrm{NBin}(r, p)$.
&lt;ul>
&lt;li>This means $\mathrm{NBin}(1,p)$ is equivalent to $\mathrm{Geom}(p)$.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="26-poisson">2.6 Poisson&lt;/h3>
&lt;p>&lt;strong>Story&lt;/strong>: There&amp;rsquo;s no exact story to derive a Poisson. The only situation in which you&amp;rsquo;ll have to come up with the Poisson on your own is in approximation, and that is quite rare.&lt;/p>
&lt;p>&lt;strong>Approximate story&lt;/strong>: Say there are many rare events $A_1, A_2, \ldots, A_n$ (so $n$ large and $P(A_i) = &amp;laquo; 1$, which stands for much smaller than $1$) which are nearly independent (which doesn&amp;rsquo;t have a rigorous definition). Then if we let $\lambda = \sum_{i=1}^n P(A_i)$, $X = \sum_{i=1}^n I(A_i)$ is approximately distributed $\mathrm{Pois}(\lambda)$.&lt;/p>
&lt;p>&lt;strong>Connections&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>As you can see in the approximate story, you can use the Poisson to count the number of independent/weakly-dependent rare events that occur.&lt;/li>
&lt;li>Suppose $X \sim \mathrm{Pois}(\lambda)$ and $Y \sim \mathrm{Pois}(\mu)$ with $X, Y$ independent. Then $X + Y \sim \mathrm{Pois}(\lambda + \mu)$.&lt;/li>
&lt;li>Chicken-Egg: suppose a chicken lays $N$ eggs, with $N \sim \mathrm{Pois}(\lambda)$. Suppose each egg has a probability $p$ of hatching, with each egg&amp;rsquo;s hatching being independent, and let $X$ be the number of eggs that hatch and $Y$ be the number of eggs that don&amp;rsquo;t hatch.
&lt;ul>
&lt;li>$X$ and $Y$ are independent. $X$ and $Y$ are very conditionally independent given $N$ since $N = X+Y$.&lt;/li>
&lt;li>$X \sim \mathrm{Pois}(\lambda p)$, $Y \sim \mathrm{Pois}(\lambda (1-p))$.&lt;/li>
&lt;li>$X | N = n \sim \mathrm{Bin}(n, p)$.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="3-expectation">3. Expectation&lt;/h2>
&lt;div class="alert alert-note">
&lt;div>
The &lt;strong>expectation&lt;/strong> of a random variable $X$ with support $A$ is the weighted average of its possible values, where we weight based on the probability of $X$ taking on each value in its support. It is formally defined as
$$
E(X) = \sum_{x \in A} x P(X = x)
$$
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Linearity&lt;/strong> states that for any random variables $X, Y$ (which can be dependent!) and real number $c$,
\begin{align*}
E(X + Y) &amp;amp;= E(X) + E(Y),\\
E(cX) &amp;amp;= cE(X).
\end{align*}
The &lt;strong>law of the unconscious statistician (LOTUS)&lt;/strong> states that the expectation of any function of a random variable, $g(X)$, can be found by
$$
E(g(X)) = \sum_{x \in A} g(x) P(X = x).
$$
For example, if we want to find $E(X^2)$, we simply swap $x^2$ in for $x$ in the expectation formula to get $E(X^2) = \sum_{x \in A} x^2 P(X = x)$. Note that the probabilities here don&amp;rsquo;t change, only what goes in front.&lt;/p>
&lt;h3 id="31-indicator-random-variables">3.1 Indicator Random Variables&lt;/h3>
&lt;p>An &lt;strong>indicator random variable&lt;/strong> converts an event into a Bernoulli random variable. For an event $A$ with $P(A) = p$, the corresponding indicator random variable $I(A) \sim \mathrm{Bern}(p)$. This random variable is defined such that $I(A) = 1$ if $A$ occurs and $I(A) = 0$ if $A^c$ occurs. You might see other equivalent notation like $I_A$ or $I$, just be clear about which event your indicator random variable corresponds to.&lt;/p>
&lt;p>The &lt;strong>fundamental bridge&lt;/strong> (vocab which is not used outside of Stat 110) gives that
$$
E(I(A)) = P(A).
$$
We use this result a lot to calculate expectations of random variables that can be expressed as the sum of indicators. This is nice because the indicators can be dependent, but linearity allows us to break the expectations apart! A very common workflow to calculate an expectation is to write&lt;/p>
&lt;ol>
&lt;li>Write the random variable as the sum of indicators, $X = \sum_i I(A_i)$, where each $A_i$ is an event.&lt;/li>
&lt;li>Apply linearity, $E(X) = \sum_i E(I(A_i))$.&lt;/li>
&lt;li>Use the fundamental bridge, $E(X) = \sum_i P(A_i)$.&lt;/li>
&lt;/ol>
&lt;!-- ### 3.2 St. Petersburg -->
&lt;h3 id="32-variance">3.2 Variance&lt;/h3>
&lt;div class="alert alert-note">
&lt;div>
The &lt;strong>variance&lt;/strong> is a measure of spread, defined for a random variable $X$ as
$$
Var(X) = E((X - E(X))^2).
$$
&lt;/div>
&lt;/div>
&lt;p>Here basically all of the facts you have to know about variance:&lt;/p>
&lt;ul>
&lt;li>It&amp;rsquo;s usually calculated using an equivalent formula, $$Var(X) = E(X^2) - (E(X))^2.$$&lt;/li>
&lt;li>It is always nonnegative. In fact, variance is only zero if $P(X = x) = 1$ for some $x \in \mathbb R$: in other words, $X$ takes on a certain value with probability $1$. If this is not the case, the variance will be positive.&lt;/li>
&lt;li>For a scalar $c \in \mathbb R$ (a number, not random) and a random variable $X$,
\begin{align*}
Var(cX) &amp;amp;= c^2 Var(X)\\
Var(X + c) &amp;amp;= Var(X).
\end{align*}&lt;/li>
&lt;li>For &lt;em>independent&lt;/em> random variables $X$ and $Y$
$$
Var(X + Y) = Var(X) + Var(Y).
$$
For &lt;em>dependent&lt;/em> random variables $X$ and $Y$,
$$
Var(X+Y) \ne Var(X) + Var(Y).
$$&lt;/li>
&lt;/ul>
&lt;h2 id="4-handy-math-facts">4. Handy math facts&lt;/h2>
&lt;ul>
&lt;li>You are expected to know how to find the sum of an infinite geometric series: if $|x| &amp;lt; 1$,
$$
\sum_{n=0}^\infty x^n = \frac{1}{1-x}.
$$
otherwise the sum does not exist (it diverges). For finite geometric series (and any $x \ne 1$),
$$
\sum_{n=0}^\infty x^n = \frac{1-x^n}{1-x}.
$$&lt;/li>
&lt;li>You are also expected to be familiar with some $e^x$ approximations, but you usually won&amp;rsquo;t be asked to approximate without prompting. The Taylor series of $e^x$ is
$$
e^x = \sum_{n=0}^\infty \frac{x^n}{n!}.
$$
The compound interest formula also gives
$$
e^x = \lim_{n \to \infty} (1+\frac{x}{n})^n.
$$&lt;/li>
&lt;/ul></description></item><item><title>Week 3: Conditional Probability Examples and Random Variables</title><link>https://srihari-ganesh.github.io/stat110/week3/</link><pubDate>Tue, 26 Sep 2023 00:00:00 +0000</pubDate><guid>https://srihari-ganesh.github.io/stat110/week3/</guid><description>&lt;h2 id="0-logistical-info">0. Logistical Info&lt;/h2>
&lt;ul>
&lt;li>Section date: 9/27&lt;/li>
&lt;li>Associated lectures: 9/19, 9/21&lt;/li>
&lt;li>Associated pset: Pset 3, due 9/29&lt;/li>
&lt;li>Office hours on 9/27 from 7-9pm at Quincy Dining Hall&lt;/li>
&lt;li>Remember to fill out the attendance form&lt;/li>
&lt;li>Scroll to &lt;a href="#4-summary">section 4&lt;/a> for a concise content summary.&lt;/li>
&lt;/ul>
&lt;h3 id="01-summary--practice-problem-pdfs">0.1 Summary + Practice Problem PDFs&lt;/h3>
&lt;p>&lt;a href="https://srihari-ganesh.github.io/uploads/stat110/Section%203%20Handout.pdf" target="_blank">Summary + Practice Problems PDF&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://srihari-ganesh.github.io/uploads/stat110/Section%203%20Solutions.pdf" target="_blank">Practice Problem Solutions PDF&lt;/a>&lt;/p>
&lt;h2 id="1-examples-from-class">1. Examples from class&lt;/h2>
&lt;p>The lecture on 9/19 was full of examples of conditional probability. Here are my takeaways for each. I did not rewrite the examples because solutions are quite well-written in the book and/or in lecture.&lt;/p>
&lt;h3 id="11-winter-girl-examples-225-227-in-the-book">1.1 Winter girl (Examples 2.2.5-2.2.7 in the book)&lt;/h3>
&lt;ul>
&lt;li>Define your events as specifically as possible, there are a lot of details that surprisingly can change probabilities.&lt;/li>
&lt;li>See if you can simplify your events (both mathematically and logically). For example, if $A$ is the event that there&amp;rsquo;s at least one girl and $B$ is the event that there are two girls, $P(A, B) = P(B)$ since if there are two girls, there is automatically at least one girls.&lt;/li>
&lt;/ul>
&lt;h3 id="12-monty-hall-example-271-and-many-practice-problems-in-the-book">1.2 Monty Hall (Example 2.7.1 and many practice problems in the book)&lt;/h3>
&lt;p>You can use the Law of Total Probability in some extreme ways! Condition on things that make your life much, much easier - in Monty Hall problems (and the variants that Joe likes to write), I very often condition on the location of the car or use Bayes&amp;rsquo; rule to move information about the car&amp;rsquo;s location into the condition!&lt;/p>
&lt;h3 id="13-simpsons-paradox-example-283-in-the-book">1.3 Simpson&amp;rsquo;s paradox (Example 2.8.3 in the book)&lt;/h3>
&lt;p>I think it&amp;rsquo;s a good to develop the skill of coming up with similar paradoxes - at the very least, it can test your understanding of probability. My understanding of this phenomenon is that there are two tasks - a hard and an easy task. Doctor A might have a better success rate in each task, but Doctor B can still have a higher overall success rate.&lt;/p>
&lt;p>This happens because Doctor A does more of the harder task, which drags their average down, while Doctor B inflates their average by doing the easier task more often. There are some other intuitive corollaries - like how some students may learn a lot but have lower GPAs then other students because they take a higher proportion of challenging classes.&lt;/p>
&lt;p>To construct these paradoxes, I think you need a hard task (where both doctors have a &amp;ldquo;low&amp;rdquo; success rate) and an easy task (where both doctors have a higher success rate). Then doctor A has to do more of the hard task, while doctor B needs to do more of the easy task, to weight their averages differently.&lt;/p>
&lt;h3 id="14-gamblers-ruin-example-273-in-the-book">1.4 Gambler&amp;rsquo;s ruin (Example 2.7.3 in the book)&lt;/h3>
&lt;p>You will be assessed (in pset and/or exam) on your ability to apply the gambler&amp;rsquo;s ruin result in other contexts, but you will never have to re-derive it. So you can figure out how variables in your problem correspond to gambler&amp;rsquo;s ruin (or even set up the difference equation) then just jump to plugging in the solution given.&lt;/p>
&lt;p>In the gambler&amp;rsquo;s ruin problem, gambler $A$ starts with $i$ dollars and gambler $B$ starts with $N-i$ dollars. They keep making 1 dollar bets (which gambler $A$ has a probability $p$ of winning) until someone runs out of money (either gambler $A$ has $0$ dollars or gambler $B$ has 0 dollars). We often define $q = 1-p$ for notational convenience.&lt;/p>
&lt;p>If we define $p_i$ to be the probability that gambler $A$ wins if they start with $i$ dollars, then using first-step analysis we find that
$$
p_i = p_{i+1} p + p_{i-1}q.
$$
This gives a difference equation solution of
$$
p_i = \begin{cases} \frac{1 - (\frac{q}{p})^i}{1 - (\frac{q}{p})^N}&amp;amp; p \ne 1/2\\
\frac{i}{N}&amp;amp; p = 1/2 \end{cases}
$$&lt;/p>
&lt;p>To match a problem to this, you should&lt;/p>
&lt;ul>
&lt;li>Make sure that &amp;ldquo;bets&amp;rdquo; are worth 1 dollar each.&lt;/li>
&lt;li>Make sure that gambler $A$ loses if they hit 0 dollars, and wins if they hit some fixed amount of dollars ($N$)&lt;/li>
&lt;li>Make sure there is a constant probability of winning each bet.&lt;/li>
&lt;/ul>
&lt;h2 id="2-random-variables">2. Random variables&lt;/h2>
&lt;h3 id="21-definition">2.1 Definition&lt;/h3>
&lt;p>A &lt;strong>random variable&lt;/strong> summarizes some experiment. So if you have a sample space $S$, for each possible outcome $\omega \in S$, your random variable takes on a certain (real number). Here are some examples of random variables:&lt;/p>
&lt;ul>
&lt;li>Say we&amp;rsquo;re rolling a die, so $S$ is the set of possible rolls. Then we could have $X = 1$ if we roll a $1$, $X = 2$ if we roll a $2$, and so on.&lt;/li>
&lt;li>We could define another random variable $Y$ to be the square of the roll. so $Y = 1$ if we roll a 1, $Y = 16$ if we roll a $4$, etc.&lt;/li>
&lt;li>Random variables don&amp;rsquo;t have to take on different values for every outcome. So we could have $Z = 2$ if we roll an even number and $Z = 1$ if we roll an odd number.&lt;/li>
&lt;li>They also don&amp;rsquo;t have to be discrete values - we could have a random variable $T$ represent the exact temperature in the room right now.&lt;/li>
&lt;/ul>
&lt;h3 id="22-defining-discrete-random-variables">2.2 Defining discrete random variables&lt;/h3>
&lt;p>A random variable is &lt;strong>discrete&lt;/strong> if it has a finite or countably infinite number of values (something like $1, 2, 3, 4, \ldots$ is countably infinite. If instead your random variable can take on any value in an interval - like any real number, or any real number between 2 and 4, etc. - then it is uncountable).&lt;/p>
&lt;p>We can uniquely describe a random variable by its &lt;strong>probability mass function&lt;/strong>. This basically tells us the probability that the random variable takes on each possible value. For example, the probability mass function for the first dice-roll example, $X$ is
$$
P(X = x) = \begin{cases}
\frac{1}{6} &amp;amp; x \in \{1, 2, 3, 4, 5, 6\}\\
0 &amp;amp; \text{else}.
\end{cases}
$$
The little $x$ is a dummy variable - it&amp;rsquo;s just a general way of going through every possible value. For example, we can now tell that there is a $1/6$ probability that $X = 4$, which corresponds to a $1/6$ probability that we roll a $4$, which makes sense.&lt;/p>
&lt;p>Some important facts:&lt;/p>
&lt;ul>
&lt;li>&amp;ldquo;$X = 1$&amp;rdquo; is an event, so we can take its probability. We &lt;em>cannot&lt;/em> take the probability of a random variable, so $P(X)$ is a category error.&lt;/li>
&lt;li>When writing a PMF
&lt;ul>
&lt;li>Every probability should be between $0$ and $1$ (inclusive), which holds for all probabilities.&lt;/li>
&lt;li>The probabilities in the PMF should sum to $1$:
$$
\sum_{x \in \mathbb{R}} P(X = x) = 1.
$$
This comes from both axioms of probability, since the events for each possible value of $X$ partition the entire sample space.&lt;/li>
&lt;li>I always have the &amp;ldquo;$0$, else&amp;rdquo; statement.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>The &lt;strong>support&lt;/strong> of a random variable is the set of possible values it can take on. So the support for $X$ is ${1, 2, 3, 4, 5, 6}$, and the support for $Z$ is ${1, 2}$, and so on. You should always define the support, too.&lt;/li>
&lt;/ul>
&lt;h3 id="23-a-more-mathy-definition-of-random-variables">2.3 A more mathy definition of random variables&lt;/h3>
&lt;p>Random variables are functions. If notation makes more sense to you, maybe this will be useful.&lt;/p>
&lt;p>Remember that in any random experiment, we have a sample space $S$, where each element of $S$ is a possible outcome. Exactly one of those outcomes will happen. You can think of a random variable $X$ as a function that maps outcomes to the real number line, so $X : S \to \mathbb{R}$. So if some outcome $\omega \in S$ happens, then the random variable gives us the real number $X(\omega)$. There&amp;rsquo;s nothing random about the function $X$ - each outcome always goes to the same real number - but instead the randomness comes from which outcome actually ends up happening.&lt;/p>
&lt;p>So when we think about PMFs and any probabilities with random variables, we&amp;rsquo;re using a bit of shorthand. It&amp;rsquo;s not immediately obvious that $X = x$ is an event, so let&amp;rsquo;s translate (again, remember $x$ is just a fixed number. I could just as easily use $y$ or $a$ as a variable, or $0$ or $1.5$ or $-17$ if I want a specific value):
\begin{align}
P(X = x) = P(\{\omega \in S : X(\omega) = x\})
\end{align}
That set, $\{\omega \in S : X(\omega) = x\}$, is a subset of the sample space and thus is definitely an event. We can take a probability of that. Note that there can be multiple outcomes in this subset, which is the same as saying that $X$ is &lt;em>not injective&lt;/em>.&lt;/p>
&lt;p>So when we start talking about fancier random varibles - like $X^2$ - we can still dissect the probabilities
\begin{align}
P(X^2 = y) &amp;amp;= P(\{\omega \in S : X^2(\omega) = y\})\\
&amp;amp;= P(\{\omega \in S : X(\omega) = \sqrt{y} \text{ or } X(\omega) = -\sqrt{y}\})\\
&amp;amp;= P((X = \sqrt{y}) \cup (X = -\sqrt{y}))\\
&amp;amp;= P(X = \sqrt{y}) + P(X = -\sqrt{y}),
\end{align}
assuming $y$ is positive and splitting up the probability in the last step because the two events are disjoint (a random variable can&amp;rsquo;t equal two different values at the same time).&lt;/p>
&lt;p>The support can also now be redefined as the function - basically, the support is $\{x \in \mathbb{R} : \text{ there exists } \omega \in S \text{ such that } X(\omega) = x\}$.&lt;/p>
&lt;h2 id="3-distributions">3. Distributions&lt;/h2>
&lt;p>A distribution is a type of random variable. I think this makes the most sense through example. For discrete distributions (which correspond to discrete random variables), we usually motivate them with a story and maybe some counting. &lt;strong>Stories are extremely important and should be internalized,&lt;/strong> not just the PMFs.&lt;/p>
&lt;h3 id="31-bernoulli-distribution">3.1 Bernoulli Distribution&lt;/h3>
&lt;p>You perform an experiment that consists of $1$ trial, where the possible outcomes are success or failure. There is a probability $p$ of success and probability $q = 1-p$ of failure. This is summarized by a random variable $X$, where $X = 1$ if the trial is a success and $X = 0$ if it is a failure. We then say $X$ is distributed Bernoulli with parameter $p$, or in notation, $X \sim \mathrm{Bern}(p)$.&lt;/p>
&lt;p>The PMF is
$$
P(X = x) = \begin{cases}
p &amp;amp; x = 1\\
q &amp;amp; x = 0\\
0 &amp;amp; \text{else}
\end{cases}
$$&lt;/p>
&lt;p>For a concrete example, a rigged coin toss has a probability $p = 0.7$ of landing heads (success) and probability $q = 0.3$ of landing tails (failure). If $X = 1$ when the coin lands heads and $X = 0$ when it lands tails, then $X \sim \mathrm{Bern}(0.7)$.&lt;/p>
&lt;p>Note that you CANNOT set a random variable &lt;em>equal&lt;/em> to a distribution. You have to use the $\sim$ symbol and say &amp;ldquo;distributed as.&amp;rdquo;&lt;/p>
&lt;h3 id="32-binomial-distribution">3.2 Binomial Distribution&lt;/h3>
&lt;p>You perform an experiment with $n$, independent Bernoulli trials, each of which is a success with the same probability $p$. Then the random variable $Y$, the number of successful trials, is distributed Binomial with $n$ trials and success probability $p$. In notation, $Y \sim \mathrm{Bin}(n, p)$.&lt;/p>
&lt;p>We can find the PMF with some counting:
\begin{align}
P(Y = y) &amp;amp;= \begin{cases}
\binom{n}{y} p^y (1-p)^{n-y} &amp;amp; y \in \{0, 1, 2, \ldots, n\}\\
0 &amp;amp; \text{else}.
\end{cases}
\end{align}&lt;/p>
&lt;p>NOTE: a Binomial random variable $Y$ can be represented as the sum of $n$ independent Bernoulli random variables, each with success probability $p$.&lt;/p>
&lt;h2 id="4-summary">4. Summary&lt;/h2>
&lt;h3 id="41-examples-from-class">4.1 Examples from class&lt;/h3>
&lt;p>Some takeaways:&lt;/p>
&lt;ul>
&lt;li>Define events very specifically (winter girl)&lt;/li>
&lt;li>See if you can simplify your problems with logic, not just relying on grinding through math (winter girl)&lt;/li>
&lt;li>Use the Law of Total Probability to condition on anything/everything you wish you knew
&lt;ul>
&lt;li>In Monty Hall, you can often condition on the location of the car!&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>To mimic Simpson&amp;rsquo;s paradox, set up some &amp;ldquo;hard&amp;rdquo; and &amp;ldquo;easy&amp;rdquo; tasks and make the better doctor do more of the hard tasks&lt;/li>
&lt;li>Learn how to turn a problem into the gambler&amp;rsquo;s ruin problem:
&lt;ul>
&lt;li>Make losing happen at $0$, and winning happen at some fixed $N$&lt;/li>
&lt;li>Make sure each bet/step is only one dollar in either direction&lt;/li>
&lt;li>Make sure the probability of winning each individual bet is constant&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="42-random-variables">4.2 Random variables&lt;/h3>
&lt;p>&lt;strong>Random variables&lt;/strong> are a numerical (real number) summary of the outcome of your experiment. So for each possible outcome, the random variable takes on a certain value. Multiple outcomes can lead to the same value of the random variable. The &lt;strong>support&lt;/strong> of a random variable is the set of possible values it can take on.&lt;/p>
&lt;p>We define discrete random variables by their &lt;strong>probability mass function&lt;/strong>. You should define the probability $P(X = x)$ for each $x$ in the random variable&amp;rsquo;s support, and always write probability $0$ for any value of $x$ that is not in the support. The PMF should always give valid probabilities, and should sum to $1$.&lt;/p>
&lt;h3 id="43-distributions">4.3 Distributions&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Bernoulli distribution&lt;/strong>: you conduct a single trial that succeeds with probability $p$. $X = 1$ if the trial succeeds and $X = 0$ if it fails. Then $X \sim \mathrm{Bern}(p)$ and has a PMF
$$
P(X = x) = \begin{cases}
p &amp;amp; x = 1\\
1 - p &amp;amp; x = 0\\
0 &amp;amp; \text{else}
\end{cases}
$$&lt;/li>
&lt;li>&lt;strong>Binomial distribution&lt;/strong>: you conducts $n$ independent trials that each succeed with probability $p$. $Y$ is the total number of successes among the $n$ trials. Then $Y \sim \mathrm{Bin}(n, p)$ and the PMF is
$$
P(Y = y) = \begin{cases}
\binom{n}{y} p^y (1-p)^{n-y} &amp;amp; y \in \{0, 1, \ldots, n\}\\
0 &amp;amp; \text{else}
\end{cases}
$$&lt;/li>
&lt;/ul></description></item><item><title>Week 2: Conditional Probability</title><link>https://srihari-ganesh.github.io/stat110/week2/</link><pubDate>Sat, 16 Sep 2023 00:00:00 +0000</pubDate><guid>https://srihari-ganesh.github.io/stat110/week2/</guid><description>&lt;h2 id="0-logistical-info">0. Logistical Info&lt;/h2>
&lt;ul>
&lt;li>Section date: 9/20&lt;/li>
&lt;li>Associated lectures: 9/12, 9/14&lt;/li>
&lt;li>Associated pset: Pset 2, due 9/22&lt;/li>
&lt;li>Office hours on 9/20 from 7-9pm at Quincy Dining Hall&lt;/li>
&lt;li>Remember to fill out the attendance form&lt;/li>
&lt;li>Scroll to &lt;a href="#5-summary">section 5&lt;/a> for a concise content summary.&lt;/li>
&lt;/ul>
&lt;h3 id="01-summary--practice-problem-pdfs">0.1 Summary + Practice Problem PDFs&lt;/h3>
&lt;p>&lt;a href="https://srihari-ganesh.github.io/uploads/stat110/Section%202%20Handout.pdf" target="_blank">Summary + Practice Problems PDF&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://srihari-ganesh.github.io/uploads/stat110/Section%202%20Solutions.pdf" target="_blank">Practice Problem Solutions PDF&lt;/a>&lt;/p>
&lt;h2 id="1-brushing-up-on-the-definition-of-probability">1. Brushing up on the definition of probability&lt;/h2>
&lt;p>We&amp;rsquo;ll restate the axioms for the general definition of probability:&lt;/p>
&lt;div class="alert alert-note">
&lt;div>
&lt;p>&lt;strong>Definition of probability&lt;/strong>:&lt;br>
There are just two axioms (rules that probabilities have to follow):&lt;/p>
&lt;ol>
&lt;li>$P(S) = 1, P(\emptyset) = 0.$&lt;/li>
&lt;li>If events $A_1, A_2, \ldots$ are disjoint, then $$
P\left( \bigcup_{j=1}^\infty A_j\right) = \sum_{j=1}^\infty P(A_j).
$$&lt;/li>
&lt;/ol>
&lt;p>In other words, if $A_1, A_2, \ldots$ partition some event $B$, then $P(B) = \sum_{j=1}^\infty P(A_j)$.&lt;/p>
&lt;/div>
&lt;/div>
&lt;!-- axioms are what mathematicians use to encode intuitive definitions -->
&lt;div class="alert alert-warning">
&lt;div>
&lt;p>&lt;strong>Tips for calculating probabilities&lt;/strong>:&lt;/p>
&lt;ol>
&lt;li>Define events for every aspect of the problem (e.g., &amp;ldquo;$A$ = the event that it rains tomorrow, $B$ = the event that it rained today&amp;rdquo;)&lt;/li>
&lt;li>Write out the probabilities that you are given in the problem using notation (e.g., &amp;ldquo;$P(A|B) = 1/2$, $P(B) = 1/4$).&lt;/li>
&lt;li>Write the probability that you want to calculate using notation (e.g., we want to calculate the unconditional probability that it rains tomorrow, $P(A)$).&lt;/li>
&lt;li>Figure out how the tools we have learned allow you to utilize the probabiliies that you do know (step 2) to calculate the probabilities that you don&amp;rsquo;t know (step 3).&lt;/li>
&lt;/ol>
&lt;/div>
&lt;/div>
&lt;p>There are some important results that follow:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Probability of a complement&lt;/strong>: If $A$ is an event a sample space $S$,
$$
P(A) = 1 - P(A^c).
$$
Concisely, the probability of an event occuring is $1$ minus the probability of the event not occuring.&lt;/li>
&lt;li>&lt;strong>Probability of a union&lt;/strong>: For events $A$, $B$, we have
$$
P(A \cup B) = P(A) + P(B) - P(A \cap B).
$$
It&amp;rsquo;s also useful to &amp;ldquo;disjointify&amp;rdquo; $A \cup B$ into a partition ($A \cup B^c, A \cap B, A^c \cup B$) which allows us to use the second axiom and get
$$
P(A \cup B) = P(A \cup B^c) + P(A \cap B) + P(A^c \cup B).
$$&lt;/li>
&lt;li>&lt;strong>Principle of Inclusion-Exclusion (PIE)&lt;/strong>: this is a general formula for the probability of the union of $n$ events
\begin{align*}
P(\bigcup_{i=1}^n A_i) &amp;amp;= \sum_i P(A_i) - \sum_{i &amp;lt; j} P(A_i \cap A_j)\\
&amp;amp;+ \sum_{i &amp;lt; j &amp;lt; k} P(A_i \cap A_j \cap A_k) - \cdots + (-1)^{n+1} P(\bigcap_{i=1}^n A_i).
\end{align*}
Note that the formula for the probability of the union of two events is the $n=2$ case of PIE. &lt;br>
A potential workflow (that you saw on Pset 1) for the probability of an intersection, $P(A_1 \cap \cdots \cap A_n)$, is to
&lt;ul>
&lt;li>Use complementary counting and DeMorgan&amp;rsquo;s law (in that order) to turn the intersection into a union:
\begin{align*}
P(A_1 \cap \cdots \cap A_n) &amp;amp;= 1 - P((A_1 \cap \cdots \cap A_n)^c)\\
&amp;amp;= 1 - P(A_1^c \cup \cdots \cup A_n^c)
\end{align*}&lt;/li>
&lt;li>Apply PIE to the union $P(A_1^c \cup \cdots \cup A_n^c)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;!-- TODO ADD VISUALIZATION -->
&lt;h2 id="2-conditional-probability">2. Conditional Probability&lt;/h2>
&lt;div class="alert alert-warning">
&lt;div>
&lt;p>&lt;strong>Notation note&lt;/strong>:&lt;/p>
&lt;p>We will start writing $P(A \cap B)$ as $P(A, B)$ (i.e., commas between events and intersections are equivalent).&lt;/p>
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
&lt;p>&lt;strong>Conditional probability&lt;/strong>:&lt;/p>
&lt;p>If $A$ and $B$ are two events, then the probability that $A$ occurs &lt;strong>conditional&lt;/strong> on the fact that $B$ occurs (or &lt;em>given&lt;/em> that $B$ occurs) is notated as $P(A|B)$ and equals
$$
P(A|B) = \frac{P(A, B)}{P(B)}.
$$
All conditions go to the right of the bar symbol $|$.&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>We read $P(A|B)$ is the &amp;ldquo;probability of $A$ given $B$&amp;rdquo; or &amp;ldquo;probability of $A$ conditioned on $B$&amp;rdquo; Intuitively, we can consider that if we know $B$ occurs, $B$ basically becomes our new sample space, so we take the probability that both $A$ and $B$ occurs, $P(A, B)$, and rescale it by the probability that $B$ occurs, $P(B)$.&lt;/p>
&lt;video controls >
&lt;source src="https://srihari-ganesh.github.io/media/videos/Section2_ConditionalProbability.mp4" type="video/mp4">
&lt;/video>
&lt;p>We&amp;rsquo;re also quick to note that conditional probabilities are the same as &amp;ldquo;normal&amp;rdquo; probabilities &amp;mdash; in fact, all probabilities can be considered conditional, we just treat some conditions more implicitly than others since they are more obvious/always involved to the problem. We&amp;rsquo;ll use &lt;strong>extra conditioning&lt;/strong> to refer to problems where some conditions are always present (i.e., we never want to/don&amp;rsquo;t know how to calculate the probability of those conditionns). For example, to calculate the probability that it rains tomorrow ($A$) given that it rained today $B$, we would right $P(A|B)$. However, we are implicitly conditioning on a lot of things: that the world exists tomorrow ($W$), that I will be on Harvard campus when I check whether it rains $H$, etc. So we could incorporate these extra conditions into our problem to write the definition of &lt;strong>conditional probability with extra conditioning&lt;/strong>:
$$
P(A|B, H, W) = \frac{P(A, B | H, W)}{P(B | H, W)}
$$
As you can see, when we want certain events to be &lt;em>extra conditions&lt;/em>, they are conditions in every related probability we calculate. Each time we apply a formula for conditional probability, we have to choose whether to treat the each condition like $B$ (free to move around) or like $H$ (extra conditioning/always a condition).&lt;/p>
&lt;h2 id="3-tools-using-conditional-probability">3. Tools using Conditional Probability&lt;/h2>
&lt;div class="alert alert-warning">
&lt;div>
If you ever need to solve a problem involving a sequence of things (like a game with many turns, or a random walk, or so on) and are stuck, try &lt;strong>first-step analysis&lt;/strong>: conditioning what happens after the first step. You&amp;rsquo;ll often be able to get a recursive equation that is easier to solve.
&lt;/div>
&lt;/div>
&lt;h3 id="31-probability-of-an-intersection">3.1 Probability of an Intersection&lt;/h3>
&lt;p>For events $A, B$ we can rearrange the definition of conditional probability to find the probability of their intersection:
$$
P(A, B) = P(A) P(B | A) = P(B) P(A | B).
$$
This works for intersections of $n$ events:
$$
P(A_1, A_2, \ldots, A_n) = P(A_{i_1}) P(A_{i_2} | A_{i_1}) \cdots P(A_{i_n} | A_{i_1}, A_{i_2}, \ldots A_{i_{n-1}})
$$
where $i_1, i_2, \ldots, i_n$ is a permutation of $1, 2, \ldots, n$. Note that we can choose the conditions in any order we want, and pick the order to our best convenience &amp;mdash; for example, if you only know the unconditional probability of $A_8$, you should let $i_1 = 8$.&lt;/p>
&lt;p>The probability of an intersection &lt;em>with extra conditioning&lt;/em> is
$$
P(A, B | C) = P(A | C) P(B | A, C) = P(B | C) P(A | B, C).
$$&lt;/p>
&lt;h3 id="32-law-of-total-probability-lotp">3.2 Law of Total Probability (LOTP)&lt;/h3>
&lt;p>The &lt;strong>law of total probability (LOTP)&lt;/strong> is a clever rephrasing of the second axiom of probability (the probability of a partition is the sum of probabilities): if $A_1, A_2, \ldots, A_n$ partition the sample space, then
\begin{align*}
P(B) &amp;amp;= P(B, A_1) + P(B, A_2) + \cdots + P(B, A_n).
\end{align*}
See Figure 2.3 from Blizstein &amp;amp; Hwang below for a visualization:
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Visual diagram of the law of total probability, showing an event split up into pieces by its intersections with a partition of the sample space." srcset="
/media/images/Section2_LOTP_hu81255567074645dc13bc1b2d32e50da6_60337_e16aa315eb347d762ed01077f51a4a21.webp 400w,
/media/images/Section2_LOTP_hu81255567074645dc13bc1b2d32e50da6_60337_6c867f1081982bae899a8c3d574550e0.webp 760w,
/media/images/Section2_LOTP_hu81255567074645dc13bc1b2d32e50da6_60337_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://srihari-ganesh.github.io/media/images/Section2_LOTP_hu81255567074645dc13bc1b2d32e50da6_60337_e16aa315eb347d762ed01077f51a4a21.webp"
width="624"
height="410"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>We usually break down each of the terms using the result for the probability of an intersection &lt;a href="#31-probability-of-an-intersection">section 3.1&lt;/a>
$$
P(B) = P(B | A_1) P(A_1) + P(B | A_2) P(A_2) + \cdots + P(B | A_n) P(A_n)
$$
When you only know $B$ in terms of conditional probabilities, we can make those conditional probabilities appear through LOTP. As Joe likes to say, &lt;em>condition on what you wish you knew&lt;/em>:&lt;/p>
&lt;p>LOTP with &lt;em>extra conditioning&lt;/em> is
$$
P(B | C) = P(B | A_1, C) P(A_1 | C) + P(B | A_2, C) P(A_2 | C) + \cdots + P(B | A_n, C) P(A_n| C)
$$&lt;/p>
&lt;h3 id="33-bayes-rule">3.3 Bayes&amp;rsquo; Rule&lt;/h3>
&lt;p>&lt;strong>Bayes&amp;rsquo; Rule&lt;/strong> is also a result of the formula for the probability of an intersection:
$$
P(B|A) = \frac{P(A|B) P(B)}{P(A)}.
$$
The denominator often gets expanded out using LOTP (&lt;a href="#32-law-of-total-probability-lotp">section 3.2&lt;/a>), often with a partition containing $B$ like
$$
P(B|A) = \frac{P(A|B) P(B)}{P(A|B) P(B) + P(A|B^c) P(B^c)}.
$$
Bayes&amp;rsquo; rule is used in situations where we don&amp;rsquo;t know how to calculate the probability of $B$ given $A$, $P(B|A)$, but know how to calculate the probability of $A$ given $B$, $P(A|B)$.&lt;/p>
&lt;p>Bayes&amp;rsquo; rule with &lt;em>extra conditioning&lt;/em> is
$$
P(B | A, C) = \frac{P(A | B, C) P(B | C)}{P(A | B, C) P(B|C) + P(A|B^c, C) P(B^c|C)}
$$&lt;/p>
&lt;h2 id="4-independence">4. Independence&lt;/h2>
&lt;div class="alert alert-note">
&lt;div>
&lt;p>&lt;strong>Independence for two events&lt;/strong>&lt;/p>
&lt;p>Two events $A, B$ are independent if
$$
P(A, B) = P(A) P(B)
$$&lt;/p>
&lt;p>For $P(A), P(B) &amp;gt; 0$, this either of the following as well:
\begin{align*}
P(A|B) &amp;amp;= P(A) \text{ or }\
P(B|A) &amp;amp;= P(B)
\end{align*}&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>Intuitively, independence means that information about $A$ (e.g., knowing whether $A$ occurs) gives us no information about $B$. Some&lt;/p>
&lt;ul>
&lt;li>Note that independence goes both ways &amp;mdash; if $A$ is independent of $B$, then $B$ is independent of $A$.&lt;/li>
&lt;li>If $A$ is independent of $B$, then $A$ is independent of $B^c$ and $A^c$ is independent of $B^c$.&lt;/li>
&lt;/ul>
&lt;div class="alert alert-warning">
&lt;div>
Independence and disjointness are not the same! In fact, if $A, B$ are disjoint, then if $A$ occurs we know that $B$ did not occur, so they are very &lt;em>dependent&lt;/em>.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
&lt;p>&lt;strong>Independence for many events&lt;/strong>&lt;/p>
&lt;p>A group of events $A_1, A_2, \ldots, A_n$, are independent if for any subset $A_{i_1}, A_{i_2}, \ldots, A_{i_k}$,
$$
P(A_{i_1}, A_{i_2}, \ldots, A_{i_k}) = P(A_{i_1}) P(A_{i_2}) \cdots P(A_{i_k})
$$
Note that pairwise independence (e.g., showing that $P(A_i, A_j) = P(A_i) P(A_j)$ for all $i,j$) is required, but not enough, to show that joint independence of all of the sets.&lt;/p>
&lt;/div>
&lt;/div>
&lt;h3 id="41-conditional-independence">4.1 Conditional Independence&lt;/h3>
&lt;p>&lt;strong>Conditional independence&lt;/strong> follows a similar formula: $A, B$ are conditionally independent given $C$ if
$$
P(A, B | C) = P(A|C) P(B|C).
$$
You can see how this is analogous to extra conditioning for the other results. Conditional independence of many events is similarly defined.&lt;/p>
&lt;div class="alert alert-warning">
&lt;div>
Conditional independence and independence are not the same thing, and one often exists without the other!
&lt;/div>
&lt;/div>
&lt;h2 id="5-summary">5. Summary&lt;/h2>
&lt;p>&lt;strong>Notation note&lt;/strong>: see that we use commas and intersections interchangeably (i.e., $P(A, B, C) = P(A \cap B \cap C)$).&lt;/p>
&lt;p>&lt;strong>Tips for calculating probabilities&lt;/strong>:&lt;/p>
&lt;ol>
&lt;li>Define events for every aspect of the problem (e.g., &amp;ldquo;$A$ = the event that it rains tomorrow, $B$ = the event that it rained today&amp;rdquo;)&lt;/li>
&lt;li>Write out the probabilities that you are given in the problem using notation (e.g., &amp;ldquo;$P(A|B) = 1/2$, $P(B) = 1/4$).&lt;/li>
&lt;li>Write the probability that you want to calculate using notation (e.g., we want to calculate the unconditional probability that it rains tomorrow, $P(A)$).&lt;/li>
&lt;li>Figure out how the tools we have learned allow you to utilize the probabiliies that you do know (step 2) to calculate the probabilities that you don&amp;rsquo;t know (step 3).&lt;/li>
&lt;/ol>
&lt;h3 id="51-definition-of-probability">5.1 Definition of Probability&lt;/h3>
&lt;p>&lt;strong>Axioms of probability:&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>With sample space $S$, \begin{align*}
P(S) &amp;amp;= 1\\
P(\emptyset) &amp;amp;= 0.
\end{align*}&lt;/li>
&lt;li>For $A_1, A_2, \ldots, $ that partition $B$ (this can be finite or infinite),
\begin{align*}
P(B) = \sum_{j=1}^\infty P(A_j)
\end{align*}&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Probability of a complement&lt;/strong>: For event $A$,
$$
P(A) = 1 - P(A^c)
$$&lt;/p>
&lt;p>&lt;strong>Probability of a union&lt;/strong>: For events $A$ and $B$,
\begin{align*}
P(A \cup B) &amp;amp;= P(A) + P(B) - P(A \cap B)\\ &amp;amp;= P(A \cap B^c) + P(B \cap A^c) + P(A \cap B)
\end{align*}&lt;/p>
&lt;p>&lt;strong>Principle of Inclusion-Exclusion&lt;/strong>: For events $A_1, \ldots, A_n$,
\begin{align*}
P(\bigcup_{i=1}^n A_i) &amp;amp;= \sum_i P(A_i) - \sum_{i &amp;lt; j} P(A_i \cap A_j)\\
&amp;amp;+ \sum_{i &amp;lt; j &amp;lt; k} P(A_i \cap A_j \cap A_k) - \cdots + (-1)^{n+1} P(\bigcap_{i=1}^n A_i).
\end{align*}&lt;/p>
&lt;h3 id="52-conditional-probability">5.2 Conditional Probability&lt;/h3>
&lt;p>&lt;strong>Conditional probability&lt;/strong>: For events $A$ and $B$, the probability of $A$ given $B$ (i.e., given that $B$ occured) is
\begin{align*}
P(A|B) = \frac{P(A \cap B)}{P(B)}.
\end{align*}
&lt;em>&amp;hellip;with extra conditioning&lt;/em>:
\begin{align*}
P(A|B, C) = \frac{P(A \cap B | C)}{P(B | C)}.
\end{align*}&lt;/p>
&lt;h3 id="53-conditional-probability-tools">5.3 Conditional Probability Tools&lt;/h3>
&lt;p>&lt;strong>First-step analysis&lt;/strong>: If you ever need to solve a problem involving a sequence of things (like a game with many turns, or a random walk, or so on) and are stuck, try first-step analysis: conditioning what happens after the first step. You&amp;rsquo;ll often be able to get a recursive equation that is easier to solve.&lt;/p>
&lt;p>&lt;strong>Probability of an intersection&lt;/strong>:
\begin{align*}
P(A_1, A_2, \ldots A_n) &amp;amp;= P(A_1) P(A_2 | A_1) \cdots P(A_n | A_1, \ldots, A_{n-1})\\
&amp;amp;= P(A_n) P(A_{n-1} | A_n) \cdots P(A_1 | A_2, \ldots, A_n), \\
&amp;amp;= [\text{chaining in any order that is convenient for you}].
\end{align*}
&lt;em>&amp;hellip;with extra conditioning&lt;/em>:
\begin{align*}
P(A_1, A_2, \ldots A_n | C) &amp;amp;= P(A_1 | C) P(A_2 | A_1, C) \cdots P(A_n | A_1, \ldots, A_{n-1}, C)
\end{align*}
&lt;strong>Law of Total Probability (LOTP)&lt;/strong>: for events $A_1, A_2, \ldots, A_n$ that partition $S$, we can find $P(B)$ by
\begin{align*}
P(B) &amp;amp;= P(B, A_1) + P(B, A_2) + \cdots + P(B, A_n) \\
&amp;amp;= P(B | A_1) P(A_1) + P(B | A_2) P(A_2) + \cdots + P(B | A_n) P(A_n).
\end{align*}
We pick $A_1, A_2, \ldots, A_n$ to &amp;ldquo;condition on what we wish we knew.&amp;rdquo; These are situations where you don&amp;rsquo;t know $P(B)$, but you know $P(B | A_1), (B | A_2)$, etc.&lt;/p>
&lt;p>&lt;em>&amp;hellip;with extra conditioning&lt;/em>:
\begin{align*}
P(B|C) &amp;amp;= P(B, A_1|C) + P(B, A_2|C) + \cdots + P(B, A_n|C) \\
&amp;amp;= P(B | A_1,C) P(A_1|C) + P(B | A_2,C) P(A_2|C) + \cdots + P(B | A_n,C) P(A_n|C).
\end{align*}
&lt;strong>Bayes&amp;rsquo; Rule&lt;/strong>: for events $A, B$, if we want to calculate $P(B|A)$ but can only know how to calculate $P(A|B)$,
\begin{align*}
P(B|A) &amp;amp;= \frac{P(A|B) P(B)}{P(A)}\\
&amp;amp;= \frac{P(A|B) P(B)}{P(A|B) P(B) + P(A|B^c) P(B^c)},
\end{align*}
where we commonly expand the denominator using the Law of Total Probability (LOTP).&lt;/p>
&lt;p>&lt;em>&amp;hellip;with extra conditioning&lt;/em>:
\begin{align*}
P(B|A, C) &amp;amp;= \frac{P(A|B, C) P(B|C)}{P(A|C)} \\
&amp;amp;= \frac{P(A|B, C) P(B|C)}{P(A|B, C) P(B|C) + P(A|B^c, C) P(B^c |C)}
\end{align*}&lt;/p>
&lt;h3 id="54-independence">5.4 Independence&lt;/h3>
&lt;p>&lt;strong>Independence&lt;/strong>: $A, B$ are defined to be independent if
\begin{align*}
P(A, B) = P(A) P(B).
\end{align*}
Note that if $A, B$ are independent, then so are $A, B^c$ and $A^c, B$, and $A^c, B^c$; basically any functions of $A$ and $B$ are independent.&lt;/p>
&lt;p>&lt;strong>VERY IMPORTANT&lt;/strong>: Disjointness and independence are not the same thing, and disjoint events are in fact usually independent.&lt;/p>
&lt;p>A set of events $A_1, A_2, \ldots, A_n$ is independent if any subset of the events $A_{j_1}, \ldots, A_{j_k}$ follows the equation.
\begin{align*}
P(A_{j_1}, \ldots, A_{j_k}) &amp;amp;= P(A_{j_1}) \cdots P(A_{j_k}).
\end{align*}
Basically, for any combination of independent events, we should be able to factor out the probabilities.&lt;/p>
&lt;p>&lt;strong>Conditional independence&lt;/strong>:
\begin{align*}
P(A, B | C) = P(A|C) P(B|C).
\end{align*}
&lt;strong>VERY IMPORTANT&lt;/strong>: Independence and conditional independence are not the same/do not imply each other. There is no guarantee that independent events are conditionally independent, or vice versa.&lt;/p>
&lt;!-- ## -1. Content to cover
- vandermonde's identity + birthday problem... maybe vibes but not focused. Maybe do a practice problem
- also disease testing example -->
&lt;!-- What skills to develop?
- Being able to make problem-specific calculations
- Identifying when we're independent of conditioning
- Picking what to condition on - what you wish you knew, or a single-step analysis to get you closer/recursive
- Make sure we have scattered examples as well! Maybe will work through practice problems as we go instead of leaving all for the end.
- intuitive limits of values of priors, show that extreme values stay extreme!
- maybe visualize/show bayesian updating as an average b/w prior and posterior?
--></description></item><item><title>Week 1: Counting and Definitions of Probability</title><link>https://srihari-ganesh.github.io/stat110/week1/</link><pubDate>Wed, 30 Aug 2023 00:00:00 +0000</pubDate><guid>https://srihari-ganesh.github.io/stat110/week1/</guid><description>&lt;h2 id="0-info">0. Info&lt;/h2>
&lt;ul>
&lt;li>Section date: 9/13&lt;/li>
&lt;li>Associated lectures: 9/5, 9/7&lt;/li>
&lt;li>Associated pset: Pset 1, due 9/15&lt;/li>
&lt;li>Where to find me
&lt;ul>
&lt;li>Section: Wednesdays 3-4:15pm, see section info on Canvas for location.
&lt;ul>
&lt;li>Section will always cover content for the pset due on Friday (in two days)&lt;/li>
&lt;li>Specifically, this means we&amp;rsquo;ll focus on the lectures from the previous week (six and eight days previous), and not the lecture from the day before section&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Office Hours: Wednesdays 7-9pm, see office hours info on Canvas/Google Calendar for location. I will often stay for the office hours that come right after (Wednesday 9-11pm), and occasionally may drop by at the Thursday 7:30-9:30pm office hours in the same location as well.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Syllabus clarifications
&lt;ul>
&lt;li>Problem Sets: due Fridays at 5pm. They will cover content up to the previous week (e.g., up to the lecture 8 days before the due date).&lt;/li>
&lt;li>Attendance: If your grade ends up being slightly below a grade cutoff, attendance in lecture in section may boost you over. Attendance (or lack thereof) will not be used to penalize.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Me: Srihari Ganesh (senior)
&lt;ul>
&lt;li>Concentrating in Chemical &amp;amp; Physical Biology and Math w/ a master&amp;rsquo;s in Stat&lt;/li>
&lt;li>Doing computational biology research in diffusion models for protein structure generation&lt;/li>
&lt;li>Love teaching Stat + playing IM sports on the side&lt;/li>
&lt;li>Happy to chat about everything though I&amp;rsquo;m not an expert on anything&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="1-set-theory">1. Set Theory&lt;/h2>
&lt;div class="alert alert-note">
&lt;div>
&lt;p>A &lt;strong>set&lt;/strong> is an unordered collection of unique items. For example, $A = \{ 8, 1, 2 \}$ is a set with items (&lt;strong>elements&lt;/strong>) $8$, $1$, and $2$.&lt;/p>
&lt;ul>
&lt;li>Sets are unordered, so $\{8, 1, 2 \} = \{2, 8, 1\}$.&lt;/li>
&lt;li>Sets contain unique elemnts, so $\{8, 1, 2, 1\}$ is NOT a set.&lt;/li>
&lt;li>Sets are a collection of any type of item, so $\{A, B, C\}$, $\{\text{car, cow, walnut}\}$, and $\{$, , $\}$ are all sets.&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/div>
&lt;h3 id="11-definitions">1.1 Definitions&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>$A$ is a &lt;strong>subset&lt;/strong> of $B$ (notated $A \subset B$ or $A \subseteq B$) if every element of $A$ is also an element of $B$.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The &lt;strong>empty set&lt;/strong> (notated $\emptyset$) is the set with no elements, $\{\}$.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The &lt;strong>union&lt;/strong> (notated $A \cup B$) is the set of all elements that are in $A$ or $B$ (or both).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The &lt;strong>intersection&lt;/strong> (notated $A \cap B$) is the set of all elements that are in both $A$ and $B$.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$A$ and $B$ are &lt;strong>disjoint&lt;/strong> if $A$ and $B$ share no elements (notated $A \cap B = \emptyset$)&lt;/p>
&lt;ul>
&lt;li>A collection of sets $A_1, \ldots, A_n$ are &lt;strong>disjoint&lt;/strong> if no pair of sets shares any elements ($A_i \cap A_j = \emptyset$ for all $i, j$)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>A collection of sets $B_1, B_2, \ldots, B_n$ are a &lt;strong>partition&lt;/strong> of $B$ if&lt;/p>
&lt;ul>
&lt;li>$B_1, B_2, \ldots, B_n$ are disjoint, and&lt;/li>
&lt;li>$B_1 \cup B_2 \cup \cdots \cup B_n = B$.&lt;/li>
&lt;/ul>
&lt;p>Intuitively, $B_1, B_2, \ldots, B_n$ are (non-overlapping) puzzle pieces that form the the puzzle, $B$, when put together.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The &lt;strong>complement&lt;/strong> of a set $A$ (notated $A^c$) is the set of all elements not in $A$&lt;/p>
&lt;ul>
&lt;li>Complements have to be taken relative to some superset $S$ (notated $A \subset S$). We think of this set as the &amp;ldquo;universe&amp;rdquo; that $A$ lives in, and is usually implied by context.&lt;/li>
&lt;li>ex. Let $S = \{1, 2, 3 \}, A = \{1, 3\}$. Then $A^c = \{2\}$.&lt;/li>
&lt;li>Note that $A$ and $A^c$ partition $S$.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>The &lt;strong>difference&lt;/strong> between two sets $B \setminus A$ is the set of all elements in $B$ which are not in $A$.&lt;/p>
&lt;ul>
&lt;li>Note that $B \cap A$, $B \setminus A$ partition $B$.&lt;/li>
&lt;li>An equivalent notation is $B \setminus A = B \cap A^c$.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>The &lt;strong>cardinality&lt;/strong> of a set, |A|, indicates the size of the set. For finite sets, it is the number of elements in the set.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>We notate set membership in the following way: letting $A = \{8, 1, 2\}$,&lt;/p>
&lt;ul>
&lt;li>&amp;ldquo;$2$ is in $A$&amp;rdquo; is notated $2 \in A$.&lt;/li>
&lt;li>&amp;ldquo;$0$ is not in $A$&amp;rdquo; is notated $0 \notin A$.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="12-verbal-understanding">1.2 Verbal Understanding&lt;/h3>
&lt;p>We want to think about basic set operations in words.&lt;/p>
&lt;ul>
&lt;li>$x \in A$ reads as &amp;ldquo;$x$ is in $A$&amp;rdquo;.
&lt;ul>
&lt;li>We tend to work with sets by thinking about whether some variable or number is in the (e.g., whether or not $x \in A$) instead of just looking at the set written out like $A = \{8, 1, 2\}$.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>$A \cup B$ reads as &amp;ldquo;$A$ &lt;em>or&lt;/em> $B$&amp;rdquo;. So we can say $x \in A \cup B$ intuitively reads as &amp;ldquo;$x$ is in $A$ &lt;strong>or&lt;/strong> $x$ is in $B$&amp;rdquo;.
&lt;ul>
&lt;li>Be careful to note that we are using the inclusive or, so $A \textit{ or } B$ means $A$, $B$, or both.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>$A \cap B$ reads as &amp;ldquo;$A$ &lt;em>and&lt;/em> $B$&amp;rdquo;. So $x \in A \cap B$ reads as &amp;ldquo;$x$ is in $A$ &lt;strong>and&lt;/strong> $x$ is in $B$&amp;rdquo;.&lt;/li>
&lt;li>$A^c$ reads as &amp;ldquo;not $A$&amp;rdquo;, so $x \in A^c$ reads as &amp;ldquo;$x$ is not in $A$&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;p>We can apply this so see that the two versions of the set difference are equivalent. $x \in B \setminus A$ reads as &amp;ldquo;$x$ is in $B$ but not in $A$&amp;rdquo;, while $x \in B \cap A^c$ reads as &amp;ldquo;$x$ is in B and in $A^c$ (i.e., not in $A$).&amp;rdquo;&lt;/p>
&lt;p>What if we have multiple operations in the same expression?&lt;/p>
&lt;ul>
&lt;li>$x \in (A \cap B)^c$ reads as &amp;ldquo;$x$ is not in both $A$ and $B$&amp;rdquo;. This means that either $x$ is not in $A$ or $x$ is not in $B$, so $x \in A^c \cup B^c$.&lt;/li>
&lt;li>$x \in (A \cup B)^c$ read as &amp;ldquo;$x$ is not in $A$ or $B$&amp;rdquo;. This means that $x$ is not in $A$ and $x$ is not in $B$, so $x \in A^c \cap B^c$.
We see in the next section that these statements are essentially DeMorgan&amp;rsquo;s laws.&lt;/li>
&lt;/ul>
&lt;h3 id="13-demorgans-laws">1.3 DeMorgan&amp;rsquo;s Laws&lt;/h3>
&lt;div class="alert alert-note">
&lt;div>
&lt;strong>DeMorgan&amp;rsquo;s Laws&lt;/strong>
\begin{align}
(A \cap B)^c &amp;amp;= A^c \cup B^c \\
(A \cup B)^c &amp;amp;= A^c \cap B^c
\end{align}
&lt;/div>
&lt;/div>
&lt;p>By talking through the expressions, you should eventually be able to convince yourself that the laws are true.&lt;br>
These videos also walk through the process visually:
&lt;video controls >
&lt;source src="https://srihari-ganesh.github.io/media/videos/Section1_DeMorgan1.mp4" type="video/mp4">
&lt;/video>
&lt;video controls >
&lt;source src="https://srihari-ganesh.github.io/media/videos/Section1_DeMorgan2.mp4" type="video/mp4">
&lt;/video>
&lt;/p>
&lt;h2 id="2-counting">2. Counting&lt;/h2>
&lt;h3 id="20-some-useful-problem-solving-strategies-lifted-from-95-lecture">2.0 Some useful problem-solving strategies (lifted from 9/5 lecture)&lt;/h3>
&lt;ul>
&lt;li>Try simple/extreme cases (i.e., set $n$ to be a small number and brute-force)
&lt;ul>
&lt;li>For each tool below, we&amp;rsquo;ll motivate it with an example. You can click the drop-down to view the example.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Give objects of interest &lt;em>names&lt;/em> or &lt;em>ID numbers&lt;/em>&lt;/li>
&lt;li>Draw a diagram&lt;/li>
&lt;li>Make a story&lt;/li>
&lt;/ul>
&lt;h3 id="21-multiplication-rule">2.1 Multiplication Rule&lt;/h3>
&lt;p>&lt;strong>How many different combinations can you make?&lt;/strong>&lt;br>
&lt;details class="spoiler " id="spoiler-4">
&lt;summary>&lt;ins>Click for example&lt;/ins>&lt;/summary>
&lt;p>&lt;p>If an ice cream parlor has $4$ different flavors and $3$ different cup sizes, there are $4 \times 3 = 12$ different orders I could make.&lt;/p>
&lt;p>This example exhibits all of the above problem-solving strategies. We took the general multiplication rule (see general statement below) and, made a story about an ice cream parlor, and set $a = 4$, $b=3$. We drew a diagram (the tree below), and gave the objects (flavors and cup sizes) names in that diagram.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Tree diagram of the ice cream parlor example, combining four ice cream flavors and 3 cup sizes to yield 4 times 3 = 12 possible orders." srcset="
/media/images/Section1_Tree_hu9544750ee73ba0725668d11c52a82e97_147874_29105694be846087f9bc432364e890bc.webp 400w,
/media/images/Section1_Tree_hu9544750ee73ba0725668d11c52a82e97_147874_9b2c3ae9f8dc6a45b203c506f9118536.webp 760w,
/media/images/Section1_Tree_hu9544750ee73ba0725668d11c52a82e97_147874_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://srihari-ganesh.github.io/media/images/Section1_Tree_hu9544750ee73ba0725668d11c52a82e97_147874_29105694be846087f9bc432364e890bc.webp"
width="760"
height="428"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;/p>
&lt;/details>&lt;/p>
&lt;p>&lt;ins>General statement&lt;/ins>: Say the outcome of a full experiment is given by the outcomes of two-subexperiments, $A$ and $B$. If sub-experiment $A$ has $a$ possible outcomes and sub-experiment $B$ has $b$ possible outcomes, then the full experiment has $a \times b$ possible outcomes.&lt;/p>
&lt;ul>
&lt;li>Note here that this will work as long as the outcome of $A$ doesn&amp;rsquo;t change the &lt;em>number&lt;/em> of possible outcomes of $B$, even if $A$ changes &lt;em>which&lt;/em> outcomes are possible for $B$.&lt;/li>
&lt;/ul>
&lt;h3 id="22-factorial">2.2 Factorial&lt;/h3>
&lt;p>&lt;strong>How many ways can you order $n$ distinct items?&lt;/strong>&lt;br>
&lt;details class="spoiler " id="spoiler-5">
&lt;summary>&lt;ins>Click for example&lt;/ins>&lt;/summary>
&lt;p>How many ways can $n=52$ card deck of cards be shuffled? I have $52$ options for the first card, then $52-1 = 51$ options for the second card, $52-2 = 50$ options for the third card, and so on. The total number of ways is thus $$52 \times 51 \times 50 \times \cdots \times 2 \times 1.$$&lt;/p>
&lt;/details>&lt;/p>
&lt;p>&lt;ins>General statement&lt;/ins>: We use the factorial, $n!$, as a shorthand for this specific application of the multiplication rule. We have $n$ choices for the first item; we can then pick $(n-1)$ choices for the second item, and so on, giving
$$
n! = n \times (n-1) \times (n-2) \cdots \times 2 \times 1.
$$&lt;/p>
&lt;h3 id="23-permutations">2.3 Permutations&lt;/h3>
&lt;p>&lt;strong>How many ways can you choose an ordered collection of $k$ distinct items from a set of $n$ distinct items?&lt;/strong>&lt;br>
&lt;details class="spoiler " id="spoiler-6">
&lt;summary>&lt;ins>Click for example&lt;/ins>&lt;/summary>
&lt;p>I want to take a $k=3$ week trip in New England, spending each week in a different state. New England has $n=6$ states, so using the multiplication rule, I have $6 \times 5 \times 4 = 120$ possible trips I can go on.&lt;/p>
&lt;/details>
&lt;ins>General statement&lt;/ins>: Using the multiplication rule, the solution is
$$
n \times (n-1) \times \cdots \times (n-k+1) = \frac{n!}{(n-k)!}.
$$&lt;/p>
&lt;div class="alert alert-warning">
&lt;div>
&lt;p>Be careful with off-by-one errors when you&amp;rsquo;re listing consecutive integers.&lt;/p>
&lt;p>For a list of $k$ consecutive integers, the first and last numbers should have a difference of $k-1$.&lt;/p>
&lt;p>For example, the list &amp;ldquo;$3, 4, 5, 6$&amp;rdquo; has $4$ items, so the difference between the first and last numbers if $6-3 = 3 = 4-1$.&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>How many ways can you order a set of $n$ items, where some items are repeated?&lt;/strong>&lt;br>
&lt;details class="spoiler " id="spoiler-8">
&lt;summary>&lt;ins>Click for example&lt;/ins>&lt;/summary>
&lt;p>&lt;p>How many anagrams (orderings/permutations) are there of the word &amp;ldquo;BANANA&amp;rdquo;? In other words, how many $n=6$ letter words have 3 A&amp;rsquo;s, 1 B, and 2 N&amp;rsquo;s?&lt;/p>
&lt;p>If we treated the letters as distinct, there would be $6!$ ways to order them. However, the 3 A&amp;rsquo;s are identical, so we are overcounting. For example, if we artificially distinguish the A&amp;rsquo;s, we see that&lt;br>
&amp;ldquo;B&lt;em>A&lt;/em>N&lt;strong>A&lt;/strong>N&lt;ins>A&lt;/ins>&amp;rdquo;, &amp;ldquo;B&lt;strong>A&lt;/strong>N&lt;em>A&lt;/em>N&lt;ins>A&lt;/ins>&amp;rdquo;, &amp;ldquo;B&lt;ins>A&lt;/ins>N&lt;strong>A&lt;/strong>N&lt;em>A&lt;/em>&amp;rdquo;, &amp;ldquo;B&lt;ins>A&lt;/ins>N&lt;em>A&lt;/em>N&lt;strong>A&lt;/strong>&amp;rdquo;, &amp;ldquo;B&lt;strong>A&lt;/strong>N&lt;ins>A&lt;/ins>N&lt;em>A&lt;/em>&amp;rdquo;, and &amp;ldquo;B&lt;em>A&lt;/em>N&lt;ins>A&lt;/ins>N&lt;strong>A&lt;/strong>&amp;rdquo;&lt;br>
are all counted as different words. This means there we are overcounting the orderings of the 3 A&amp;rsquo;s by $3!$, so we can divide to get $6!/3!$.&lt;br>
Similarly, we are overcounting the orderings of the 2 N&amp;rsquo;s by $2!$. There is only one B, so we are not overcounting that. This gives a final answer of $\frac{6!}{3!2!} = 120$ orderings.&lt;/p>
&lt;/p>
&lt;/details>&lt;/p>
&lt;p>&lt;ins>General statement&lt;/ins>: say we have $n$ items total with $m$ types, made up of $n_1$ items of type 1, $n_2$ items of type 2, and so forth, with $n = \sum_{i=1}^m n_i$. Then the number of permutations is
\begin{equation}
\frac{n!}{n_1! n_2! \cdots n_m!}.
\end{equation}&lt;/p>
&lt;h3 id="24-binomial-coefficient">2.4 Binomial Coefficient&lt;/h3>
&lt;p>&lt;strong>How many ways can you choose an unordered group of $k$ items, without replacement, from a set of $n$ distinct items?&lt;/strong>&lt;br>
In order words, if a set has $n$ elements, how many subsets with exactly $k$ elements does it have?&lt;/p>
&lt;details class="spoiler " id="spoiler-9">
&lt;summary>&lt;ins>Click for example&lt;/ins>&lt;/summary>
&lt;p>&lt;p>From my $n=4$ nuclear family members, how many ways can I choose $k=2$ of them to go on an all-expenses paid trip to Cambridge, Massachusetts with me?&lt;/p>
&lt;p>I can use the multiplication rule, so I have $4$ options for the first family member and $3$ options for the second. However, I don&amp;rsquo;t care about the order, so I can divide by $2!$ to get $\frac{4 \times 3}{2!} = 6$ unordered groups of $2$ family members.&lt;/p>
&lt;/p>
&lt;/details>
&lt;p>&lt;ins> General statement&lt;/ins>: &lt;a href="#23-permutations">Using our previous permutation result&lt;/a>, we can first pick $k$ items in order: $n \times (n-1) \times \cdots \times (n-k+1) = \frac{n!}{(n-k)!}$.&lt;/p>
&lt;p>Since we don&amp;rsquo;t care about the order, we can then correct our overcounting by dividing by $k!$ to get the answer, which is called the &lt;strong>binomial coefficient&lt;/strong>:
\begin{equation}
\binom{n}{k} = \frac{n!}{k!(n-k)!}.
\end{equation}&lt;/p>
&lt;h3 id="25-counting-subsets">2.5 Counting Subsets&lt;/h3>
&lt;p>&lt;strong>How many ways can you select an unordered group of items (of any size, including $0$), without replacement, from a set of $n$ distinct items?&lt;/strong>&lt;br>
Alternatively worded, given a set of size $n$, how many different subsets can you come up with (including the empty set)?&lt;/p>
&lt;p>The first item of the set is either in the the group or not, so there are 2 possibilities. This applies for each item in the set &amp;mdash; it is either in the group or not. We can apply the multiplication rule to get the that the number of possible groups is
\begin{equation}
2 \times 2 \times \cdots \times 2 = 2^n.
\end{equation}&lt;/p>
&lt;h3 id="26-bose-einstein">2.6 Bose-Einstein&lt;/h3>
&lt;p>&lt;strong>How many ways can you choose an unordered group of $k$ items, with replacement, from a set of $n$ items?&lt;/strong>&lt;/p>
&lt;details class="spoiler " id="spoiler-10">
&lt;summary>&lt;ins>Click for example&lt;/ins>&lt;/summary>
&lt;p>&lt;p>At a donut shop with $n=3$ different flavors, how many ways distinct orders of $k=5$ donuts can I make?&lt;/p>
&lt;p>Let&amp;rsquo;s start with smaller values of $n$ and up the complexity. With only $n=1$ flavor, there&amp;rsquo;s only one possible order (all donuts of one flavor). With $n=2$ flavors (e.g., frosted and glazed), the order is uniquely determined by how many frosted donuts there are; for example, if we have $3$ frosted donuts, we know there must be $5-3$ glazed donuts. In the example below, we say that the donuts to the left of the divider are frosted, and those to the right are glazed.&lt;/p>
&lt;video controls >
&lt;source src="https://srihari-ganesh.github.io/media/videos/Section1_BE1.mp4" type="video/mp4">
&lt;/video>
&lt;p>For larger $n$, we follow a similar idea and count in a clever way. Your order can be uniquely represented by how many donuts of each flavor you buy, so we can think of placing dividers between donuts to uniquely represent our order. Each &amp;ldquo;box&amp;rdquo; created by the dividers represents a flavor: those to the left of the first divider are frosted, between the first and second divider are glazed, between the second and third divider are jam, etc.&lt;/p>
&lt;video controls >
&lt;source src="https://srihari-ganesh.github.io/media/videos/Section1_BE2.mp4" type="video/mp4">
&lt;/video>
&lt;p>Note that we need $4-1 = 3$ dividers to create $4$ categories. So how do we count ways to put $2$ dividers among $5$ donuts? We&amp;rsquo;ll count by treating dividers as distinguishable, then correct for the overcounting later. We have $5+1=6$ options for where to put the first divider (to the left of the first donut, between the first and second donut, etc.). The second divider has $5+1+1=7$ optinos, since there are now $6$ items ($5$ donuts, $1$ divider) that we can go between or to the left/right of. The third divider has $5+2+1=8$ options. By this pattern, we have $6 \times 7 \times 8$ ways to place dividers. However, the dividers themselves don&amp;rsquo;t tell us anything about how the donuts are divied up - only their locations - so we don&amp;rsquo;t care about the order they were placed. Thus, we divide by $3!$ ways to order the dividers to get $\frac{6 \times 7 \times 8}{3!} = \binom{8}{3}$.&lt;/p>
&lt;p>Another way to think about this is that we have $5 + (4-1) = 8$ &amp;ldquo;slots&amp;rdquo; (donuts and dividers) in a row, and we need to pick out the locations of the $3$ dividers. We can use the binomial coefficient to do this in $\binom{8}{3}$ ways.&lt;/p>
&lt;/p>
&lt;/details>
&lt;p>I encourage you to check out the example above for much of the derivation of the Bose-Einstein counting method.&lt;/p>
&lt;p>&lt;ins>General statement&lt;/ins>: We are trying to split the $k$ items into $n$ categories, which we can do by placing $n-1$ dividers between the $k$ items. We can re-frame this problem as selecting the locations of $n-1$ dividers from $(n-1)+k$ slots (items + dividers), which means our solution is $$\binom{n-1+k}{n-1} = \binom{n-1+k}{k}.$$&lt;/p>
&lt;p>Note: Bose-Einstein is a bit of a &amp;ldquo;big hammer&amp;rdquo; &amp;mdash; it can solve some pretty complicated problems, but also some simpler ones. You should make sure you have exhausted your other tools, and that your problem matches the &amp;ldquo;unordered, with replacement&amp;rdquo; requirement before trying this out.&lt;/p>
&lt;h2 id="3-definitions-of-probability">3. Definitions of Probability&lt;/h2>
&lt;ul>
&lt;li>The &lt;strong>sample space&lt;/strong> $S$ is the set of all possible &lt;strong>outcomes&lt;/strong> of an experiment (i.e., each element of $S$ is a unique outcome).
&lt;ul>
&lt;li>After an experiment is completed, exactly one of the outcomes will have occurred.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>An &lt;strong>event&lt;/strong> $A$ is any subset of $S$ ($A \subset S$) - anything that we could say either &amp;ldquo;happened&amp;rdquo; or &amp;ldquo;didn&amp;rsquo;t happen&amp;rdquo; once the experiment takes place. We calculate the probabilities of events (e.g., the probability that an event $A$ happens).
&lt;ul>
&lt;li>After an experiment is completed, every event will have
&lt;ul>
&lt;li>&amp;ldquo;Happened&amp;rdquo; or &amp;ldquo;happened&amp;rdquo; if it contains the outcome which actually occurred.&lt;/li>
&lt;li>&amp;ldquo;Not happened&amp;rdquo; or &amp;ldquo;not occurred&amp;rdquo; if it does not contain the outcome which actually occured.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="31-naive-definition-of-probability">3.1 Naive Definition of Probability&lt;/h3>
&lt;details class="spoiler " id="spoiler-11">
&lt;summary>&lt;ins>Click for example&lt;/ins>&lt;/summary>
&lt;p>&lt;p>When rolling a standard die, the sample space is typically denoted by $S = \{1, 2, 3, 4, 5, 6\}$, where each element of $S$ is a possible outcome.&lt;/p>
&lt;p>Possible events could be rolling a $1$ (the event being $A = \{1\}$), rolling an odd number, $B = \{1, 3, 5\}$, or rolling a small number $C = \{1, 2, 3\}$.&lt;/p>
&lt;p>For a fair die, you might intuitively know that the probability of $A$ is $1/6$, the probability of $B$ is $3/6=1/2$, and the probability of $C$ is $3/6=1/2$. Also see that it&amp;rsquo;s possible that the events $A, B, C$ all occur, even though only one outcome actually occur.&lt;/p>
&lt;video controls >
&lt;source src="https://srihari-ganesh.github.io/media/videos/Section1_Events.mp4" type="video/mp4">
&lt;/video>
&lt;/p>
&lt;/details>
&lt;div class="alert alert-note">
&lt;div>
&lt;strong>Naive definition of probability&lt;/strong>:&lt;br>
We assume that there are a finite number of outcomes (the sample space, $S$, is finite), and that each individual outcome has an equal probability of occuring. Then the probability of an event $A$ is.
$$
P_{\text{naive}}(A) = \frac{\text{# of outcomes in $A$}}{\text{# of outcomes in $S$}} = \frac{|A|}{|S|}.
$$
&lt;/div>
&lt;/div>
&lt;p>Limitations:&lt;/p>
&lt;ul>
&lt;li>Assumes that all outcomes are equally likely
&lt;ul>
&lt;li>Shaky definition of what &amp;ldquo;equally likely&amp;rdquo; means because we can arbitrarily construct the outcomes.&lt;/li>
&lt;li>&lt;em>ex.&lt;/em> Say the weather tomorrow has two possible outcomes: $\{\text{precipitation}, \text{no precipitation}\}$. By the naive definition, there&amp;rsquo;s a $50%$ chance of no precipitation.&lt;br>
Alternatively, I could have said the possible outcomes are $\{\text{rain}, \text{sleet}, \text{hail}, \text{snow}, \text{no precipitation}\}$. Then there&amp;rsquo;s only a $20%$ chance of no precipitation&amp;hellip; so which is it?&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Assumes the sample space is finite
&lt;ul>
&lt;li>What if the sample space is of possible temperatures? There are infinitely many possible temperatures, they can&amp;rsquo;t each have a $1/\infty$ probability.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="32-general-definition-of-probability">3.2 General Definition of Probability&lt;/h3>
&lt;div class="alert alert-note">
&lt;div>
&lt;p>&lt;strong>Definition of probability&lt;/strong>:&lt;br>
There are just two axioms (rules that probabilities have to follow):&lt;/p>
&lt;ol>
&lt;li>$P(S) = 1, P(\emptyset) = 0.$&lt;/li>
&lt;li>If events $A_1, A_2, \ldots$ are disjoint, then $$
P\left( \bigcup_{j=1}^\infty A_j\right) = \sum_{j=1}^\infty P(A_j).
$$&lt;/li>
&lt;/ol>
&lt;p>In other words, if $A_1, A_2, \ldots$ partition some event $B$, then $P(B) = \sum_{j=1}^\infty P(A_j)$.&lt;/p>
&lt;/div>
&lt;/div>
&lt;h4 id="321-probability-of-complements">3.2.1 Probability of Complements&lt;/h4>
&lt;p>Let&amp;rsquo;s say we know the probability of some event $A$. Then $$P(A^c) = 1 - P(A).$$ This follows directly from the axioms of probability: since $A^c, A$ partition the sample space $S$, we know $P(A) + P(A^c) = P(S) = 1$. When you&amp;rsquo;re asked to calculate the probability of events in the form of &amp;ldquo;at least&amp;rdquo; or &amp;ldquo;at most&amp;rdquo;, the probability of the complement may sometimes be simpler to calculate.&lt;/p>
&lt;h4 id="322-principle-of-inclusion-exclusion-pieprobability-of-unions">3.2.2 Principle of Inclusion-Exclusion (PIE)/Probability of unions&lt;/h4>
&lt;p>Say we have two events (not necessarily disjoint) $A$ and $B$. We can partition $A \cup B$ into $A \setminus B, A \cap B, B \setminus A$.
\begin{align*}
P(A \cup B) &amp;amp;= P(A \setminus B) + P(A \cap B) + P(B \setminus A)\\
P(A \cup B) &amp;amp;= P(A \setminus B) + P(A \cap B) + P(B \setminus A) + P (A \cap B) - P(A \cap B)\\
P(A \cup B) &amp;amp;= P(A) + P(B) - P(A \cap B),
\end{align*}
where we apply the second axiom of probability using the fact that $A \setminus B, A \cap B$ partition $A$ and the fact that $B \setminus A, A \cap B$ partition $B$.&lt;/p>
&lt;p>This extends to finding the probability of the union of any $n$ events, which is called the &lt;strong>Principle of Inclusion-Exclusion (PIE)&lt;/strong>
\begin{align*}
P(A_1 \cup A_2 \cup \cdots \cup A_n) &amp;amp;= \sum_{i=1}^n P(A_i) - \sum_{i&amp;lt;j} P(A_i \cap A_j) + \sum_{i &amp;lt; j &amp;lt; k} P(A_i \cap A_j \cap A_k) \\
&amp;amp;- \cdots + (-1)^{n-1} P(A_1 \cap A_2 \cap \cdots \cap A_n).
\end{align*}&lt;/p>
&lt;p>We are a bit careless with notation in the sums: $\sum_{i&amp;lt;j}$ means that we sum over every (unordered) pair of events. This formula is better remembered in words: add up the probabilities of all individual events, subtract the probabilities of every pair, add back the probabilities of every triple, etc.; remember that you start positive, then alternate signs.&lt;/p>
&lt;p>PIE is another tool that is a big hammer: it will always work if you&amp;rsquo;re finding the probability of a union (the probability that at least one event occurs), but it is unwieldy for most problems and should be one of your last resorts.&lt;/p>
&lt;h2 id="4-summary">4. Summary&lt;/h2>
&lt;h3 id="41-set-theory">4.1 Set Theory&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Sets&lt;/strong>: unordered collections of items&lt;/li>
&lt;li>Important set operations: intersection, union, complement, difference, cardinality&lt;/li>
&lt;li>Important properties: set membership, subsets, disjoint sets, partitions&lt;/li>
&lt;li>Theorems: DeMorgan&amp;rsquo;s laws&lt;/li>
&lt;/ul>
&lt;h3 id="42-counting">4.2 Counting&lt;/h3>
&lt;p>All of our counting is driven by the multiplication rule. Here&amp;rsquo;s what we&amp;rsquo;ve built from that, working from a set of $n$ distinct items:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>select how many items?&lt;/th>
&lt;th>ordered or unordered?&lt;/th>
&lt;th>with or without replacement?&lt;/th>
&lt;th>solution&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>take all $n$ items,&lt;/td>
&lt;td>order the items,&lt;/td>
&lt;td>without replacement&lt;/td>
&lt;td>n!&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>select $k$ items,&lt;/td>
&lt;td>ordered,&lt;/td>
&lt;td>without replacement&lt;/td>
&lt;td>$\frac{n!}{(n-k)!}$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>select $k$ items,&lt;/td>
&lt;td>unordered,&lt;/td>
&lt;td>without replacment&lt;/td>
&lt;td>$\binom{n}{k}$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>select any number of items,&lt;/td>
&lt;td>unordered,&lt;/td>
&lt;td>without replacement&lt;/td>
&lt;td>$2^n$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>select $k$ items,&lt;/td>
&lt;td>unordered,&lt;/td>
&lt;td>with replacement&lt;/td>
&lt;td>$\binom{n+k-1}{k}$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>select $k$ items,&lt;/td>
&lt;td>ordered,&lt;/td>
&lt;td>with replacement&lt;/td>
&lt;td>$n^k$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>You may also have seen this sampling table (below) in lecture (for counting the numbers of ways to draw a sample of $k$ items from $n$ distinct items). I suggest breaking down every counting problem into whether order matters and whether you are sampling with replacement, and trying out the corresponding approach in the table.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Order matters&lt;/th>
&lt;th>Order doesn&amp;rsquo;t matter&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>With replacement&lt;/td>
&lt;td>$n^k$&lt;/td>
&lt;td>$\binom{n+k-1}{k}$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Without replacement&lt;/td>
&lt;td>$\frac{n!}{(n-k)!}$&lt;/td>
&lt;td>$\binom{n}{k}$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>We also talked about the permutation problem with $n$ items that aren&amp;rsquo;t necessarily distinct (order all $n$ items without replacement), where we compensate for overcounting after the fact: $$\frac{n!}{n_1! n_2! \cdots n_m!}.$$&lt;/p>
&lt;h3 id="43-definitions-of-probability">4.3 Definitions of Probability&lt;/h3>
&lt;p>&lt;strong>Naive probability&lt;/strong>: probably the most intuitive (and yet very flawed) way to define probability: the probability is proportional to the number of outcomes. For example, there are 3 ways to roll an even number on a die, so by this definition, there is a $3/6 = 1/2$ probability of rolling an even number.&lt;/p>
&lt;p>&lt;strong>(Non-naive) probability&lt;/strong>: given by two axioms:&lt;/p>
&lt;ol>
&lt;li>$P(S) = 1$&lt;/li>
&lt;li>$P\left(\bigcup_{j=1}^\infty A_j \right) = \sum_{j=1}^\infty P(A_j)$ for $A_1, A_2, \ldots$ disjoint.&lt;/li>
&lt;/ol>
&lt;p>The principle of inclusion-exclusion and the probability of complements follow from the axioms.&lt;/p>
&lt;h2 id="5-practice-problems">5. Practice Problems&lt;/h2>
&lt;p>&lt;a href="https://srihari-ganesh.github.io/uploads/stat110/Section%201%20Practice%20Problems.pdf" target="_blank">Practice Problem PDF&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://srihari-ganesh.github.io/uploads/stat110/Section%201%20Solutions.pdf" target="_blank">Practice Problem Solutions PDF&lt;/a>&lt;/p>
&lt;ol>
&lt;li>
&lt;p>I am interested in five subjects (statistics, math, computer science, chemistry, and biology) and am deciding between 100, 102, 110, and 111 in each department (e.g., my options are Stat 100, Stat 102, Stat 110, Stat 111, Math 100, Math 102, etc.). Assume unless stated otherwise that each class (or set of classes) is equally likely to be chosen if possible.&lt;/p>
&lt;ol>
&lt;li>How many different schedules of four courses can I take?&lt;/li>
&lt;li>I want to send my friend a text of my 4-class schedule. My text will look be a list like &amp;ldquo;Chemistry 101, Stat 110, Biology 111, Math 101&amp;rdquo;, which is considered a different text from &amp;ldquo;Stat 110, Chemistry 101, Biology 111, Math 101&amp;rdquo;. How many different such texts could I send?&lt;/li>
&lt;li>If I take 5 courses, what is the probability that I take exactly 1 course from each department?&lt;/li>
&lt;li>My advisor wants to know that I have backup plans, so they ask me to send 2 schedules with 4 courses each, where the schedules can have no overlap. How many such sets of two schedules can I send my advisor?&lt;/li>
&lt;li>After I finalize my schedule, my advisor now wants to know how many classes I&amp;rsquo;m taking in each subject, so I send them a list that follows the following format of the following example (with the subjects always in this order):&lt;br>
&lt;br>
[Statistics: 2, Math: 0, Computer Science: 1, Chemistry: 1, Biology: 1]&lt;br>
&lt;br>
How many such lists can I send for a 4-class schedule?&lt;/li>
&lt;li>If I take 6 courses, what is the probability that there&amp;rsquo;s at least 1 subject in which I am not taking any courses? Solve this problem in two ways.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>&lt;em>(Problem statement taken from Rachel Li and Ginnie Ma&amp;rsquo;s notes)&lt;/em> Hockey Stick Identity: show using a story proof that
$$
\sum_{j=k}^n \binom{j}{k} = \binom{n+1}{k+1}.
$$
Hint: Imagine arranging a group of people by age, and then think about the oldest person in a chosen subgroup.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Use problem 1.4 to construct a story proof for the following identity (assuming $n \ge 2k$). Recall that in problem 1.4, my advisor asked me to make 2 (non-overlapping) schedules with 4 courses each.
$$
\binom{n}{2k} \binom{2k}{k} = \binom{n}{k}\binom{n-k}{k}
$$&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="51-extra-practice-problems-if-you-have-time-during-section">5.1 Extra practice problems (if you have time during section)&lt;/h3>
&lt;p>&lt;em>Taken from the section materials of Rachel Li and Ginnie Ma (&amp;lsquo;23)&lt;/em>&lt;/p>
&lt;ol start="4">
&lt;li>
&lt;p>(&lt;strong>Lattice Walks&lt;/strong>) You start at the origin of a grid. Assuming you can only move up or to the right one unit at a time, how many different paths exist between you and position $(10, 10)$? Next, assume you are in a 3D-space. How many different paths exist between you and position $(10, 10, 10)$? Last, say your motions are defined by a 2-dimensional random walk: where you either walk right or up one unit with equal probability. After 20 steps, what is the probability that you are at position $(10, 10)$?&lt;/p>
&lt;/li>
&lt;li>
&lt;p>(&lt;strong>Coin Toss Gambling&lt;/strong>, &lt;em>Question Credit: Zhou, 2008&lt;/em>) Two gamblers are playing a coin toss game. Gambler A has $(n + 1)$ fair coins. Gambler B has $n$ fair coins. What is the probability that A will have more heads than B if they all flip their coins? &lt;br>
&lt;em>Hint: What are the possible results (events) if we compare the number of heads in As first $n$ coins with Bs $n$ coins? By making the number of coins equal, we can take advantage of symmetry. For each event, what will happen if As last coin is a head? Or Tails?&lt;/em>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>(&lt;strong>Teenager Steps&lt;/strong>) A teenage boy recently went through his first growth spurt, and can now walk up stairs one or two stairs at a time. How many different ways are there for him to ascend to the top of a staircase with $n$ stairs? &lt;br>
&lt;em>Hint: start with small $n$. The solution should look familiar!&lt;/em>&lt;/p>
&lt;/li>
&lt;/ol></description></item><item><title>Stat 110 Section Notes</title><link>https://srihari-ganesh.github.io/projects/stat110/</link><pubDate>Tue, 30 Aug 2022 00:00:00 +0000</pubDate><guid>https://srihari-ganesh.github.io/projects/stat110/</guid><description/></item><item><title>A Local Existence and Uniqueness Theorem for First Order Differential Equations</title><link>https://srihari-ganesh.github.io/projects/math22b/</link><pubDate>Sat, 08 May 2021 00:00:00 +0000</pubDate><guid>https://srihari-ganesh.github.io/projects/math22b/</guid><description>&lt;p>As a student in Math 22b, taught by &lt;a href="https://www.math.harvard.edu/people/grundmeier-dusty/" id="Harvard" type="link">&lt;b>Dr. Dusty Grundmeier&lt;/b>&lt;/a>, I learned introductory proof-based vector calculus with some exposure to limits and continuity. However, I was interested in getting more exposure to real analysis, so for my final project I learned about existence and uniqueness theorems for differential equations and wrote an expository paper presenting a simple case of such a theorem.&lt;/p></description></item></channel></rss>
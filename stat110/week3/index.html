<!doctype html><!-- This site was created with Wowchemy. https://www.wowchemy.com --><!-- Last Published: February 10, 2024 --><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&family=Merriweather&family=Roboto+Mono&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&family=Merriweather&family=Roboto+Mono&display=swap" media=print onload='this.media="all"'><script src=/js/mathjax-config.js></script>
<link rel=stylesheet href=/css/vendor-bundle.min.047268c6dd09ad74ba54a0ba71837064.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script>
<link rel=stylesheet href=/css/wowchemy.9e3515c7534a90d8339ca9703c3e3bce.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Srihari Ganesh"><meta name=description content="Lecture dates: 9/19, 9/21; Section date: 9/27"><link rel=alternate hreflang=en-us href=https://srihari-ganesh.github.io/stat110/week3/><link rel=canonical href=https://srihari-ganesh.github.io/stat110/week3/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu238e72ff877ecedb95a45ec9a2f2431b_15216_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu238e72ff877ecedb95a45ec9a2f2431b_15216_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#707070"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@wowchemy"><meta property="twitter:creator" content="@wowchemy"><meta property="twitter:image" content="https://srihari-ganesh.github.io/media/icon_hu238e72ff877ecedb95a45ec9a2f2431b_15216_512x512_fill_lanczos_center_3.png"><meta property="og:site_name" content="Srihari Ganesh"><meta property="og:url" content="https://srihari-ganesh.github.io/stat110/week3/"><meta property="og:title" content="Week 3: Conditional Probability Examples and Random Variables | Srihari Ganesh"><meta property="og:description" content="Lecture dates: 9/19, 9/21; Section date: 9/27"><meta property="og:image" content="https://srihari-ganesh.github.io/media/icon_hu238e72ff877ecedb95a45ec9a2f2431b_15216_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2023-09-26T00:00:00+00:00"><meta property="article:modified_time" content="2023-09-26T00:00:00+00:00"><title>Week 3: Conditional Probability Examples and Random Variables | Srihari Ganesh</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=0eb0524cfeceda00a2289760a17f5467><script src=/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js></script><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Srihari Ganesh</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Srihari Ganesh</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#projects><span>Projects</span></a></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Teaching</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/#teaching><span>Experience</span></a>
<a class=dropdown-item href=/stat110><span>Stat 110 Section Notes</span></a></div></li><li class=nav-item><a class=nav-link href=/#contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><div class="container-fluid docs"><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 docs-sidebar"><form class="docs-search d-flex align-items-center"><button class="btn docs-toggle d-md-none p-0 mr-md-3 w-100" type=button data-toggle=collapse data-target=#docs-nav aria-controls=docs-nav aria-expanded=false aria-label="Toggle section navigation"><div class=d-flex><span class="d-md-none pl-1 flex-grow-1 text-left overflow-hidden">Stat 110 Section Notes</span>
<span><i class="fas fa-chevron-down"></i></span></div></button></form><nav class="collapse docs-links" id=docs-nav><ul class="nav docs-sidenav"><li><a href=/stat110/>Stat 110 Section Notes</a></li><li><a href=/stat110/week11/>Week 11: Markov Chains</a></li><li><a href=/stat110/week10/>Week 10: Adam's & Eve's Laws, Inequalities, and Limit Theorems</a></li><li><a href=/stat110/week9/>Week 9: Transformations, Gamma/Beta, Conditional Expectation</a></li><li><a href=/stat110/week8/>Week 8: Multinomial, Multivariate Normal</a></li><li><a href=/stat110/week7/>Week 7: Poisson Processes, Joint Distributions, and Covariance</a></li><li><a href=/stat110/week6/>Week 6: Universality of the Uniform, Normal, Expo, and Moments</a></li><li><a href=/stat110/week5/>Week 5: Continuous Distributions</a></li><li><a href=/stat110/week4/>Week 4: Discrete Distributions and Expectation</a></li><li class=active><a href=/stat110/week3/>Week 3: Conditional Probability Examples and Random Variables</a></li><li><a href=/stat110/week2/>Week 2: Conditional Probability</a></li><li><a href=/stat110/week1/>Week 1: Counting and Definitions of Probability</a></li></ul></nav></div><div class="d-none d-xl-block col-xl-2 docs-toc"><ul class="nav toc-top"><li><a href=# id=back_to_top class=docs-toc-title>Contents</a></li></ul><nav id=TableOfContents><ul><li><a href=#0-logistical-info>0. Logistical Info</a><ul><li><a href=#01-summary--practice-problem-pdfs>0.1 Summary + Practice Problem PDFs</a></li></ul></li><li><a href=#1-examples-from-class>1. Examples from class</a><ul><li><a href=#11-winter-girl-examples-225-227-in-the-book>1.1 Winter girl (Examples 2.2.5-2.2.7 in the book)</a></li><li><a href=#12-monty-hall-example-271-and-many-practice-problems-in-the-book>1.2 Monty Hall (Example 2.7.1 and many practice problems in the book)</a></li><li><a href=#13-simpsons-paradox-example-283-in-the-book>1.3 Simpson&rsquo;s paradox (Example 2.8.3 in the book)</a></li><li><a href=#14-gamblers-ruin-example-273-in-the-book>1.4 Gambler&rsquo;s ruin (Example 2.7.3 in the book)</a></li></ul></li><li><a href=#2-random-variables>2. Random variables</a><ul><li><a href=#21-definition>2.1 Definition</a></li><li><a href=#22-defining-discrete-random-variables>2.2 Defining discrete random variables</a></li><li><a href=#23-a-more-mathy-definition-of-random-variables>2.3 A more mathy definition of random variables</a></li></ul></li><li><a href=#3-distributions>3. Distributions</a><ul><li><a href=#31-bernoulli-distribution>3.1 Bernoulli Distribution</a></li><li><a href=#32-binomial-distribution>3.2 Binomial Distribution</a></li></ul></li><li><a href=#4-summary>4. Summary</a><ul><li><a href=#41-examples-from-class>4.1 Examples from class</a></li><li><a href=#42-random-variables>4.2 Random variables</a></li><li><a href=#43-distributions>4.3 Distributions</a></li></ul></li></ul></nav></div><main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role=main><div class=docs-article-container></div><div class=docs-article-container><h1>Week 3: Conditional Probability Examples and Random Variables</h1><article class=article-style><h2 id=0-logistical-info>0. Logistical Info</h2><ul><li>Section date: 9/27</li><li>Associated lectures: 9/19, 9/21</li><li>Associated pset: Pset 3, due 9/29</li><li>Office hours on 9/27 from 7-9pm at Quincy Dining Hall</li><li>Remember to fill out the attendance form</li><li>Scroll to <a href=#4-summary>section 4</a> for a concise content summary.</li></ul><h3 id=01-summary--practice-problem-pdfs>0.1 Summary + Practice Problem PDFs</h3><p><a href=/uploads/stat110/Section%203%20Handout.pdf target=_blank>Summary + Practice Problems PDF</a></p><p><a href=/uploads/stat110/Section%203%20Solutions.pdf target=_blank>Practice Problem Solutions PDF</a></p><h2 id=1-examples-from-class>1. Examples from class</h2><p>The lecture on 9/19 was full of examples of conditional probability. Here are my takeaways for each. I did not rewrite the examples because solutions are quite well-written in the book and/or in lecture.</p><h3 id=11-winter-girl-examples-225-227-in-the-book>1.1 Winter girl (Examples 2.2.5-2.2.7 in the book)</h3><ul><li>Define your events as specifically as possible, there are a lot of details that surprisingly can change probabilities.</li><li>See if you can simplify your events (both mathematically and logically). For example, if $A$ is the event that there&rsquo;s at least one girl and $B$ is the event that there are two girls, $P(A, B) = P(B)$ since if there are two girls, there is automatically at least one girls.</li></ul><h3 id=12-monty-hall-example-271-and-many-practice-problems-in-the-book>1.2 Monty Hall (Example 2.7.1 and many practice problems in the book)</h3><p>You can use the Law of Total Probability in some extreme ways! Condition on things that make your life much, much easier - in Monty Hall problems (and the variants that Joe likes to write), I very often condition on the location of the car or use Bayes&rsquo; rule to move information about the car&rsquo;s location into the condition!</p><h3 id=13-simpsons-paradox-example-283-in-the-book>1.3 Simpson&rsquo;s paradox (Example 2.8.3 in the book)</h3><p>I think it&rsquo;s a good to develop the skill of coming up with similar paradoxes - at the very least, it can test your understanding of probability. My understanding of this phenomenon is that there are two tasks - a hard and an easy task. Doctor A might have a better success rate in each task, but Doctor B can still have a higher overall success rate.</p><p>This happens because Doctor A does more of the harder task, which drags their average down, while Doctor B inflates their average by doing the easier task more often. There are some other intuitive corollaries - like how some students may learn a lot but have lower GPAs then other students because they take a higher proportion of challenging classes.</p><p>To construct these paradoxes, I think you need a hard task (where both doctors have a &ldquo;low&rdquo; success rate) and an easy task (where both doctors have a higher success rate). Then doctor A has to do more of the hard task, while doctor B needs to do more of the easy task, to weight their averages differently.</p><h3 id=14-gamblers-ruin-example-273-in-the-book>1.4 Gambler&rsquo;s ruin (Example 2.7.3 in the book)</h3><p>You will be assessed (in pset and/or exam) on your ability to apply the gambler&rsquo;s ruin result in other contexts, but you will never have to re-derive it. So you can figure out how variables in your problem correspond to gambler&rsquo;s ruin (or even set up the difference equation) then just jump to plugging in the solution given.</p><p>In the gambler&rsquo;s ruin problem, gambler $A$ starts with $i$ dollars and gambler $B$ starts with $N-i$ dollars. They keep making 1 dollar bets (which gambler $A$ has a probability $p$ of winning) until someone runs out of money (either gambler $A$ has $0$ dollars or gambler $B$ has 0 dollars). We often define $q = 1-p$ for notational convenience.</p><p>If we define $p_i$ to be the probability that gambler $A$ wins if they start with $i$ dollars, then using first-step analysis we find that
$$
p_i = p_{i+1} p + p_{i-1}q.
$$
This gives a difference equation solution of
$$
p_i = \begin{cases} \frac{1 - (\frac{q}{p})^i}{1 - (\frac{q}{p})^N}& p \ne 1/2\\
\frac{i}{N}& p = 1/2 \end{cases}
$$</p><p>To match a problem to this, you should</p><ul><li>Make sure that &ldquo;bets&rdquo; are worth 1 dollar each.</li><li>Make sure that gambler $A$ loses if they hit 0 dollars, and wins if they hit some fixed amount of dollars ($N$)</li><li>Make sure there is a constant probability of winning each bet.</li></ul><h2 id=2-random-variables>2. Random variables</h2><h3 id=21-definition>2.1 Definition</h3><p>A <strong>random variable</strong> summarizes some experiment. So if you have a sample space $S$, for each possible outcome $\omega \in S$, your random variable takes on a certain (real number). Here are some examples of random variables:</p><ul><li>Say we&rsquo;re rolling a die, so $S$ is the set of possible rolls. Then we could have $X = 1$ if we roll a $1$, $X = 2$ if we roll a $2$, and so on.</li><li>We could define another random variable $Y$ to be the square of the roll. so $Y = 1$ if we roll a 1, $Y = 16$ if we roll a $4$, etc.</li><li>Random variables don&rsquo;t have to take on different values for every outcome. So we could have $Z = 2$ if we roll an even number and $Z = 1$ if we roll an odd number.</li><li>They also don&rsquo;t have to be discrete values - we could have a random variable $T$ represent the exact temperature in the room right now.</li></ul><h3 id=22-defining-discrete-random-variables>2.2 Defining discrete random variables</h3><p>A random variable is <strong>discrete</strong> if it has a finite or countably infinite number of values (something like $1, 2, 3, 4, \ldots$ is countably infinite. If instead your random variable can take on any value in an interval - like any real number, or any real number between 2 and 4, etc. - then it is uncountable).</p><p>We can uniquely describe a random variable by its <strong>probability mass function</strong>. This basically tells us the probability that the random variable takes on each possible value. For example, the probability mass function for the first dice-roll example, $X$ is
$$
P(X = x) = \begin{cases}
\frac{1}{6} & x \in \{1, 2, 3, 4, 5, 6\}\\
0 & \text{else}.
\end{cases}
$$
The little $x$ is a dummy variable - it&rsquo;s just a general way of going through every possible value. For example, we can now tell that there is a $1/6$ probability that $X = 4$, which corresponds to a $1/6$ probability that we roll a $4$, which makes sense.</p><p>Some important facts:</p><ul><li>&ldquo;$X = 1$&rdquo; is an event, so we can take its probability. We <em>cannot</em> take the probability of a random variable, so $P(X)$ is a category error.</li><li>When writing a PMF<ul><li>Every probability should be between $0$ and $1$ (inclusive), which holds for all probabilities.</li><li>The probabilities in the PMF should sum to $1$:
$$
\sum_{x \in \mathbb{R}} P(X = x) = 1.
$$
This comes from both axioms of probability, since the events for each possible value of $X$ partition the entire sample space.</li><li>I always have the &ldquo;$0$, else&rdquo; statement.</li></ul></li><li>The <strong>support</strong> of a random variable is the set of possible values it can take on. So the support for $X$ is ${1, 2, 3, 4, 5, 6}$, and the support for $Z$ is ${1, 2}$, and so on. You should always define the support, too.</li></ul><h3 id=23-a-more-mathy-definition-of-random-variables>2.3 A more mathy definition of random variables</h3><p>Random variables are functions. If notation makes more sense to you, maybe this will be useful.</p><p>Remember that in any random experiment, we have a sample space $S$, where each element of $S$ is a possible outcome. Exactly one of those outcomes will happen. You can think of a random variable $X$ as a function that maps outcomes to the real number line, so $X : S \to \mathbb{R}$. So if some outcome $\omega \in S$ happens, then the random variable gives us the real number $X(\omega)$. There&rsquo;s nothing random about the function $X$ - each outcome always goes to the same real number - but instead the randomness comes from which outcome actually ends up happening.</p><p>So when we think about PMFs and any probabilities with random variables, we&rsquo;re using a bit of shorthand. It&rsquo;s not immediately obvious that $X = x$ is an event, so let&rsquo;s translate (again, remember $x$ is just a fixed number. I could just as easily use $y$ or $a$ as a variable, or $0$ or $1.5$ or $-17$ if I want a specific value):
\begin{align}
P(X = x) = P(\{\omega \in S : X(\omega) = x\})
\end{align}
That set, $\{\omega \in S : X(\omega) = x\}$, is a subset of the sample space and thus is definitely an event. We can take a probability of that. Note that there can be multiple outcomes in this subset, which is the same as saying that $X$ is <em>not injective</em>.</p><p>So when we start talking about fancier random varibles - like $X^2$ - we can still dissect the probabilities
\begin{align}
P(X^2 = y) &= P(\{\omega \in S : X^2(\omega) = y\})\\
&= P(\{\omega \in S : X(\omega) = \sqrt{y} \text{ or } X(\omega) = -\sqrt{y}\})\\
&= P((X = \sqrt{y}) \cup (X = -\sqrt{y}))\\
&= P(X = \sqrt{y}) + P(X = -\sqrt{y}),
\end{align}
assuming $y$ is positive and splitting up the probability in the last step because the two events are disjoint (a random variable can&rsquo;t equal two different values at the same time).</p><p>The support can also now be redefined as the function - basically, the support is $\{x \in \mathbb{R} : \text{ there exists } \omega \in S \text{ such that } X(\omega) = x\}$.</p><h2 id=3-distributions>3. Distributions</h2><p>A distribution is a type of random variable. I think this makes the most sense through example. For discrete distributions (which correspond to discrete random variables), we usually motivate them with a story and maybe some counting. <strong>Stories are extremely important and should be internalized,</strong> not just the PMFs.</p><h3 id=31-bernoulli-distribution>3.1 Bernoulli Distribution</h3><p>You perform an experiment that consists of $1$ trial, where the possible outcomes are success or failure. There is a probability $p$ of success and probability $q = 1-p$ of failure. This is summarized by a random variable $X$, where $X = 1$ if the trial is a success and $X = 0$ if it is a failure. We then say $X$ is distributed Bernoulli with parameter $p$, or in notation, $X \sim \mathrm{Bern}(p)$.</p><p>The PMF is
$$
P(X = x) = \begin{cases}
p & x = 1\\
q & x = 0\\
0 & \text{else}
\end{cases}
$$</p><p>For a concrete example, a rigged coin toss has a probability $p = 0.7$ of landing heads (success) and probability $q = 0.3$ of landing tails (failure). If $X = 1$ when the coin lands heads and $X = 0$ when it lands tails, then $X \sim \mathrm{Bern}(0.7)$.</p><p>Note that you CANNOT set a random variable <em>equal</em> to a distribution. You have to use the $\sim$ symbol and say &ldquo;distributed as.&rdquo;</p><h3 id=32-binomial-distribution>3.2 Binomial Distribution</h3><p>You perform an experiment with $n$, independent Bernoulli trials, each of which is a success with the same probability $p$. Then the random variable $Y$, the number of successful trials, is distributed Binomial with $n$ trials and success probability $p$. In notation, $Y \sim \mathrm{Bin}(n, p)$.</p><p>We can find the PMF with some counting:
\begin{align}
P(Y = y) &= \begin{cases}
\binom{n}{y} p^y (1-p)^{n-y} & y \in \{0, 1, 2, \ldots, n\}\\
0 & \text{else}.
\end{cases}
\end{align}</p><p>NOTE: a Binomial random variable $Y$ can be represented as the sum of $n$ independent Bernoulli random variables, each with success probability $p$.</p><h2 id=4-summary>4. Summary</h2><h3 id=41-examples-from-class>4.1 Examples from class</h3><p>Some takeaways:</p><ul><li>Define events very specifically (winter girl)</li><li>See if you can simplify your problems with logic, not just relying on grinding through math (winter girl)</li><li>Use the Law of Total Probability to condition on anything/everything you wish you knew<ul><li>In Monty Hall, you can often condition on the location of the car!</li></ul></li><li>To mimic Simpson&rsquo;s paradox, set up some &ldquo;hard&rdquo; and &ldquo;easy&rdquo; tasks and make the better doctor do more of the hard tasks</li><li>Learn how to turn a problem into the gambler&rsquo;s ruin problem:<ul><li>Make losing happen at $0$, and winning happen at some fixed $N$</li><li>Make sure each bet/step is only one dollar in either direction</li><li>Make sure the probability of winning each individual bet is constant</li></ul></li></ul><h3 id=42-random-variables>4.2 Random variables</h3><p><strong>Random variables</strong> are a numerical (real number) summary of the outcome of your experiment. So for each possible outcome, the random variable takes on a certain value. Multiple outcomes can lead to the same value of the random variable. The <strong>support</strong> of a random variable is the set of possible values it can take on.</p><p>We define discrete random variables by their <strong>probability mass function</strong>. You should define the probability $P(X = x)$ for each $x$ in the random variable&rsquo;s support, and always write probability $0$ for any value of $x$ that is not in the support. The PMF should always give valid probabilities, and should sum to $1$.</p><h3 id=43-distributions>4.3 Distributions</h3><ul><li><strong>Bernoulli distribution</strong>: you conduct a single trial that succeeds with probability $p$. $X = 1$ if the trial succeeds and $X = 0$ if it fails. Then $X \sim \mathrm{Bern}(p)$ and has a PMF
$$
P(X = x) = \begin{cases}
p & x = 1\\
1 - p & x = 0\\
0 & \text{else}
\end{cases}
$$</li><li><strong>Binomial distribution</strong>: you conducts $n$ independent trials that each succeed with probability $p$. $Y$ is the total number of successes among the $n$ trials. Then $Y \sim \mathrm{Bin}(n, p)$ and the PMF is
$$
P(Y = y) = \begin{cases}
\binom{n}{y} p^y (1-p)^{n-y} & y \in \{0, 1, \ldots, n\}\\
0 & \text{else}
\end{cases}
$$</li></ul></article><div class=article-tags><a class="badge badge-light" href=/tag/expository/>expository</a>
<a class="badge badge-light" href=/tag/teaching/>teaching</a></div><div class=article-widget><div class=post-nav><div class=post-nav-item><div class=meta-nav>Previous</div><a href=/stat110/week4/ rel=next>Week 4: Discrete Distributions and Expectation</a></div><div class=post-nav-item><div class=meta-nav>Next</div><a href=/stat110/week2/ rel=prev>Week 2: Conditional Probability</a></div></div></div></div><div class=body-footer><p>Last updated on Sep 26, 2023</p></div><footer class=site-footer><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></main></div></div></div><div class=page-footer></div><script src=/js/vendor-bundle.min.f64289d8217e08e3afcd597d60836062.js></script>
<script src=https://cdn.jsdelivr.net/npm/anchor-js@5.0.0/anchor.min.js integrity="sha256-aQmOEF2ZD4NM/xt4hthzREIo/2PFkOX/g01WjxEV7Ys=" crossorigin=anonymous></script>
<script>anchors.add()</script><script id=page-data type=application/json>{"use_headroom":false}</script><script src=/en/js/wowchemy.min.387a7b38a3dfead6f96c96742d20f5af.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>
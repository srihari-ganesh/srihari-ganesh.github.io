<!doctype html><!-- This site was created with Wowchemy. https://www.wowchemy.com --><!-- Last Published: November 8, 2023 --><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&family=Merriweather&family=Roboto+Mono&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&family=Merriweather&family=Roboto+Mono&display=swap" media=print onload='this.media="all"'><script src=/js/mathjax-config.js></script>
<link rel=stylesheet href=/css/vendor-bundle.min.047268c6dd09ad74ba54a0ba71837064.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script>
<link rel=stylesheet href=/css/wowchemy.9e3515c7534a90d8339ca9703c3e3bce.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Srihari Ganesh"><meta name=description content="Lecture dates: 9/24, 9/26, 10/3; Section date: 10/4"><link rel=alternate hreflang=en-us href=https://srihari-ganesh.github.io/stat110/week4/><link rel=canonical href=https://srihari-ganesh.github.io/stat110/week4/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu238e72ff877ecedb95a45ec9a2f2431b_15216_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu238e72ff877ecedb95a45ec9a2f2431b_15216_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#707070"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@wowchemy"><meta property="twitter:creator" content="@wowchemy"><meta property="twitter:image" content="https://srihari-ganesh.github.io/media/icon_hu238e72ff877ecedb95a45ec9a2f2431b_15216_512x512_fill_lanczos_center_3.png"><meta property="og:site_name" content="Srihari Ganesh"><meta property="og:url" content="https://srihari-ganesh.github.io/stat110/week4/"><meta property="og:title" content="Week 4: Discrete Distributions and Expectation | Srihari Ganesh"><meta property="og:description" content="Lecture dates: 9/24, 9/26, 10/3; Section date: 10/4"><meta property="og:image" content="https://srihari-ganesh.github.io/media/icon_hu238e72ff877ecedb95a45ec9a2f2431b_15216_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2023-10-02T00:00:00+00:00"><meta property="article:modified_time" content="2023-10-02T00:00:00+00:00"><title>Week 4: Discrete Distributions and Expectation | Srihari Ganesh</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=3e5517c495e5b4df00d931d8848899a4><script src=/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js></script><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Srihari Ganesh</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Srihari Ganesh</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#projects><span>Projects</span></a></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Teaching</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/#teaching><span>Experience</span></a>
<a class=dropdown-item href=/stat110><span>Stat 110 Section Notes</span></a></div></li><li class=nav-item><a class=nav-link href=/#contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><div class="container-fluid docs"><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 docs-sidebar"><form class="docs-search d-flex align-items-center"><button class="btn docs-toggle d-md-none p-0 mr-md-3 w-100" type=button data-toggle=collapse data-target=#docs-nav aria-controls=docs-nav aria-expanded=false aria-label="Toggle section navigation"><div class=d-flex><span class="d-md-none pl-1 flex-grow-1 text-left overflow-hidden">Stat 110 Section Notes</span>
<span><i class="fas fa-chevron-down"></i></span></div></button></form><nav class="collapse docs-links" id=docs-nav><ul class="nav docs-sidenav"><li><a href=/stat110/>Stat 110 Section Notes</a></li><li><a href=/stat110/week8/>Week 8: Multinomial, Multivariate Normal</a></li><li><a href=/stat110/week7/>Week 7: Poisson Processes, Joint Distributions, and Covariance</a></li><li><a href=/stat110/week6/>Week 6: Universality of the Uniform, Normal, Expo, and Moments</a></li><li><a href=/stat110/week5/>Week 5: Continuous Distributions</a></li><li class=active><a href=/stat110/week4/>Week 4: Discrete Distributions and Expectation</a></li><li><a href=/stat110/week3/>Week 3: Conditional Probability Examples and Random Variables</a></li><li><a href=/stat110/week2/>Week 2: Conditional Probability</a></li><li><a href=/stat110/week1/>Week 1: Counting and Definitions of Probability</a></li></ul></nav></div><div class="d-none d-xl-block col-xl-2 docs-toc"><ul class="nav toc-top"><li><a href=# id=back_to_top class=docs-toc-title>Contents</a></li></ul><nav id=TableOfContents><ul><li><a href=#0-logistical-info>0. Logistical Info</a><ul><li><a href=#01-summary--practice-problem-pdfs>0.1 Summary + Practice Problem PDFs</a></li></ul></li><li><a href=#1-random-variables>1. Random variables</a></li><li><a href=#2-discrete-distributions>2. Discrete distributions</a><ul><li><a href=#21-bernoulli>2.1 Bernoulli</a></li><li><a href=#22-binomial>2.2 Binomial</a></li><li><a href=#23-hypergeometric>2.3 Hypergeometric</a></li><li><a href=#24-geometricfirst-success>2.4 Geometric/First Success</a></li><li><a href=#25-negative-binomial>2.5 Negative Binomial</a></li><li><a href=#26-poisson>2.6 Poisson</a></li></ul></li><li><a href=#3-expectation>3. Expectation</a><ul><li><a href=#31-indicator-random-variables>3.1 Indicator Random Variables</a></li><li><a href=#32-variance>3.2 Variance</a></li></ul></li><li><a href=#4-handy-math-facts>4. Handy math facts</a></li></ul></nav></div><main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role=main><div class=docs-article-container></div><div class=docs-article-container><h1>Week 4: Discrete Distributions and Expectation</h1><article class=article-style><h2 id=0-logistical-info>0. Logistical Info</h2><ul><li>Section date: 10/4</li><li>Associated lectures: 9/24, 9/26, 10/3</li><li>Associated pset: Pset 4, due 10/6</li><li>Midterm: 10/10</li><li>Office hours on 10/4 from 7-9pm at Quincy Dining Hall</li><li>Exam office hours on 10/7 and 10/9 from 8-10pm at Quincy Dining Hall</li><li>Remember to fill out the attendance form</li><li>Given the structure of my section, I&rsquo;m shifting away from a lot of explanation on this webpage. I may come back in the future and add more examples, but it doesn&rsquo;t make much sense since we&rsquo;re mainly doing practice problems in section. So this week, I have no concise summary section on the webpage because it&rsquo;s all pretty tight - check out the handout below if you want it.</li></ul><h3 id=01-summary--practice-problem-pdfs>0.1 Summary + Practice Problem PDFs</h3><p><a href=/uploads/stat110/Section%204%20Handout.pdf target=_blank>Summary + Practice Problems PDF</a></p><p><a href=/uploads/stat110/Section%204%20Solutions.pdf target=_blank>Practice Problem Solutions PDF</a></p><h2 id=1-random-variables>1. Random variables</h2><p>Let&rsquo;s use the precise mathematical definition from last time: <strong>random variables</strong> assign real numbers to possible outcomes of an experiment. In other words, they map the sample space to the real line. So for a random variable $X$, for every outcome in the sample space, $\omega \in S$, there is a corrsponding real number, $X(\omega) \in \mathbb{R}$.</p><p>Here&rsquo;s the terminology of a random variable that we&rsquo;ve talked about thus far, where we continue using $X$ as an example of a random variable.</p><ul><li>The <strong>support</strong>: what is the set of values that a random variable can take on? This is equivalent to the image/range of of $X$ on $S$, $X(S)$.</li><li>For a named distribution like a Binomial, we say $X$ is distributed Binomial using $X \sim \mathrm{Bin}(n, p)$, where we have to set possible values of the parameters $n$ and $p$ for our specific problem. You CANNOT set $X = \mathrm{Bin}(n, p)$: named distributions cannot equal random variables, they are just a blueprint for what the random variable looks like.</li><li>The <strong>probability mass function (PMF)</strong>: for any real number $x$ (or $t$ or $y$, it&rsquo;s just a filler variable), what is the probability that $X$ takes on this value? This is notated $P(X = x)$.<ul><li>You should address every possible value of $x$: $P(X = x) = 0$ if $x$ is not in the support of $X$, $\sum_{x \in \text{ support of } X } P(X = x) = 1$, and every probability should be valid (nonnegative, between $0$ and $1$ inclusive).</li></ul></li><li><em>NEW:</em> the <strong>cumulative density function (CDF)</strong>: for any real number $x$, what is the probability that $X$ takes on a value that is less than or equal to $x$? This is notated $P(X \le x)$.<ul><li>You should again address every possible value of $x$, both in and outside of the support.</li><li>Here, the requirements for a valid CDF are that $P(X \le x) = 0$ if $x$ is less than the smallest value in the support and $P(X \le x) = 1$ if $x$ is greater than the biggest value in the support. For an infinite support, we should have $P(X \le x) \to 0$ as $x \to -\infty$ and $P(X \le x) \to 1$ as $x \to \infty$.</li><li>Additionally, a CDF should be non-decreasing (i.e., either increasing or a flat line).</li></ul></li><li>We often abbreviate to say random variables are <strong>independent and identically distributed (i.i.d.)</strong>.</li></ul><p>Here&rsquo;s a general approach for defining the distribution of a random variable (r.v.). You can give the distribution using either the PMF, the CDF, or a named distribution with the parameters defined.</p><ol><li>Define the support of your r.v.</li><li>See if the random variable matches the story of any of the named distributions we have discussed. To see if an r.v. matches a distribution, some things to check are<ul><li>For which named distributions is the support of your r.v. possible?</li><li>Are there draws/samples/trials? If so, are they independent?</li><li>If there is sampling, is it done with or without replacement?</li></ul></li><li>If you can match a named distribution, what are the parameters? Are those parameters allowed for that named distribution?</li><li>If you can&rsquo;t match a named distribution, how can you calculate the PMF using the information you checked about sampling and your counting skills?</li></ol><h2 id=2-discrete-distributions>2. Discrete distributions</h2><p>You can find details like the support, PMF, CDF, expectation, and variance in the table of distributions on page 605 of the textbook or page 3 of the midterm handout. We&rsquo;ll focus on the stories and connections between distributions. For these discrete random variables (except for the Poisson), you should develop comfort with calculating their PMFs from scratch.</p><h3 id=21-bernoulli>2.1 Bernoulli</h3><p><strong>Story</strong>: We run a trial with probability $p$ of success. Let the random variable $X$ be $1$ if the trial succeeds or $0$ if the trial fails. Then $X \sim \mathrm{Bern}(p)$.</p><p><strong>Connections</strong>:</p><ul><li>For $X \sim \mathrm{Bern}(p)$, $1-X \sim \mathrm{Bern}(1-p)$.</li><li>For $X \sim \mathrm{Bern}(p)$, $X^2 = X$, so $X^2 \sim \mathrm{Bern}(p)$. If you&rsquo;re wondering why, check the support!</li></ul><h3 id=22-binomial>2.2 Binomial</h3><p><strong>Story</strong>: We run $n$ <strong>independent</strong> trials, each with an <strong>equal</strong> probability $p$ of success. Let $X$ be the number of successful trials. Then $X \sim \mathrm{Bin}(n,p)$.</p><p><strong>Connections</strong>:</p><ul><li>For $n$ <strong>independent and identically distributed</strong> Bernoulli random variables $X_1, \ldots, X_n \stackrel{i.i.d.}{\sim} Bern(p)$, $$\sum_{i=1}^n X_i \sim \mathrm{Bin}(n, p).$$<ul><li>This means $\mathrm{Bern}(p)$ is equivalent to $\mathrm{Bin}(1,p)$.</li></ul></li><li>For independent random variables $X \sim \mathrm{Bin}(n, p)$ and $Y \sim \mathrm{Bin}(m, p)$, $$X+Y \sim \mathrm{Bin}(n+m,p).$$</li></ul><h3 id=23-hypergeometric>2.3 Hypergeometric</h3><p><strong>Story</strong>:</p><ul><li><em>Capture/recapture elk</em>: There are $N$ elk in the forest. In the past, we captured and tagged $m$ of the elk. We now recapture $n$ of the elk, where every set of $n$ is equally likely and elk are sampled without replacement. Let $X$ be the number of tagged elk among our $n$ recaptured elk. Then $X \sim \mathrm{HGeom}(m, N-m, n)$.</li><li><em>White and black balls in an urn</em>: There are $w$ white balls and $b$ black balls in a urn. We draw $n$ balls from the urn without replacement, where each set of $n$ balls is equally likely to be drawn. Let $X$ be the number of white balls in our sample. Then $X \sim \mathrm{HGeom}(w, b, n)$.</li></ul><p><strong>Connections</strong>:</p><ul><li>Notice the comparison between the Binomial and the Hypergeometric: using the urn story, if we sampled <em>with</em> replacement our random variable would be distributed $\mathrm{Bin}(n, \frac{w}{w+b})$.</li></ul><h3 id=24-geometricfirst-success>2.4 Geometric/First Success</h3><p><strong>Story</strong>: Suppose we&rsquo;re running independent Bernoulli trials with probability $p$ of success. We stop running trials once one succeeds. Let $X$ be the number of failed trials before (and <em>not</em> including) the first successful trial. Then $X \sim \mathrm{Geom}(p)$.</p><p><strong>Connections</strong>:</p><ul><li>The First Success distribution is essentially the same as the Geometric, but we include the first successful trial as part of our count. So it always holds that for $X \sim \mathrm{Geom}(p)$, we have $X+1 \sim \mathrm{FS}(p)$.</li><li>Note that the Geometric/First Success distributions have infinite supports, while the Binomial has a fixed number of trials. This is a quick way to tell them apart.</li></ul><h3 id=25-negative-binomial>2.5 Negative Binomial</h3><p><strong>Story</strong>: Suppose we&rsquo;re running independent Bernoulli trials with probability $p$ of success. We stop running trials after the $r^{th}$ success. Let $X$ be the number of failed trials before the $r^{th}$ success (not including any of the successes in that count). Then $X \sim \mathrm{NBin}(r,p)$.</p><p><strong>Connections</strong>:</p><ul><li>For <strong>independent and identically distributed</strong> $X_1, X_2, \ldots, X_r \stackrel{i.i.d.}{\sim} \mathrm{Geom}(p)$, we get $\sum_{i=1}^r X_i \sim \mathrm{NBin}(r, p)$.<ul><li>This means $\mathrm{NBin}(1,p)$ is equivalent to $\mathrm{Geom}(p)$.</li></ul></li></ul><h3 id=26-poisson>2.6 Poisson</h3><p><strong>Story</strong>: There&rsquo;s no exact story to derive a Poisson. The only situation in which you&rsquo;ll have to come up with the Poisson on your own is in approximation, and that is quite rare.</p><p><strong>Approximate story</strong>: Say there are many rare events $A_1, A_2, \ldots, A_n$ (so $n$ large and $P(A_i) = &#171; 1$, which stands for much smaller than $1$) which are nearly independent (which doesn&rsquo;t have a rigorous definition). Then if we let $\lambda = \sum_{i=1}^n P(A_i)$, $X = \sum_{i=1}^n I(A_i)$ is approximately distributed $\mathrm{Pois}(\lambda)$.</p><p><strong>Connections</strong>:</p><ul><li>As you can see in the approximate story, you can use the Poisson to count the number of independent/weakly-dependent rare events that occur.</li><li>Suppose $X \sim \mathrm{Pois}(\lambda)$ and $Y \sim \mathrm{Pois}(\mu)$ with $X, Y$ independent. Then $X + Y \sim \mathrm{Pois}(\lambda + \mu)$.</li><li>Chicken-Egg: suppose a chicken lays $N$ eggs, with $N \sim \mathrm{Pois}(\lambda)$. Suppose each egg has a probability $p$ of hatching, with each egg&rsquo;s hatching being independent, and let $X$ be the number of eggs that hatch and $Y$ be the number of eggs that don&rsquo;t hatch.<ul><li>$X$ and $Y$ are independent. $X$ and $Y$ are very conditionally independent given $N$ since $N = X+Y$.</li><li>$X \sim \mathrm{Pois}(\lambda p)$, $Y \sim \mathrm{Pois}(\lambda (1-p))$.</li><li>$X | N = n \sim \mathrm{Bin}(n, p)$.</li></ul></li></ul><h2 id=3-expectation>3. Expectation</h2><div class="alert alert-note"><div>The <strong>expectation</strong> of a random variable $X$ with support $A$ is the weighted average of its possible values, where we weight based on the probability of $X$ taking on each value in its support. It is formally defined as
$$
E(X) = \sum_{x \in A} x P(X = x)
$$</div></div><p><strong>Linearity</strong> states that for any random variables $X, Y$ (which can be dependent!) and real number $c$,
\begin{align*}
E(X + Y) &= E(X) + E(Y),\\
E(cX) &= cE(X).
\end{align*}
The <strong>law of the unconscious statistician (LOTUS)</strong> states that the expectation of any function of a random variable, $g(X)$, can be found by
$$
E(g(X)) = \sum_{x \in A} g(x) P(X = x).
$$
For example, if we want to find $E(X^2)$, we simply swap $x^2$ in for $x$ in the expectation formula to get $E(X^2) = \sum_{x \in A} x^2 P(X = x)$. Note that the probabilities here don&rsquo;t change, only what goes in front.</p><h3 id=31-indicator-random-variables>3.1 Indicator Random Variables</h3><p>An <strong>indicator random variable</strong> converts an event into a Bernoulli random variable. For an event $A$ with $P(A) = p$, the corresponding indicator random variable $I(A) \sim \mathrm{Bern}(p)$. This random variable is defined such that $I(A) = 1$ if $A$ occurs and $I(A) = 0$ if $A^c$ occurs. You might see other equivalent notation like $I_A$ or $I$, just be clear about which event your indicator random variable corresponds to.</p><p>The <strong>fundamental bridge</strong> (vocab which is not used outside of Stat 110) gives that
$$
E(I(A)) = P(A).
$$
We use this result a lot to calculate expectations of random variables that can be expressed as the sum of indicators. This is nice because the indicators can be dependent, but linearity allows us to break the expectations apart! A very common workflow to calculate an expectation is to write</p><ol><li>Write the random variable as the sum of indicators, $X = \sum_i I(A_i)$, where each $A_i$ is an event.</li><li>Apply linearity, $E(X) = \sum_i E(I(A_i))$.</li><li>Use the fundamental bridge, $E(X) = \sum_i P(A_i)$.</li></ol><!-- ### 3.2 St. Petersburg --><h3 id=32-variance>3.2 Variance</h3><div class="alert alert-note"><div>The <strong>variance</strong> is a measure of spread, defined for a random variable $X$ as
$$
Var(X) = E((X - E(X))^2).
$$</div></div><p>Here basically all of the facts you have to know about variance:</p><ul><li>It&rsquo;s usually calculated using an equivalent formula, $$Var(X) = E(X^2) - (E(X))^2.$$</li><li>It is always nonnegative. In fact, variance is only zero if $P(X = x) = 1$ for some $x \in \mathbb R$: in other words, $X$ takes on a certain value with probability $1$. If this is not the case, the variance will be positive.</li><li>For a scalar $c \in \mathbb R$ (a number, not random) and a random variable $X$,
\begin{align*}
Var(cX) &= c^2 Var(X)\\
Var(X + c) &= Var(X).
\end{align*}</li><li>For <em>independent</em> random variables $X$ and $Y$
$$
Var(X + Y) = Var(X) + Var(Y).
$$
For <em>dependent</em> random variables $X$ and $Y$,
$$
Var(X+Y) \ne Var(X) + Var(Y).
$$</li></ul><h2 id=4-handy-math-facts>4. Handy math facts</h2><ul><li>You are expected to know how to find the sum of an infinite geometric series: if $|x| &lt; 1$,
$$
\sum_{n=0}^\infty x^n = \frac{1}{1-x}.
$$
otherwise the sum does not exist (it diverges). For finite geometric series (and any $x \ne 1$),
$$
\sum_{n=0}^\infty x^n = \frac{1-x^n}{1-x}.
$$</li><li>You are also expected to be familiar with some $e^x$ approximations, but you usually won&rsquo;t be asked to approximate without prompting. The Taylor series of $e^x$ is
$$
e^x = \sum_{n=0}^\infty \frac{x^n}{n!}.
$$
The compound interest formula also gives
$$
e^x = \lim_{n \to \infty} (1+\frac{x}{n})^n.
$$</li></ul></article><div class=article-tags><a class="badge badge-light" href=/tag/expository/>expository</a>
<a class="badge badge-light" href=/tag/teaching/>teaching</a></div><div class=article-widget><div class=post-nav><div class=post-nav-item><div class=meta-nav>Previous</div><a href=/stat110/week5/ rel=next>Week 5: Continuous Distributions</a></div><div class=post-nav-item><div class=meta-nav>Next</div><a href=/stat110/week3/ rel=prev>Week 3: Conditional Probability Examples and Random Variables</a></div></div></div></div><div class=body-footer><p>Last updated on Oct 2, 2023</p></div><footer class=site-footer><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> â€” the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></main></div></div></div><div class=page-footer></div><script src=/js/vendor-bundle.min.f64289d8217e08e3afcd597d60836062.js></script>
<script src=https://cdn.jsdelivr.net/npm/anchor-js@5.0.0/anchor.min.js integrity="sha256-aQmOEF2ZD4NM/xt4hthzREIo/2PFkOX/g01WjxEV7Ys=" crossorigin=anonymous></script>
<script>anchors.add()</script><script id=page-data type=application/json>{"use_headroom":false}</script><script src=/en/js/wowchemy.min.387a7b38a3dfead6f96c96742d20f5af.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>
<!doctype html><!-- This site was created with Wowchemy. https://www.wowchemy.com --><!-- Last Published: December 26, 2023 --><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&family=Merriweather&family=Roboto+Mono&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&family=Merriweather&family=Roboto+Mono&display=swap" media=print onload='this.media="all"'><script src=/js/mathjax-config.js></script>
<link rel=stylesheet href=/css/vendor-bundle.min.047268c6dd09ad74ba54a0ba71837064.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script>
<link rel=stylesheet href=/css/wowchemy.9e3515c7534a90d8339ca9703c3e3bce.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Srihari Ganesh"><meta name=description content="Lecture dates: 10/24, 10/26; Section date: 11/1"><link rel=alternate hreflang=en-us href=https://srihari-ganesh.github.io/stat110/week7/><link rel=canonical href=https://srihari-ganesh.github.io/stat110/week7/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu238e72ff877ecedb95a45ec9a2f2431b_15216_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu238e72ff877ecedb95a45ec9a2f2431b_15216_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#707070"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@wowchemy"><meta property="twitter:creator" content="@wowchemy"><meta property="twitter:image" content="https://srihari-ganesh.github.io/media/icon_hu238e72ff877ecedb95a45ec9a2f2431b_15216_512x512_fill_lanczos_center_3.png"><meta property="og:site_name" content="Srihari Ganesh"><meta property="og:url" content="https://srihari-ganesh.github.io/stat110/week7/"><meta property="og:title" content="Week 7: Poisson Processes, Joint Distributions, and Covariance | Srihari Ganesh"><meta property="og:description" content="Lecture dates: 10/24, 10/26; Section date: 11/1"><meta property="og:image" content="https://srihari-ganesh.github.io/media/icon_hu238e72ff877ecedb95a45ec9a2f2431b_15216_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2023-11-06T00:00:00+00:00"><meta property="article:modified_time" content="2023-11-06T00:00:00+00:00"><title>Week 7: Poisson Processes, Joint Distributions, and Covariance | Srihari Ganesh</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=cac427f62539a9f51e3d007b4e742c23><script src=/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js></script><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Srihari Ganesh</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Srihari Ganesh</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#projects><span>Projects</span></a></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Teaching</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/#teaching><span>Experience</span></a>
<a class=dropdown-item href=/stat110><span>Stat 110 Section Notes</span></a></div></li><li class=nav-item><a class=nav-link href=/#contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><div class="container-fluid docs"><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 docs-sidebar"><form class="docs-search d-flex align-items-center"><button class="btn docs-toggle d-md-none p-0 mr-md-3 w-100" type=button data-toggle=collapse data-target=#docs-nav aria-controls=docs-nav aria-expanded=false aria-label="Toggle section navigation"><div class=d-flex><span class="d-md-none pl-1 flex-grow-1 text-left overflow-hidden">Stat 110 Section Notes</span>
<span><i class="fas fa-chevron-down"></i></span></div></button></form><nav class="collapse docs-links" id=docs-nav><ul class="nav docs-sidenav"><li><a href=/stat110/>Stat 110 Section Notes</a></li><li><a href=/stat110/week11/>Week 11: Markov Chains</a></li><li><a href=/stat110/week10/>Week 10: Adam's & Eve's Laws, Inequalities, and Limit Theorems</a></li><li><a href=/stat110/week9/>Week 9: Transformations, Gamma/Beta, Conditional Expectation</a></li><li><a href=/stat110/week8/>Week 8: Multinomial, Multivariate Normal</a></li><li class=active><a href=/stat110/week7/>Week 7: Poisson Processes, Joint Distributions, and Covariance</a></li><li><a href=/stat110/week6/>Week 6: Universality of the Uniform, Normal, Expo, and Moments</a></li><li><a href=/stat110/week5/>Week 5: Continuous Distributions</a></li><li><a href=/stat110/week4/>Week 4: Discrete Distributions and Expectation</a></li><li><a href=/stat110/week3/>Week 3: Conditional Probability Examples and Random Variables</a></li><li><a href=/stat110/week2/>Week 2: Conditional Probability</a></li><li><a href=/stat110/week1/>Week 1: Counting and Definitions of Probability</a></li></ul></nav></div><div class="d-none d-xl-block col-xl-2 docs-toc"><ul class="nav toc-top"><li><a href=# id=back_to_top class=docs-toc-title>Contents</a></li></ul><nav id=TableOfContents><ul><li><a href=#0-logistical-info>0. Logistical Info</a><ul><li><a href=#01-summary--practice-problem-pdfs>0.1 Summary + Practice Problem PDFs</a></li></ul></li><li><a href=#1-moment-generating-functions-mgfs>1. Moment Generating Functions (MGFs)</a></li><li><a href=#2-poisson-processes>2. Poisson Processes</a></li><li><a href=#3-marginal-conditional-and-joint-distributions>3. Marginal, Conditional, and Joint Distributions</a></li><li><a href=#4-covariance-and-correlation>4. Covariance and Correlation</a></li></ul></nav></div><main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role=main><div class=docs-article-container></div><div class=docs-article-container><h1>Week 7: Poisson Processes, Joint Distributions, and Covariance</h1><article class=article-style><h2 id=0-logistical-info>0. Logistical Info</h2><ul><li>Section date: 11/1</li><li>Associated lectures: 10/24, 10/26</li><li>Associated pset: Pset 7, due 11/3</li><li>Office hours on 11/1 from 7-9pm at Quincy Dining Hall</li><li>Remember to fill out the attendance form</li></ul><h3 id=01-summary--practice-problem-pdfs>0.1 Summary + Practice Problem PDFs</h3><p><a href=/uploads/stat110/Section%207%20Handout.pdf target=_blank>Summary + Practice Problems PDF</a></p><p><a href=/uploads/stat110/Section%207%20Solutions.pdf target=_blank>Practice Problem Solutions PDF</a></p><h2 id=1-moment-generating-functions-mgfs>1. Moment Generating Functions (MGFs)</h2><p>For a random variable $X$, the $\mathbf{n^{th}}$ <strong>moment</strong> is $E(X^n)$.</p><div class="alert alert-note"><div>For a random variable $X$, the <strong>moment generating function (MGF)</strong> is $$M_X(t) = E(e^{tX})$$ for $t \in \mathbb{R}$. If the MGF exists, then
\begin{align*}
M_X(0) &= 1,\\
\frac{d^n}{dt^n} M_X(t) |_{t=0} = M_X^{(n)}(t) &= E(X^n).
\end{align*}
You should sanity-check that $M_X(0) = 1$ whenever you calculate an MGF.</div></div><p>Useful MGF results:</p><ul><li>For independent random variables, $X, Y$ with MGFs $M_X, M_Y$, then $M_{X+Y}(t) = M_X(t) M_Y(t)$.</li><li>For random variable $X$ and scalars $a, b$,
\begin{align*}
M_{a+bX}(t) = e^{at} M_X(bt)
\end{align*} since $M_{a+bX}(t) = E(e^{t(a+bX)}) = e^{at} E(e^{btX})$.</li></ul><div class="alert alert-note"><div><p>A distribution is uniquely determined by any of the following:</p><!-- \begin{enumerate} --><ul><li>PMF (common for discrete),</li><li>PDF,</li><li>CDF (common for continuous),</li><li>MGF, or</li><li>matching to a named distribution (common).</li></ul><!-- \end{enumerate} --></div></div><h2 id=2-poisson-processes>2. Poisson Processes</h2><div class="alert alert-note"><div><p>Consider a problem similar to Blissville/Blotchville, where $T_1, T_2, \ldots,$ represent the arrival times of busses (the amount of time from when we started waiting to when each bus arrives). Then the bus arrival process is a <strong>Poisson process</strong> with rate $\lambda$ if it satisfies the following conditions:</p><ol><li>For any interval in time of length $t > 0$, the number of arrivals in that interval is distributed $\mathrm{Pois}(\lambda t)$.</li><li>For any non-overlapping (disjoint) intervals of time, the number of bus arrivals are independent.
This applies for any &ldquo;arrival process&rdquo; where $T_1, T_2, \ldots$ correspond to arrival times.</li></ol></div></div><div class="alert alert-warning"><div>Pay attention to units: $\lambda$ is a rate. So if $\lambda$ has units of arrivals per hour, then $t$ should have units of hours.</div></div><p>Results:</p><ul><li><strong>Inter-arrival times</strong>: In a Poisson process with rate $\lambda$, the inter-arrival times (the time for the first arrival, $T_1$, and the times between consecutive arrives $T_2-T_1, T_3-T_2, \ldots$) are each independently distributed
\begin{align*}
T_1, T_2-T_1, T_3-T_2, \ldots \stackrel{i.i.d.}{\sim} \mathrm{Expo}(\lambda).
\end{align*}</li></ul><div class="alert alert-warning"><div>Additionally note that $T_2, T_3, \ldots,$ are <em>not</em> exponentially distributed. In fact, they follow Gamma distributions (which we will introduce soon): $T_n \sim \mathrm{Gamma}(n, \lambda)$.</div></div><ul><li><strong>Count-time duality</strong>: Fix a time $t > 0$. Let $N_t$ be the number of arrivals in the time interval $[0, t]$, and let $T_n$ be the arrival time of the $n$-th arrival. Then
\begin{align*}
(T_n > t) = (N_t &lt; n).
\end{align*}</li></ul><h2 id=3-marginal-conditional-and-joint-distributions>3. Marginal, Conditional, and Joint Distributions</h2><div class="alert alert-note"><div><p><em>Marginal, conditional, and joint distributions</em></p><p>Consider two random variables $X, Y$.</p><table><thead><tr><th></th><th>Joint</th><th>Marginal</th><th>Conditional</th></tr></thead><tbody><tr><td>Distribution</td><td>$(X, Y)$</td><td>$X$</td><td>$X\vert Y=y$</td></tr><tr><td>PMF</td><td>$P(X = x, Y = y)$</td><td>$P(X = x)$</td><td>$P(X = x\vert Y = y)$</td></tr><tr><td>CDF</td><td>$P(X \le x, Y \le y)$</td><td>$P(X \le x)$</td><td>$P(X \le x \vert Y = y)$</td></tr></tbody></table><p>For example, $P(X \vert Y = y)$ is a marginal PMF. All of these apply if we flip $X$ and $Y$, and PDFs follow analogously from PMFs.</p></div></div><ul><li><strong>Marginalization</strong>:
If we know the joint distribution of random variables $(X, Y)$, then we can find the marginal distribution of $X$ (and analogously, $Y$) by LOTP:
\begin{align*}
P(X = x) &= \sum_y P(X = x, Y = y), & \text{$X,Y$ discrete}.\\
f_X(x) &= \int_{-\infty}^\infty f_{X,Y}(x, y), & \text{$X,Y$ continuous}.
\end{align*}</li></ul><div class="alert alert-warning"><div>Note that marginal distributions of $X$ and $Y$ are not sufficient (not enough information) to find the joint distribution of $X, Y$.</div></div><ul><li><em>Joint from marginal and conditional</em>:
If we know the marginal distribution of $X$ and the conditional distributions $Y | X=x$ for any $x$, then we can find the joint distribution of $(X, Y)$ by factoring out our probability:
\begin{align*}
P(X = x, Y = y) &= P(X = x) P(Y = y | X = x), & \text{$X, Y$ discrete.}\\
f_{X, Y} (x, y) &= f_{X}(x) f_{Y|X=x} (y), & \text{ $X, Y$ continuous.}
\end{align*}</li></ul><div class="alert alert-note"><div><strong>Independence of random variables</strong>:
Random variables $X, Y$ are <strong>independent</strong> if for all $x$ and $y$, any of the following hold (they imply each other, if valid):
\begin{align*}
F_{X, Y} (x, y) = P(X \le x, Y \le Y) &= P(X \le x) P(Y \le Y) = F_X(x)F_Y(y), & \text{ CDFs for any $X, Y$.}\\
P(X = x, Y = y) &= P(X = x) P(Y = y), & \text{PMFs for discrete $X, Y$.}\\
f_{X, Y} (x, y) &= f_X(x) f_Y(y), &\text{PDFs for continuous, $X, Y$.}
\end{align*}</div></div><ul><li><em>2D LOTUS</em>:
Let $X, Y$ be random variables with known joint distribution. For $g: \mathrm{support}(X) \times \mathrm{support}(Y) \to \mathbb R$, LOTUS extends to 2 dimensions (or analogously for any larger dimensions) to give
\begin{align*}
E(g(X, Y)) &= \begin{cases}
\sum_x \sum_y g(x, y) P(X = x, Y = y), & \text{ $X, Y$ discrete}\\
\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x, y) f_{X, Y}(x, y)dx dy, & \text{ $X, Y$ continuous}.
\end{cases}
\end{align*}</li></ul><h2 id=4-covariance-and-correlation>4. Covariance and Correlation</h2><div class="alert alert-note"><div><p><strong>Covariance and Correlation</strong></p><p>$\newcommand{\cov}{\mathrm{Cov}}\newcommand{\corr}{\mathrm{Corr}}\newcommand{\var}{\mathrm{Var}}\newcommand{\sd}{\mathrm{SD}}$
The <strong>covariance</strong> of random variables $X, Y$ is
\begin{align*}
\cov(X, Y) &= E\left( \left[X - EX \right] \left[Y - EY\right]\right)
\end{align*}
where $EX$ is shorthand for $E(X)$. Equivalently,
\begin{align*}
\cov(X, Y) &= E(XY) - E(X) E(Y).
\end{align*}</p><p>The <strong>correlation</strong> of random variables $X, Y$ is
\begin{align*}
\corr(X, Y) &= \frac{\cov(X, Y)}{\sqrt{\var(X) \var(Y)}}\
&= \frac{\cov(X, Y)}{\sd(X)\sd(Y)},
\end{align*}
where $\sd(X) = \sqrt{\var(X)}$ is the standard deviation of $X$. Equivalently, we first standardize $X$ and $Y$, then find their covariance:
\begin{align*}
\corr(X, Y) &= \cov\left( \frac{X - E(X)}{\sd(X)}, \frac{Y - E(Y)}{\sd (Y)} \right).
\end{align*}</p></div></div><p>$X$ and $Y$ are</p><ul><li><strong>positively correlated</strong> if $\corr(X, Y) > 0$,</li><li><strong>negatively correlated</strong> if $\corr(X, Y) &lt; 0$,</li><li><strong>uncorrelated</strong> if $\corr(X, Y) = 0$.</li></ul><p>Since correlation and covariance have the same sign, this also applies for positive/negative/zero covariance.</p><p><strong>Properties of covariance</strong>: see page 327 in Blitzstein & Huang for full list. Let $X, Y, W, Z$ be random variables, as well as those of the form $X_1, X_2, \ldots,$.</p><ul><li>If $X, Y$ are independent, then $\cov(X, Y) = 0$ (so $X, Y$ are uncorrelated).</li><li>$\cov(X, X) = \var(X)$.</li><li>$\var(\sum_i X_i) = \sum_i \var(X_i) + \sum_{i&lt;j} 2 \cov(X_i, X_j)$.<ul><li>This can be especially useful for finding the variance of a sum of indicators.</li></ul></li><li>\begin{align*}\cov(X+Y, W+Z) &= \cov(X, W) + \cov(X, Z)\\ &+ \cov(Y, W) + \cov(Y, Z).\end{align*}</li><li>$\cov(aX, bY) = ab \cov(X, Y)$.</li></ul><p>The last two properties are referred to as <strong>bilinearity</strong>.</p><p><strong>Properties of correlation</strong>
Let $X, Y$ be random variables.</p><ul><li>-If $X, Y$ are independent, then $\corr(X, Y) = 0$ (so $X, Y$ are uncorrelated)</li><li>$-1 \le \corr(X, Y) \le 1$.</li></ul><div class="alert alert-warning"><div><strong>Uncorrelated does NOT imply independent</strong>: In the previous two results, we noted independent random variables have zero correlation and zero covariance. However, the converse does not apply: uncorrelated random variables are not necessarily independent.</div></div></article><div class=article-tags><a class="badge badge-light" href=/tag/expository/>expository</a>
<a class="badge badge-light" href=/tag/teaching/>teaching</a></div><div class=article-widget><div class=post-nav><div class=post-nav-item><div class=meta-nav>Previous</div><a href=/stat110/week8/ rel=next>Week 8: Multinomial, Multivariate Normal</a></div><div class=post-nav-item><div class=meta-nav>Next</div><a href=/stat110/week6/ rel=prev>Week 6: Universality of the Uniform, Normal, Expo, and Moments</a></div></div></div></div><div class=body-footer><p>Last updated on Nov 6, 2023</p></div><footer class=site-footer><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> â€” the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></main></div></div></div><div class=page-footer></div><script src=/js/vendor-bundle.min.f64289d8217e08e3afcd597d60836062.js></script>
<script src=https://cdn.jsdelivr.net/npm/anchor-js@5.0.0/anchor.min.js integrity="sha256-aQmOEF2ZD4NM/xt4hthzREIo/2PFkOX/g01WjxEV7Ys=" crossorigin=anonymous></script>
<script>anchors.add()</script><script id=page-data type=application/json>{"use_headroom":false}</script><script src=/en/js/wowchemy.min.387a7b38a3dfead6f96c96742d20f5af.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>
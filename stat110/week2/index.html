<!doctype html><!-- This site was created with Wowchemy. https://www.wowchemy.com --><!-- Last Published: December 6, 2023 --><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&family=Merriweather&family=Roboto+Mono&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&family=Merriweather&family=Roboto+Mono&display=swap" media=print onload='this.media="all"'><script src=/js/mathjax-config.js></script>
<link rel=stylesheet href=/css/vendor-bundle.min.047268c6dd09ad74ba54a0ba71837064.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script>
<link rel=stylesheet href=/css/wowchemy.9e3515c7534a90d8339ca9703c3e3bce.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Srihari Ganesh"><meta name=description content="Lecture dates: 9/12, 9/14; Section date: 9/20"><link rel=alternate hreflang=en-us href=https://srihari-ganesh.github.io/stat110/week2/><link rel=canonical href=https://srihari-ganesh.github.io/stat110/week2/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu238e72ff877ecedb95a45ec9a2f2431b_15216_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu238e72ff877ecedb95a45ec9a2f2431b_15216_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#707070"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@wowchemy"><meta property="twitter:creator" content="@wowchemy"><meta property="twitter:image" content="https://srihari-ganesh.github.io/media/icon_hu238e72ff877ecedb95a45ec9a2f2431b_15216_512x512_fill_lanczos_center_3.png"><meta property="og:site_name" content="Srihari Ganesh"><meta property="og:url" content="https://srihari-ganesh.github.io/stat110/week2/"><meta property="og:title" content="Week 2: Conditional Probability | Srihari Ganesh"><meta property="og:description" content="Lecture dates: 9/12, 9/14; Section date: 9/20"><meta property="og:image" content="https://srihari-ganesh.github.io/media/icon_hu238e72ff877ecedb95a45ec9a2f2431b_15216_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2023-09-16T00:00:00+00:00"><meta property="article:modified_time" content="2023-09-16T00:00:00+00:00"><title>Week 2: Conditional Probability | Srihari Ganesh</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=10c4997f7a77acf0752ac8c5370457f1><script src=/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js></script><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Srihari Ganesh</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Srihari Ganesh</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#projects><span>Projects</span></a></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Teaching</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/#teaching><span>Experience</span></a>
<a class=dropdown-item href=/stat110><span>Stat 110 Section Notes</span></a></div></li><li class=nav-item><a class=nav-link href=/#contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><div class="container-fluid docs"><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 docs-sidebar"><form class="docs-search d-flex align-items-center"><button class="btn docs-toggle d-md-none p-0 mr-md-3 w-100" type=button data-toggle=collapse data-target=#docs-nav aria-controls=docs-nav aria-expanded=false aria-label="Toggle section navigation"><div class=d-flex><span class="d-md-none pl-1 flex-grow-1 text-left overflow-hidden">Stat 110 Section Notes</span>
<span><i class="fas fa-chevron-down"></i></span></div></button></form><nav class="collapse docs-links" id=docs-nav><ul class="nav docs-sidenav"><li><a href=/stat110/>Stat 110 Section Notes</a></li><li><a href=/stat110/week11-12/>Week 11/12: Adam's & Eve's Laws, Inequalities, and Limit Theorems</a></li><li><a href=/stat110/week10/>Week 10: Adam's & Eve's Laws, Inequalities, and Limit Theorems</a></li><li><a href=/stat110/week9/>Week 9: Transformations, Gamma/Beta, Conditional Expectation</a></li><li><a href=/stat110/week8/>Week 8: Multinomial, Multivariate Normal</a></li><li><a href=/stat110/week7/>Week 7: Poisson Processes, Joint Distributions, and Covariance</a></li><li><a href=/stat110/week6/>Week 6: Universality of the Uniform, Normal, Expo, and Moments</a></li><li><a href=/stat110/week5/>Week 5: Continuous Distributions</a></li><li><a href=/stat110/week4/>Week 4: Discrete Distributions and Expectation</a></li><li><a href=/stat110/week3/>Week 3: Conditional Probability Examples and Random Variables</a></li><li class=active><a href=/stat110/week2/>Week 2: Conditional Probability</a></li><li><a href=/stat110/week1/>Week 1: Counting and Definitions of Probability</a></li></ul></nav></div><div class="d-none d-xl-block col-xl-2 docs-toc"><ul class="nav toc-top"><li><a href=# id=back_to_top class=docs-toc-title>Contents</a></li></ul><nav id=TableOfContents><ul><li><a href=#0-logistical-info>0. Logistical Info</a><ul><li><a href=#01-summary--practice-problem-pdfs>0.1 Summary + Practice Problem PDFs</a></li></ul></li><li><a href=#1-brushing-up-on-the-definition-of-probability>1. Brushing up on the definition of probability</a></li><li><a href=#2-conditional-probability>2. Conditional Probability</a></li><li><a href=#3-tools-using-conditional-probability>3. Tools using Conditional Probability</a><ul><li><a href=#31-probability-of-an-intersection>3.1 Probability of an Intersection</a></li><li><a href=#32-law-of-total-probability-lotp>3.2 Law of Total Probability (LOTP)</a></li><li><a href=#33-bayes-rule>3.3 Bayes&rsquo; Rule</a></li></ul></li><li><a href=#4-independence>4. Independence</a><ul><li><a href=#41-conditional-independence>4.1 Conditional Independence</a></li></ul></li><li><a href=#5-summary>5. Summary</a><ul><li><a href=#51-definition-of-probability>5.1 Definition of Probability</a></li><li><a href=#52-conditional-probability>5.2 Conditional Probability</a></li><li><a href=#53-conditional-probability-tools>5.3 Conditional Probability Tools</a></li><li><a href=#54-independence>5.4 Independence</a></li></ul></li></ul></nav></div><main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role=main><div class=docs-article-container></div><div class=docs-article-container><h1>Week 2: Conditional Probability</h1><article class=article-style><h2 id=0-logistical-info>0. Logistical Info</h2><ul><li>Section date: 9/20</li><li>Associated lectures: 9/12, 9/14</li><li>Associated pset: Pset 2, due 9/22</li><li>Office hours on 9/20 from 7-9pm at Quincy Dining Hall</li><li>Remember to fill out the attendance form</li><li>Scroll to <a href=#5-summary>section 5</a> for a concise content summary.</li></ul><h3 id=01-summary--practice-problem-pdfs>0.1 Summary + Practice Problem PDFs</h3><p><a href=/uploads/stat110/Section%202%20Handout.pdf target=_blank>Summary + Practice Problems PDF</a></p><p><a href=/uploads/stat110/Section%202%20Solutions.pdf target=_blank>Practice Problem Solutions PDF</a></p><h2 id=1-brushing-up-on-the-definition-of-probability>1. Brushing up on the definition of probability</h2><p>We&rsquo;ll restate the axioms for the general definition of probability:</p><div class="alert alert-note"><div><p><strong>Definition of probability</strong>:<br>There are just two axioms (rules that probabilities have to follow):</p><ol><li>$P(S) = 1, P(\emptyset) = 0.$</li><li>If events $A_1, A_2, \ldots$ are disjoint, then $$
P\left( \bigcup_{j=1}^\infty A_j\right) = \sum_{j=1}^\infty P(A_j).
$$</li></ol><p>In other words, if $A_1, A_2, \ldots$ partition some event $B$, then $P(B) = \sum_{j=1}^\infty P(A_j)$.</p></div></div><!-- axioms are what mathematicians use to encode intuitive definitions --><div class="alert alert-warning"><div><p><strong>Tips for calculating probabilities</strong>:</p><ol><li>Define events for every aspect of the problem (e.g., &ldquo;$A$ = the event that it rains tomorrow, $B$ = the event that it rained today&rdquo;)</li><li>Write out the probabilities that you are given in the problem using notation (e.g., &ldquo;$P(A|B) = 1/2$, $P(B) = 1/4$).</li><li>Write the probability that you want to calculate using notation (e.g., we want to calculate the unconditional probability that it rains tomorrow, $P(A)$).</li><li>Figure out how the tools we have learned allow you to utilize the probabiliies that you do know (step 2) to calculate the probabilities that you don&rsquo;t know (step 3).</li></ol></div></div><p>There are some important results that follow:</p><ul><li><strong>Probability of a complement</strong>: If $A$ is an event a sample space $S$,
$$
P(A) = 1 - P(A^c).
$$
Concisely, the probability of an event occuring is $1$ minus the probability of the event not occuring.</li><li><strong>Probability of a union</strong>: For events $A$, $B$, we have
$$
P(A \cup B) = P(A) + P(B) - P(A \cap B).
$$
It&rsquo;s also useful to &ldquo;disjointify&rdquo; $A \cup B$ into a partition ($A \cup B^c, A \cap B, A^c \cup B$) which allows us to use the second axiom and get
$$
P(A \cup B) = P(A \cup B^c) + P(A \cap B) + P(A^c \cup B).
$$</li><li><strong>Principle of Inclusion-Exclusion (PIE)</strong>: this is a general formula for the probability of the union of $n$ events
\begin{align*}
P(\bigcup_{i=1}^n A_i) &= \sum_i P(A_i) - \sum_{i &lt; j} P(A_i \cap A_j)\\
&+ \sum_{i &lt; j &lt; k} P(A_i \cap A_j \cap A_k) - \cdots + (-1)^{n+1} P(\bigcap_{i=1}^n A_i).
\end{align*}
Note that the formula for the probability of the union of two events is the $n=2$ case of PIE.<br>A potential workflow (that you saw on Pset 1) for the probability of an intersection, $P(A_1 \cap \cdots \cap A_n)$, is to<ul><li>Use complementary counting and DeMorgan&rsquo;s law (in that order) to turn the intersection into a union:
\begin{align*}
P(A_1 \cap \cdots \cap A_n) &= 1 - P((A_1 \cap \cdots \cap A_n)^c)\\
&= 1 - P(A_1^c \cup \cdots \cup A_n^c)
\end{align*}</li><li>Apply PIE to the union $P(A_1^c \cup \cdots \cup A_n^c)$</li></ul></li></ul><!-- TODO ADD VISUALIZATION --><h2 id=2-conditional-probability>2. Conditional Probability</h2><div class="alert alert-warning"><div><p><strong>Notation note</strong>:</p><p>We will start writing $P(A \cap B)$ as $P(A, B)$ (i.e., commas between events and intersections are equivalent).</p></div></div><div class="alert alert-note"><div><p><strong>Conditional probability</strong>:</p><p>If $A$ and $B$ are two events, then the probability that $A$ occurs <strong>conditional</strong> on the fact that $B$ occurs (or <em>given</em> that $B$ occurs) is notated as $P(A|B)$ and equals
$$
P(A|B) = \frac{P(A, B)}{P(B)}.
$$
All conditions go to the right of the bar symbol $|$.</p></div></div><p>We read $P(A|B)$ is the &ldquo;probability of $A$ given $B$&rdquo; or &ldquo;probability of $A$ conditioned on $B$&rdquo; Intuitively, we can consider that if we know $B$ occurs, $B$ basically becomes our new sample space, so we take the probability that both $A$ and $B$ occurs, $P(A, B)$, and rescale it by the probability that $B$ occurs, $P(B)$.</p><video controls>
<source src=/media/videos/Section2_ConditionalProbability.mp4 type=video/mp4></video><p>We&rsquo;re also quick to note that conditional probabilities are the same as &ldquo;normal&rdquo; probabilities &mdash; in fact, all probabilities can be considered conditional, we just treat some conditions more implicitly than others since they are more obvious/always involved to the problem. We&rsquo;ll use <strong>extra conditioning</strong> to refer to problems where some conditions are always present (i.e., we never want to/don&rsquo;t know how to calculate the probability of those conditionns). For example, to calculate the probability that it rains tomorrow ($A$) given that it rained today $B$, we would right $P(A|B)$. However, we are implicitly conditioning on a lot of things: that the world exists tomorrow ($W$), that I will be on Harvard campus when I check whether it rains $H$, etc. So we could incorporate these extra conditions into our problem to write the definition of <strong>conditional probability with extra conditioning</strong>:
$$
P(A|B, H, W) = \frac{P(A, B | H, W)}{P(B | H, W)}
$$
As you can see, when we want certain events to be <em>extra conditions</em>, they are conditions in every related probability we calculate. Each time we apply a formula for conditional probability, we have to choose whether to treat the each condition like $B$ (free to move around) or like $H$ (extra conditioning/always a condition).</p><h2 id=3-tools-using-conditional-probability>3. Tools using Conditional Probability</h2><div class="alert alert-warning"><div>If you ever need to solve a problem involving a sequence of things (like a game with many turns, or a random walk, or so on) and are stuck, try <strong>first-step analysis</strong>: conditioning what happens after the first step. You&rsquo;ll often be able to get a recursive equation that is easier to solve.</div></div><h3 id=31-probability-of-an-intersection>3.1 Probability of an Intersection</h3><p>For events $A, B$ we can rearrange the definition of conditional probability to find the probability of their intersection:
$$
P(A, B) = P(A) P(B | A) = P(B) P(A | B).
$$
This works for intersections of $n$ events:
$$
P(A_1, A_2, \ldots, A_n) = P(A_{i_1}) P(A_{i_2} | A_{i_1}) \cdots P(A_{i_n} | A_{i_1}, A_{i_2}, \ldots A_{i_{n-1}})
$$
where $i_1, i_2, \ldots, i_n$ is a permutation of $1, 2, \ldots, n$. Note that we can choose the conditions in any order we want, and pick the order to our best convenience &mdash; for example, if you only know the unconditional probability of $A_8$, you should let $i_1 = 8$.</p><p>The probability of an intersection <em>with extra conditioning</em> is
$$
P(A, B | C) = P(A | C) P(B | A, C) = P(B | C) P(A | B, C).
$$</p><h3 id=32-law-of-total-probability-lotp>3.2 Law of Total Probability (LOTP)</h3><p>The <strong>law of total probability (LOTP)</strong> is a clever rephrasing of the second axiom of probability (the probability of a partition is the sum of probabilities): if $A_1, A_2, \ldots, A_n$ partition the sample space, then
\begin{align*}
P(B) &= P(B, A_1) + P(B, A_2) + \cdots + P(B, A_n).
\end{align*}
See Figure 2.3 from Blizstein & Hwang below for a visualization:<figure><div class="d-flex justify-content-center"><div class=w-100><img alt="Visual diagram of the law of total probability, showing an event split up into pieces by its intersections with a partition of the sample space." srcset="/media/images/Section2_LOTP_hu81255567074645dc13bc1b2d32e50da6_60337_e16aa315eb347d762ed01077f51a4a21.webp 400w,
/media/images/Section2_LOTP_hu81255567074645dc13bc1b2d32e50da6_60337_6c867f1081982bae899a8c3d574550e0.webp 760w,
/media/images/Section2_LOTP_hu81255567074645dc13bc1b2d32e50da6_60337_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/media/images/Section2_LOTP_hu81255567074645dc13bc1b2d32e50da6_60337_e16aa315eb347d762ed01077f51a4a21.webp width=624 height=410 loading=lazy data-zoomable></div></div></figure></p><p>We usually break down each of the terms using the result for the probability of an intersection <a href=#31-probability-of-an-intersection>section 3.1</a>
$$
P(B) = P(B | A_1) P(A_1) + P(B | A_2) P(A_2) + \cdots + P(B | A_n) P(A_n)
$$
When you only know $B$ in terms of conditional probabilities, we can make those conditional probabilities appear through LOTP. As Joe likes to say, <em>condition on what you wish you knew</em>:</p><p>LOTP with <em>extra conditioning</em> is
$$
P(B | C) = P(B | A_1, C) P(A_1 | C) + P(B | A_2, C) P(A_2 | C) + \cdots + P(B | A_n, C) P(A_n| C)
$$</p><h3 id=33-bayes-rule>3.3 Bayes&rsquo; Rule</h3><p><strong>Bayes&rsquo; Rule</strong> is also a result of the formula for the probability of an intersection:
$$
P(B|A) = \frac{P(A|B) P(B)}{P(A)}.
$$
The denominator often gets expanded out using LOTP (<a href=#32-law-of-total-probability-lotp>section 3.2</a>), often with a partition containing $B$ like
$$
P(B|A) = \frac{P(A|B) P(B)}{P(A|B) P(B) + P(A|B^c) P(B^c)}.
$$
Bayes&rsquo; rule is used in situations where we don&rsquo;t know how to calculate the probability of $B$ given $A$, $P(B|A)$, but know how to calculate the probability of $A$ given $B$, $P(A|B)$.</p><p>Bayes&rsquo; rule with <em>extra conditioning</em> is
$$
P(B | A, C) = \frac{P(A | B, C) P(B | C)}{P(A | B, C) P(B|C) + P(A|B^c, C) P(B^c|C)}
$$</p><h2 id=4-independence>4. Independence</h2><div class="alert alert-note"><div><p><strong>Independence for two events</strong></p><p>Two events $A, B$ are independent if
$$
P(A, B) = P(A) P(B)
$$</p><p>For $P(A), P(B) > 0$, this either of the following as well:
\begin{align*}
P(A|B) &= P(A) \text{ or }\
P(B|A) &= P(B)
\end{align*}</p></div></div><p>Intuitively, independence means that information about $A$ (e.g., knowing whether $A$ occurs) gives us no information about $B$. Some</p><ul><li>Note that independence goes both ways &mdash; if $A$ is independent of $B$, then $B$ is independent of $A$.</li><li>If $A$ is independent of $B$, then $A$ is independent of $B^c$ and $A^c$ is independent of $B^c$.</li></ul><div class="alert alert-warning"><div>Independence and disjointness are not the same! In fact, if $A, B$ are disjoint, then if $A$ occurs we know that $B$ did not occur, so they are very <em>dependent</em>.</div></div><div class="alert alert-note"><div><p><strong>Independence for many events</strong></p><p>A group of events $A_1, A_2, \ldots, A_n$, are independent if for any subset $A_{i_1}, A_{i_2}, \ldots, A_{i_k}$,
$$
P(A_{i_1}, A_{i_2}, \ldots, A_{i_k}) = P(A_{i_1}) P(A_{i_2}) \cdots P(A_{i_k})
$$
Note that pairwise independence (e.g., showing that $P(A_i, A_j) = P(A_i) P(A_j)$ for all $i,j$) is required, but not enough, to show that joint independence of all of the sets.</p></div></div><h3 id=41-conditional-independence>4.1 Conditional Independence</h3><p><strong>Conditional independence</strong> follows a similar formula: $A, B$ are conditionally independent given $C$ if
$$
P(A, B | C) = P(A|C) P(B|C).
$$
You can see how this is analogous to extra conditioning for the other results. Conditional independence of many events is similarly defined.</p><div class="alert alert-warning"><div>Conditional independence and independence are not the same thing, and one often exists without the other!</div></div><h2 id=5-summary>5. Summary</h2><p><strong>Notation note</strong>: see that we use commas and intersections interchangeably (i.e., $P(A, B, C) = P(A \cap B \cap C)$).</p><p><strong>Tips for calculating probabilities</strong>:</p><ol><li>Define events for every aspect of the problem (e.g., &ldquo;$A$ = the event that it rains tomorrow, $B$ = the event that it rained today&rdquo;)</li><li>Write out the probabilities that you are given in the problem using notation (e.g., &ldquo;$P(A|B) = 1/2$, $P(B) = 1/4$).</li><li>Write the probability that you want to calculate using notation (e.g., we want to calculate the unconditional probability that it rains tomorrow, $P(A)$).</li><li>Figure out how the tools we have learned allow you to utilize the probabiliies that you do know (step 2) to calculate the probabilities that you don&rsquo;t know (step 3).</li></ol><h3 id=51-definition-of-probability>5.1 Definition of Probability</h3><p><strong>Axioms of probability:</strong></p><ol><li>With sample space $S$, \begin{align*}
P(S) &= 1\\
P(\emptyset) &= 0.
\end{align*}</li><li>For $A_1, A_2, \ldots, $ that partition $B$ (this can be finite or infinite),
\begin{align*}
P(B) = \sum_{j=1}^\infty P(A_j)
\end{align*}</li></ol><p><strong>Probability of a complement</strong>: For event $A$,
$$
P(A) = 1 - P(A^c)
$$</p><p><strong>Probability of a union</strong>: For events $A$ and $B$,
\begin{align*}
P(A \cup B) &= P(A) + P(B) - P(A \cap B)\\ &= P(A \cap B^c) + P(B \cap A^c) + P(A \cap B)
\end{align*}</p><p><strong>Principle of Inclusion-Exclusion</strong>: For events $A_1, \ldots, A_n$,
\begin{align*}
P(\bigcup_{i=1}^n A_i) &= \sum_i P(A_i) - \sum_{i &lt; j} P(A_i \cap A_j)\\
&+ \sum_{i &lt; j &lt; k} P(A_i \cap A_j \cap A_k) - \cdots + (-1)^{n+1} P(\bigcap_{i=1}^n A_i).
\end{align*}</p><h3 id=52-conditional-probability>5.2 Conditional Probability</h3><p><strong>Conditional probability</strong>: For events $A$ and $B$, the probability of $A$ given $B$ (i.e., given that $B$ occured) is
\begin{align*}
P(A|B) = \frac{P(A \cap B)}{P(B)}.
\end{align*}
<em>&mldr;with extra conditioning</em>:
\begin{align*}
P(A|B, C) = \frac{P(A \cap B | C)}{P(B | C)}.
\end{align*}</p><h3 id=53-conditional-probability-tools>5.3 Conditional Probability Tools</h3><p><strong>First-step analysis</strong>: If you ever need to solve a problem involving a sequence of things (like a game with many turns, or a random walk, or so on) and are stuck, try first-step analysis: conditioning what happens after the first step. You&rsquo;ll often be able to get a recursive equation that is easier to solve.</p><p><strong>Probability of an intersection</strong>:
\begin{align*}
P(A_1, A_2, \ldots A_n) &= P(A_1) P(A_2 | A_1) \cdots P(A_n | A_1, \ldots, A_{n-1})\\
&= P(A_n) P(A_{n-1} | A_n) \cdots P(A_1 | A_2, \ldots, A_n), \\
&= [\text{chaining in any order that is convenient for you}].
\end{align*}
<em>&mldr;with extra conditioning</em>:
\begin{align*}
P(A_1, A_2, \ldots A_n | C) &= P(A_1 | C) P(A_2 | A_1, C) \cdots P(A_n | A_1, \ldots, A_{n-1}, C)
\end{align*}
<strong>Law of Total Probability (LOTP)</strong>: for events $A_1, A_2, \ldots, A_n$ that partition $S$, we can find $P(B)$ by
\begin{align*}
P(B) &= P(B, A_1) + P(B, A_2) + \cdots + P(B, A_n) \\
&= P(B | A_1) P(A_1) + P(B | A_2) P(A_2) + \cdots + P(B | A_n) P(A_n).
\end{align*}
We pick $A_1, A_2, \ldots, A_n$ to &ldquo;condition on what we wish we knew.&rdquo; These are situations where you don&rsquo;t know $P(B)$, but you know $P(B | A_1), (B | A_2)$, etc.</p><p><em>&mldr;with extra conditioning</em>:
\begin{align*}
P(B|C) &= P(B, A_1|C) + P(B, A_2|C) + \cdots + P(B, A_n|C) \\
&= P(B | A_1,C) P(A_1|C) + P(B | A_2,C) P(A_2|C) + \cdots + P(B | A_n,C) P(A_n|C).
\end{align*}
<strong>Bayes&rsquo; Rule</strong>: for events $A, B$, if we want to calculate $P(B|A)$ but can only know how to calculate $P(A|B)$,
\begin{align*}
P(B|A) &= \frac{P(A|B) P(B)}{P(A)}\\
&= \frac{P(A|B) P(B)}{P(A|B) P(B) + P(A|B^c) P(B^c)},
\end{align*}
where we commonly expand the denominator using the Law of Total Probability (LOTP).</p><p><em>&mldr;with extra conditioning</em>:
\begin{align*}
P(B|A, C) &= \frac{P(A|B, C) P(B|C)}{P(A|C)} \\
&= \frac{P(A|B, C) P(B|C)}{P(A|B, C) P(B|C) + P(A|B^c, C) P(B^c |C)}
\end{align*}</p><h3 id=54-independence>5.4 Independence</h3><p><strong>Independence</strong>: $A, B$ are defined to be independent if
\begin{align*}
P(A, B) = P(A) P(B).
\end{align*}
Note that if $A, B$ are independent, then so are $A, B^c$ and $A^c, B$, and $A^c, B^c$; basically any functions of $A$ and $B$ are independent.</p><p><strong>VERY IMPORTANT</strong>: Disjointness and independence are not the same thing, and disjoint events are in fact usually independent.</p><p>A set of events $A_1, A_2, \ldots, A_n$ is independent if any subset of the events $A_{j_1}, \ldots, A_{j_k}$ follows the equation.
\begin{align*}
P(A_{j_1}, \ldots, A_{j_k}) &= P(A_{j_1}) \cdots P(A_{j_k}).
\end{align*}
Basically, for any combination of independent events, we should be able to factor out the probabilities.</p><p><strong>Conditional independence</strong>:
\begin{align*}
P(A, B | C) = P(A|C) P(B|C).
\end{align*}
<strong>VERY IMPORTANT</strong>: Independence and conditional independence are not the same/do not imply each other. There is no guarantee that independent events are conditionally independent, or vice versa.</p><!-- ## -1. Content to cover

- vandermonde's identity + birthday problem... maybe vibes but not focused. Maybe do a practice problem
- also disease testing example --><!-- What skills to develop?
- Being able to make problem-specific calculations
- Identifying when we're independent of conditioning
- Picking what to condition on - what you wish you knew, or a single-step analysis to get you closer/recursive
- Make sure we have scattered examples as well! Maybe will work through practice problems as we go instead of leaving all for the end.
- intuitive limits of values of priors, show that extreme values stay extreme!
- maybe visualize/show bayesian updating as an average b/w prior and posterior?
 --></article><div class=article-tags><a class="badge badge-light" href=/tag/expository/>expository</a>
<a class="badge badge-light" href=/tag/teaching/>teaching</a></div><div class=article-widget><div class=post-nav><div class=post-nav-item><div class=meta-nav>Previous</div><a href=/stat110/week3/ rel=next>Week 3: Conditional Probability Examples and Random Variables</a></div><div class=post-nav-item><div class=meta-nav>Next</div><a href=/stat110/week1/ rel=prev>Week 1: Counting and Definitions of Probability</a></div></div></div></div><div class=body-footer><p>Last updated on Sep 16, 2023</p></div><footer class=site-footer><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> â€” the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></main></div></div></div><div class=page-footer></div><script src=/js/vendor-bundle.min.f64289d8217e08e3afcd597d60836062.js></script>
<script src=https://cdn.jsdelivr.net/npm/anchor-js@5.0.0/anchor.min.js integrity="sha256-aQmOEF2ZD4NM/xt4hthzREIo/2PFkOX/g01WjxEV7Ys=" crossorigin=anonymous></script>
<script>anchors.add()</script><script id=page-data type=application/json>{"use_headroom":false}</script><script src=/en/js/wowchemy.min.387a7b38a3dfead6f96c96742d20f5af.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>
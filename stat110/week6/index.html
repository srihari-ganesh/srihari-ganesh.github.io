<!doctype html><!-- This site was created with Wowchemy. https://www.wowchemy.com --><!-- Last Published: December 26, 2023 --><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&family=Merriweather&family=Roboto+Mono&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&family=Merriweather&family=Roboto+Mono&display=swap" media=print onload='this.media="all"'><script src=/js/mathjax-config.js></script>
<link rel=stylesheet href=/css/vendor-bundle.min.047268c6dd09ad74ba54a0ba71837064.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script>
<link rel=stylesheet href=/css/wowchemy.9e3515c7534a90d8339ca9703c3e3bce.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Srihari Ganesh"><meta name=description content="Lecture dates: 10/17, 10/19; Section date: 10/25"><link rel=alternate hreflang=en-us href=https://srihari-ganesh.github.io/stat110/week6/><link rel=canonical href=https://srihari-ganesh.github.io/stat110/week6/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu238e72ff877ecedb95a45ec9a2f2431b_15216_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu238e72ff877ecedb95a45ec9a2f2431b_15216_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#707070"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@wowchemy"><meta property="twitter:creator" content="@wowchemy"><meta property="twitter:image" content="https://srihari-ganesh.github.io/media/icon_hu238e72ff877ecedb95a45ec9a2f2431b_15216_512x512_fill_lanczos_center_3.png"><meta property="og:site_name" content="Srihari Ganesh"><meta property="og:url" content="https://srihari-ganesh.github.io/stat110/week6/"><meta property="og:title" content="Week 6: Universality of the Uniform, Normal, Expo, and Moments | Srihari Ganesh"><meta property="og:description" content="Lecture dates: 10/17, 10/19; Section date: 10/25"><meta property="og:image" content="https://srihari-ganesh.github.io/media/icon_hu238e72ff877ecedb95a45ec9a2f2431b_15216_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2023-10-22T00:00:00+00:00"><meta property="article:modified_time" content="2023-10-22T00:00:00+00:00"><title>Week 6: Universality of the Uniform, Normal, Expo, and Moments | Srihari Ganesh</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=0c603db2c1f9bde191c2df0ca36e5497><script src=/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js></script><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Srihari Ganesh</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Srihari Ganesh</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#projects><span>Projects</span></a></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Teaching</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/#teaching><span>Experience</span></a>
<a class=dropdown-item href=/stat110><span>Stat 110 Section Notes</span></a></div></li><li class=nav-item><a class=nav-link href=/#contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><div class="container-fluid docs"><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 docs-sidebar"><form class="docs-search d-flex align-items-center"><button class="btn docs-toggle d-md-none p-0 mr-md-3 w-100" type=button data-toggle=collapse data-target=#docs-nav aria-controls=docs-nav aria-expanded=false aria-label="Toggle section navigation"><div class=d-flex><span class="d-md-none pl-1 flex-grow-1 text-left overflow-hidden">Stat 110 Section Notes</span>
<span><i class="fas fa-chevron-down"></i></span></div></button></form><nav class="collapse docs-links" id=docs-nav><ul class="nav docs-sidenav"><li><a href=/stat110/>Stat 110 Section Notes</a></li><li><a href=/stat110/week11/>Week 11: Markov Chains</a></li><li><a href=/stat110/week10/>Week 10: Adam's & Eve's Laws, Inequalities, and Limit Theorems</a></li><li><a href=/stat110/week9/>Week 9: Transformations, Gamma/Beta, Conditional Expectation</a></li><li><a href=/stat110/week8/>Week 8: Multinomial, Multivariate Normal</a></li><li><a href=/stat110/week7/>Week 7: Poisson Processes, Joint Distributions, and Covariance</a></li><li class=active><a href=/stat110/week6/>Week 6: Universality of the Uniform, Normal, Expo, and Moments</a></li><li><a href=/stat110/week5/>Week 5: Continuous Distributions</a></li><li><a href=/stat110/week4/>Week 4: Discrete Distributions and Expectation</a></li><li><a href=/stat110/week3/>Week 3: Conditional Probability Examples and Random Variables</a></li><li><a href=/stat110/week2/>Week 2: Conditional Probability</a></li><li><a href=/stat110/week1/>Week 1: Counting and Definitions of Probability</a></li></ul></nav></div><div class="d-none d-xl-block col-xl-2 docs-toc"><ul class="nav toc-top"><li><a href=# id=back_to_top class=docs-toc-title>Contents</a></li></ul><nav id=TableOfContents><ul><li><a href=#0-logistical-info>0. Logistical Info</a><ul><li><a href=#01-summary--practice-problem-pdfs>0.1 Summary + Practice Problem PDFs</a></li></ul></li><li><a href=#1-universality-of-the-uniform>1. Universality of the Uniform</a></li><li><a href=#2-normal-distribution>2. Normal distribution</a><ul><li><a href=#21-standard-normal>2.1 Standard Normal</a></li><li><a href=#22-normal>2.2 Normal</a></li></ul></li><li><a href=#3-exponential-distribution>3. Exponential distribution</a></li><li><a href=#4-momentsmoment-generating-functions>4. Moments/Moment Generating Functions</a></li></ul></nav></div><main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role=main><div class=docs-article-container></div><div class=docs-article-container><h1>Week 6: Universality of the Uniform, Normal, Expo, and Moments</h1><article class=article-style><h2 id=0-logistical-info>0. Logistical Info</h2><ul><li>Section date: 10/25</li><li>Associated lectures: 10/17, 10/19</li><li>Associated pset: Pset 6, due 10/27</li><li>Office hours on 10/25 from 7-9pm at Quincy Dining Hall</li><li>Please reach out if you wanted to sign up for a midterm debrief and missed the chance</li><li>Remember to fill out the attendance form</li></ul><h3 id=01-summary--practice-problem-pdfs>0.1 Summary + Practice Problem PDFs</h3><p><a href=/uploads/stat110/Section%206%20Handout.pdf target=_blank>Summary + Practice Problems PDF</a></p><p><a href=/uploads/stat110/Section%206%20Solutions.pdf target=_blank>Practice Problem Solutions PDF</a></p><h2 id=1-universality-of-the-uniform>1. Universality of the Uniform</h2><p>Recall that the standard uniform, $U \sim \mathrm{Unif}(0, 1)$, has support $(0, 1)$ with PDF $1$ in the support.</p><div class="alert alert-note"><div><p><strong>Universality of the Uniform (UoU)</strong>: If $F$ is a valid CDF that is continuous and strictly increasing over the support, then</p><ol><li>Let $U \sim \mathrm{Unif}(0, 1)$. Then $F^{-1} (U)$ is a random variable with CDF $F$.</li><li>Let $X$ have CDF $F$. Then $F(X) \sim \mathrm{Unif}(0,1)$.</li></ol><p>The first result applies to discrete random variables as well. The second result only works for continuous random variables.</p></div></div><p>This result is quite useful for simulation - if you have access to draws from a Uniform distribution, then you can transform them into draws from any distribution with a known (inverse) CDF.</p><p>We can prove UoU with the tools we&rsquo;ve learned in class. For continuous random variables with $F$ as described in the theorem,</p><ol><li>For $x \in \mathbb{R}$,
\begin{align*}
P(F^{-1}(U) &lt; x) = P(F(F^{-1}(U)) &lt; F(x)) = P(U &lt; F(x)) = F(x).
\end{align*}
So $F^{-1}(U)$ has CDF $F$. We used the CDF of $U$ in the last step, since $F(x) \in [0, 1]$.</li><li>For $u \in [0, 1]$,
\begin{align*}
P(F(X) &lt; u) &= P\left(F^{-1}(F(X)) &lt; F^{-1}(u)\right)\\
&= P(X &lt; F^{-1}(u)) = F(F^{-1}(u)) = u,
\end{align*}
so $F(X) \sim \mathrm{Unif}(0, 1)$ since it has the CDF of a standard uniform.</li></ol><h2 id=2-normal-distribution>2. Normal distribution</h2><h3 id=21-standard-normal>2.1 Standard Normal</h3><p>$Z \sim \mathcal{N}(0, 1)$ is a <strong>standard Normal</strong> random variable with support $\mathbb R$. We notate the CDF as $\Phi$ and PDF as $\phi$.</p><ul><li>(<em>Symmetry</em>) The standard Normal is symmetric about $0$. In math, for $x \in \mathbb R$, $\phi(x) = \phi(-x)$.<ul><li>This also implies that $\Phi(x) = 1 - \Phi(-x)$.</li><li>So $\Phi(0) = 0.5$.</li><li>For $Z \sim \mathcal{N}(0, 1)$, $-Z \sim \mathcal{N}(0, 1)$ as well.</li></ul></li><li>(<em>Empirical rule/68-95-99.7 rule</em>)
\begin{align*}
P(-1 &lt; Z &lt; 1) &\approx 0.68,\\
P(-2 &lt; Z &lt; 2) &\approx 0.95,\\
P(-3 &lt; Z &lt; 3) &\approx 0.997.
\end{align*}</li></ul><p>In this class, you can give exact answers in terms of $\Phi$ and $\phi$. On psets, you should also use a calculator/programming language/the empirical rule to get numerical approximations of $\Phi$.</p><h3 id=22-normal>2.2 Normal</h3><p>$X \sim \mathcal{N}(\mu, \sigma^2)$ (with $\mu \in \mathbb R, \sigma > 0$) is a <strong>Normal</strong> random variable with mean $\mu$ and variance $\sigma^2$, and also has support $\mathbb R$.</p><ul><li><p>(<em>Location-scale</em>)
For $Z \sim \mathcal{N}(0, 1)$, $\mu + \sigma Z \sim \mathcal{N}(\mu, \sigma^2)$.</p><p>More generally, for $X \sim \mathcal{N}(\mu_1, \sigma_1^2)$, $\mu_2 + \sigma_2 X \sim \mathcal{N}(\mu_2 + \mu_1 \sigma_2, \sigma_1^2 \sigma_2^2)$.</p></li><li><p>(<em>Standardization</em>)
For $X \sim \mathcal{N}(\mu, \sigma^2)$, $\frac{X-\mu}{\sigma} \sim \mathcal{N}(0, 1)$.\
We often use this to get results in terms of $\Phi$:
\begin{align*}
P(X &lt; x) = P(\frac{X-\mu}{\sigma} &lt; \frac{x-\mu}{\sigma}) = \Phi(\frac{x-\mu}{\sigma}).
\end{align*}</p></li><li><p>(<em>Empirical rule</em>) For $X \sim \mathcal{N}(\mu, \sigma^2)$,
\begin{align*}
P(\mu-\sigma &lt; X &lt; \mu+\sigma) &\approx 0.68\\
P(\mu-2\sigma &lt; X &lt; \mu+2\sigma) &\approx 0.95\\
P(\mu-3\sigma &lt; X &lt; \mu+3\sigma) &\approx 0.997
\end{align*}</p></li><li><p>(<em>Sum of independent Normals</em>)
Let $X \sim \mathcal{N}(\mu_1, \sigma_1^2)$ and $Y \sim \mathcal{N}(\mu_2, \sigma_2^2)$ with $X, Y$ independent. Then
\begin{align*}
X + Y &\sim \mathcal{N}(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2),\\
X - Y &\sim \mathcal{N}(\mu_1 - \mu_2, \sigma_1^2 + \sigma_2^2).
\end{align*}</p></li></ul><div class="alert alert-warning"><div>(<em>Variance when subtracting</em>)
See that we always add the variance above! This is also a general rule: for any independent random variables $X$ and $Y$,
\begin{align*}
Var(X+Y) = Var(X - Y) = Var(X) + Var(Y).
\end{align*}
See that this is consistent with the fact that $Var(-Y) = (-1)^2 Var(Y) = Var(Y)$.</div></div><h2 id=3-exponential-distribution>3. Exponential distribution</h2><p>$X \sim \mathrm{Expo}(\lambda)$ is an <strong>Exponential</strong> random variable with mean $\frac{1}{\lambda}$ and variance $\frac{1}{\lambda^2}$. $\lambda$ is called the <strong>rate parameter</strong>.</p><ul><li><p>(<em>Memorylessness</em>)
For $X \sim \mathrm{Expo}(\lambda)$ and any $s, t > 0$, the <strong>memoryless</strong> property of the Exponential distribution states the following (equivalent) results:
\begin{align*}
P(X > s + t \vert X > s) &= P(X > t)\\
(X - s \vert X > s) &\sim \mathrm{Expo}(\lambda).
\end{align*}
See specifically that $X-s | X>s$ is independent of the value of $s$.</p><p>The Exponential distribution is the only continuous distribution with this property. Additionally, the Geometric distribution is the only discrete distribution with support ${0, \ldots, }$ that is memoryless.</p></li></ul><div class="alert alert-warning"><div><p>For most results we talk about, you can&rsquo;t put a random variable in the place of a constant - you might recall from last week&rsquo;s problem set that we couldn&rsquo;t let the sum of $N$ independent $\mathrm{Pois}(\lambda)$ r.v.s, with $N$ random, be distributed $\mathrm{Pois}(N\lambda)$. However, with memorylessness, you can put random variables in the place of the $s$ above - so for some random variable $Y$, $P(X > t +Y | X > Y) = P(X > t)$ and $(X-Y|X > Y) \sim \mathrm{Expo}(\lambda)$ still.</p><details class=spoiler id=spoiler-0><summary><ins>Click for proof</ins></summary><p>We can prove by using LOTP and applying the constant version of memorylessness. We&rsquo;ll assume $Y$ is discrete here, but continuous case is analogous (swap sums for integrals, PMFs for PDFs).
\begin{align*}
P(X > t+Y | X > Y) &= \sum_{y} P(X > t+y | X > Y, Y = y) P(Y=y)\\
&= \sum_{y} P(X > t+y | X > y, Y = y) P(Y=y).
\end{align*}
We&rsquo;ll take a brief sidebar to show that $P(X > t + y | X > y , Y = y) = P(X > t+y | X>y)$; I think you can jump from the former to the latter using unconditional independence of $X$ and $Y$ since the extra condition is a function of $X$, but we&rsquo;ll be explicit here. We will use the definition of conditional probability, the fact that $X>t+y$ implies that $X>y$, and the unconditional independence of $X$ and $Y$.
\begin{align*}
P(X > t+y | X > y, Y = y) &= \frac{P(X > t+y, X > y, Y =y)}{P(X > y, Y = y)}\\
&= \frac{P(X > t+y, Y =y)}{P(X > y ,Y =y)}\\
&= \frac{P(X > t+y) P(Y = y) }{P(X>y)P(Y=y)}\\
&= \frac{P(X>t+y)}{P(X>y)}\\
&= \frac{P(X>t+y, X>y)}{P(X>y)}\\
&= P(X>t+y | X>y).
\end{align*}
With this information,
\begin{align*}
P(X > t+Y | X >Y) &= \sum_y P(X>t+y|X>y, Y=y) P(Y=y)\\
&= \sum_y P(X>t+y|X>y) P(Y=y)\\
&= \sum_y P(X>t) P(Y=y)\\
&= P(X>t) \sum_y P(Y=y)\\
&= P(X>t) (1) = P(X>t),
\end{align*}
where we use memorylessness to say $P(X>t+y | X>y) = P(X>t)$.</p></details></div></div><ul><li>(<em>Example of Memorylessness</em>)
Suppose you&rsquo;re waiting for a bus that will arrive in $X \sim \mathrm{Expo}(\lambda)$ minutes. If you wait for the bus for 10 minutes and it has not arrived, then the remaining time that you have to wait is still distributed $\mathrm{Expo}(\lambda)$: $X - 10 | X > 10 \sim \mathrm{Expo}(\lambda)$. So no matter how long you wait, the remaining time for you to wait has the same distribution.</li><li>(<em>Minimum of Expos</em>)
The minimum of $n$ i.i.d. $\mathrm{Expo}(\lambda)$ random variables is distributed $\mathrm{Expo}(n\lambda)$. In notation, for $X_1, \ldots, X_n \overset{i.i.d.}{\sim} \mathrm{Expo}(\lambda)$, $\min(X_1, \ldots, X_n) \sim \mathrm{Expo}(n\lambda)$.</li></ul><div class="alert alert-warning"><div><p><strong>Maximum of Expos</strong></p><p>The maximum of $n$ i.i.d. Exponential distributions is <em>not</em> does not follow an Exponential distribution.</p></div></div><div class="alert alert-note"><div><p><strong>Finding the distribution of minimums/maximums</strong></p><p>The results above can be found in the book, but they provide a general template for finding the distributions of minimums and maximums.</p><p>Let $X_1, \ldots, X_n$ be any random variables. Then the events ${\min(X_1, \ldots, X_n) > x}$ and $(X_1 > x) \cap (X_2 > x) \cap \cdots \cap (X_n > x)$ are equivalent. To convince yourself of this, think about what this means in words: the minimum of a set of numbers is greater than $x$ if and only if each one of the numbers is great than $x$.</p><p>To find the CDF of $\min(X_1, \ldots, X_n)$, a common workflow is
\begin{align*}
P(\min(X_1, \ldots, X_n) \le x) &= 1 - P(\min(X_1, \ldots, X_n) > x) = 1 - P(X_1 > x, X_2 > x, \ldots, X_n > x).
\end{align*}
If $X_1, \ldots, X_n$ are independent, then we can get that
\begin{align*}
P(X_1 > x, X_2 > x, \ldots, X_n > x) &= P(X_1 > x) P(X_2 > x) \cdots P(X_n > x)
\end{align*}
If $X_1, \ldots, X_n$ are also identically distributed, we conclude with
\begin{align*}
P(X_1 > x) P(X_2 > x) \cdots P(X_n > x) &= (P(X_1 > x))^n.
\end{align*}</p><p>For maximums, we follow a similar workflow, except instead using the fact that $${\max(X_1, \ldots, X_n) &lt; x} = \bigcap_{i=1}^n (X_i &lt; x).$$</p></div></div><h2 id=4-momentsmoment-generating-functions>4. Moments/Moment Generating Functions</h2><p>For a random variable $X$, the $\mathbf{n^{th}}$ <strong>moment</strong> is $E(X^n)$.</p><div class="alert alert-note"><div><p><strong>Moment Generating Function</strong></p><p>For a random variable $X$, the <strong>moment generating function (MGF)</strong> is $M_X(t) = E(e^{tX})$ for $t \in \mathbb{R}$. If the MGF exists, then
\begin{align*}
M_X(0) &= 1,\
\frac{d^n}{dt^n} M_X(t) |_{t=0} = M_X^{(n)}(t) &= E(X^n).
\end{align*}
You should sanity-check that $M_X(0) = 1$ whenever you calculate an MGF.</p></div></div></article><div class=article-tags><a class="badge badge-light" href=/tag/expository/>expository</a>
<a class="badge badge-light" href=/tag/teaching/>teaching</a></div><div class=article-widget><div class=post-nav><div class=post-nav-item><div class=meta-nav>Previous</div><a href=/stat110/week7/ rel=next>Week 7: Poisson Processes, Joint Distributions, and Covariance</a></div><div class=post-nav-item><div class=meta-nav>Next</div><a href=/stat110/week5/ rel=prev>Week 5: Continuous Distributions</a></div></div></div></div><div class=body-footer><p>Last updated on Oct 22, 2023</p></div><footer class=site-footer><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> â€” the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></main></div></div></div><div class=page-footer></div><script src=/js/vendor-bundle.min.f64289d8217e08e3afcd597d60836062.js></script>
<script src=https://cdn.jsdelivr.net/npm/anchor-js@5.0.0/anchor.min.js integrity="sha256-aQmOEF2ZD4NM/xt4hthzREIo/2PFkOX/g01WjxEV7Ys=" crossorigin=anonymous></script>
<script>anchors.add()</script><script id=page-data type=application/json>{"use_headroom":false}</script><script src=/en/js/wowchemy.min.387a7b38a3dfead6f96c96742d20f5af.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>